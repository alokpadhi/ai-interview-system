{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8fd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47adcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/alexeygrigorev/data-science-interviews/refs/heads/master/theory.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793f3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb04252",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b89fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883ab6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(?m)^##\\s.*(?:\\n(?!##).*)*'\n",
    "sections = re.findall(pattern, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b01840d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## Table of contents\\n\\n* [Supervised machine learning](#supervised-machinelearning)\\n* [Linear regression](#linear-regression)\\n* [Validation](#validation)\\n* [Classification](#classification)\\n* [Regularization](#regularization)\\n* [Feature selection](#feature-selection)\\n* [Decision trees](#decision-trees)\\n* [Random forest](#random-forest)\\n* [Gradient boosting](#gradient-boosting)\\n* [Parameter tuning](#parameter-tuning)\\n* [Neural networks](#neural-networks)\\n* [Optimization in neural networks](#optimization-in-neuralnetworks)\\n* [Neural networks for computer vision](#neural-networks-for-computervision)\\n* [Text classification](#text-classification)\\n* [Clustering](#clustering)\\n* [Dimensionality reduction](#dimensionality-reduction)\\n* [Ranking and search](#ranking-andsearch)\\n* [Recommender systems](#recommender-systems)\\n* [Time series](#time-series)\\n\\n<br/>\\n',\n",
       " '## Supervised machine\\xa0learning\\n\\n**What is supervised machine learning? üë∂**\\n\\nSupervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the\\xa0input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.\\n\\nk-Nearest Neighbors(KNN):Looking at the k closest labeled data points \\n\\n<br/>\\n',\n",
       " '## Linear regression\\n\\n**What is regression? Which models can you use to solve a regression problem? üë∂**\\n\\nRegression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor).\\nHere are some common regression models\\n\\n- *Linear Regression* establishes a linear relationship between target and predictor (s). It predicts a numeric value and has a shape of a straight line.\\n- *Polynomial Regression* has a regression equation with the power of independent variable more than 1. It is a curve that fits into the data points.\\n- *Ridge Regression* helps when predictors are highly correlated (multicollinearity problem). It penalizes the squares of regression coefficients but doesn‚Äôt allow the coefficients to reach zeros (uses L2 regularization).\\n- *Lasso Regression* penalizes the absolute values of regression coefficients and allows some of the coefficients to reach absolute zero (thereby allowing feature selection).\\n\\n<br/>\\n\\n**What is linear regression? When do we use it? üë∂**\\n\\nLinear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).\\n\\nWith a simple equation:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```\\n\\nB is regression coefficients, x values are the independent (explanatory) variables  and y is dependent variable.\\n\\nThe case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\\n\\nSimple linear regression:\\n\\n```\\ny = B0 + B1*x1\\n```\\n\\nMultiple linear regression:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```\\n\\n<br/>\\n\\n**What are the main assumptions of linear regression? ‚≠ê**\\n\\nThere are several assumptions of linear regression. If any of them is violated, model predictions and interpretation may be worthless or misleading.\\n\\n1. **Linear relationship** between features and target variable.\\n2. **Additivity** means that the effect of changes in one of the features on the target variable does not depend on values of other features. For example, a model for predicting revenue of a company have of two features - the number of items _a_ sold and the number of items _b_ sold. When company sells more items _a_ the revenue increases and this is independent of the number of items _b_ sold. But, if customers who buy _a_ stop buying _b_, the additivity assumption is violated.\\n3. Features are not correlated (no **collinearity**) since it can be difficult to separate out the individual effects of collinear features on the target variable.\\n4. Errors are independently and identically normally distributed (y<sub>i</sub> = B0 + B1*x1<sub>i</sub> + ... + error<sub>i</sub>):\\n   1. No correlation between errors (consecutive errors in the case of time series data).\\n   2. Constant variance of errors - **homoscedasticity**. For example, in case of time series, seasonal patterns can increase errors in seasons with higher activity.\\n   3. Errors are normally distributed, otherwise some features will have more influence on the target variable than to others. If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow.\\n\\n<br/>\\n\\n**What‚Äôs the normal distribution? Why do we care about it? üë∂**\\n\\nThe normal distribution is a continuous probability distribution whose probability density function takes the following formula:\\n\\n![formula](https://mathworld.wolfram.com/images/equations/NormalDistribution/NumberedEquation1.gif)\\n\\nwhere Œº is the mean and œÉ is the standard deviation of the distribution.\\n\\nThe normal distribution derives its importance from the **Central Limit Theorem**, which states that if we draw a large enough number of samples, their mean will follow a normal distribution regardless of the initial distribution of the sample, i.e **the distribution of the mean of the samples is normal**. It is important that each sample is independent from the other.\\n\\nThis is powerful because it helps us study processes whose population distribution is unknown to us.\\n\\n\\n<br/>\\n\\n**How do we check if a variable follows the normal distribution? \\u200d‚≠êÔ∏è**\\n\\n1. Plot a histogram out of the sampled data. If you can fit the bell-shaped \"normal\" curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected.\\n2. Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution.\\n3. Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously.\\n4. Check for Quantile-Quantile plot. It is a scatterplot created by plotting two sets of quantiles against one another. Normal Q-Q plot place the data points in a roughly straight line.\\n\\n<br/>\\n\\n**What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices? \\u200d‚≠êÔ∏è**\\n\\nData is not normal. Specially, real-world datasets or uncleaned datasets always have certain skewness. Same goes for the price prediction. Price of houses or any other thing under consideration depends on a number of factors. So, there\\'s a great chance of presence of some skewed values i.e outliers if we talk in data science terms. \\n\\nYes, you may need to do pre-processing. Most probably, you will need to remove the outliers to make your distribution near-to-normal.\\n\\n<br/>\\n\\n**What methods for solving linear regression do you know? \\u200d‚≠êÔ∏è**\\n\\nTo solve linear regression, you need to find the coefficients $`\\\\beta`$ which minimize the sum of squared errors.\\n\\nMatrix Algebra method: Let\\'s say you have `X`, a matrix of features, and `y`, a vector with the values you want to predict. After going through the matrix algebra and minimization problem, you get this solution: $`\\\\beta = (X^{T}X)^{-1}X^{T}y`$. \\n\\nBut solving this requires you to find an inverse, which can be time-consuming, if not impossible. Luckily, there are methods like Singular Value Decomposition (SVD) or QR Decomposition that can reliably calculate this part $`(X^{T}X)^{-1}X^{T}`$ (called the pseudo-inverse) without actually needing to find an inverse. The popular python ML library `sklearn` uses SVD to solve least squares.\\n\\nAlternative method: Gradient Descent. See explanation below.\\n\\n<br/>\\n\\n**What is gradient descent? How does it work? \\u200d‚≠êÔ∏è**\\n\\nGradient descent is an algorithm that uses calculus concept of gradient to try and reach local or global minima. It works by taking the negative of the gradient in a point of a given function, and updating that point repeatedly using the calculated negative gradient, until the algorithm reaches a local or global minimum, which will cause future iterations of the algorithm to return values that are equal or too close to the current point. It is widely used in machine learning applications.\\n\\n<br/>\\n\\n**What is the normal equation? \\u200d‚≠êÔ∏è**\\n\\nNormal equations are equations obtained by setting equal to zero the partial derivatives of the sum of squared errors (least squares); normal equations allow one to estimate the parameters of a multiple linear regression.\\n\\n<br/>\\n\\n**What is SGD \\u200a‚Äî\\u200a stochastic gradient descent? What‚Äôs the difference with the usual gradient descent? \\u200d‚≠êÔ∏è**\\n\\nIn both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function.\\n\\nThe difference lies in how the gradient of the loss function is estimated. In the usual GD, you have to run through ALL the samples in your training set in order to estimate the gradient and do a single update for a parameter in a particular iteration. In SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to estimate the gradient and do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent.\\n\\n<br/>\\n\\n**Which metrics for evaluating regression models do you know? üë∂**\\n\\n1. Mean Squared Error(MSE)\\n2. Root Mean Squared Error(RMSE)\\n3. Mean Absolute Error(MAE)\\n4. R¬≤ or Coefficient of Determination\\n5. Adjusted R¬≤\\n\\n<br/>\\n\\n**What are MSE and RMSE? üë∂**\\n\\nMSE stands for <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror while RMSE stands for <strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror. They are metrics with which we can evaluate models.\\n\\n<br/>\\n\\n**What is the bias-variance trade-off? üë∂**\\n\\n**Bias** is the error introduced by approximating the true underlying function, which can be quite complex, by a simpler model. **Variance** is a model sensitivity to changes in the training dataset.\\n\\n**Bias-variance trade-off** is a relationship between the expected test error and the variance and the bias - both contribute to the level of the test error and ideally should be as small as possible:\\n\\n```\\nExpectedTestError = Variance + Bias¬≤ + IrreducibleError\\n```\\n\\nBut as a model complexity increases, the bias decreases and the variance increases which leads to *overfitting*. And vice versa, model simplification helps to decrease the variance but it increases the bias which leads to *underfitting*.\\n\\n<br/>\\n\\n',\n",
       " '## Validation\\n\\n**What is overfitting? üë∂**\\n\\nWhen your model perform very well on your training set but can\\'t generalize the test set, because it adjusted a lot to the training set.\\n\\n<br/>\\n\\n**How to validate your models? üë∂**\\n\\nOne of the most common approaches is splitting data into train, validation and test parts.\\nModels are trained on train data, hyperparameters (for example early stopping) are selected based on the validation data, the final measurement is done on test dataset.\\nAnother approach is cross-validation: split dataset into K folds and each time train models on training folds and measure the performance on the validation folds.\\nAlso you could combine these approaches: make a test/holdout dataset and do cross-validation on the rest of the data. The final quality is measured on test dataset.\\n\\n<br/>\\n\\n**Why do we need to split our data into three parts: train, validation, and test? üë∂**\\n\\nThe training set is used to fit the model, i.e. to train the model with the data. The validation set is then used to provide an unbiased evaluation of a model while fine-tuning hyperparameters. This improves the generalization of the model. Finally, a test data set which the model has never \"seen\" before should be used for the final evaluation of the model. This allows for an unbiased evaluation of the model. The evaluation should never be performed on the same data that is used for training. Otherwise the model performance would not be representative.\\n\\n<br/>\\n\\n**Can you explain how cross-validation works? üë∂**\\n\\nCross-validation is the process to separate your total training set into two subsets: training and validation set, and evaluate your model to choose the hyperparameters. But you do this process iteratively, selecting different training and validation set, in order to reduce the bias that you would have by selecting only one validation set.\\n\\n<br/>\\n\\n**What is K-fold cross-validation? üë∂**\\n\\nK fold cross validation is a method of cross validation where we select a hyperparameter k. The dataset is now divided into k parts. Now, we take the 1st part as validation set and remaining k-1 as training set. Then we take the 2nd part as validation set and remaining k-1 parts as training set. Like this, each part is used as validation set once and the remaining k-1 parts are taken together and used as training set.\\nIt should not be used in a time series data.\\n\\n<br/>\\n\\n**How do we choose K in K-fold cross-validation? What‚Äôs your favorite K? üë∂**\\n\\nThere are two things to consider while deciding K: the number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained. <br/>\\nI tend to use 4 for small datasets and 5 for large ones as K.\\n\\n<br/>\\n\\n',\n",
       " '## Classification\\n\\n**What is classification? Which models would you use to solve a classification problem? üë∂**\\n\\nClassification problems are problems in which our prediction space is discrete, i.e. there is a finite number of values the output variable can be. Some models which can be used to solve classification problems are: logistic regression, decision tree, random forests, multi-layer perceptron, one-vs-all, amongst others.\\n\\n<br/>\\n\\n**What is logistic regression? When do we need to use it? üë∂**\\n\\nLogistic regression is a Machine Learning algorithm that is used for binary classification. You should use logistic regression when your Y variable takes only two values, e.g. True and False, \"spam\" and \"not spam\", \"churn\" and \"not churn\" and so on. The variable is said to be a \"binary\" or \"dichotomous\".\\n\\n<br/>\\n\\n**Is logistic regression a linear model? Why? üë∂**\\n\\nYes, Logistic Regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters.\\n\\n<br/>\\n\\n**What is sigmoid? What does it do? üë∂**\\n\\nA sigmoid function is a type of activation function, and more specifically defined as a squashing function. Squashing functions limit the output to a range between 0 and 1, making these functions useful in the prediction of probabilities.\\n\\nSigmod(x) = 1/(1+e^{-x})\\n\\n<br/>\\n\\n**How do we evaluate classification models? üë∂**\\n\\nDepending on the classification problem, we can use the following evaluation metrics:\\n\\n1. Accuracy\\n2. Precision\\n3. Recall\\n4. F1 Score\\n5. Logistic loss (also known as Cross-entropy loss)\\n6. Jaccard similarity coefficient score\\n\\n<br/>\\n\\n**What is accuracy? üë∂**\\n\\nAccuracy is a metric for evaluating classification models. It is calculated by dividing the number of correct predictions by the number of total predictions.\\n\\n<br/>\\n\\n**Is accuracy always a good metric? üë∂**\\n\\nAccuracy is not a good performance metric when there is imbalance in the dataset. For example, in binary classification with 95% of A class and 5% of B class, a constant prediction of A class would have an accuracy of 95%. In case of imbalance dataset, we need to choose Precision, recall, or F1 Score depending on the problem we are trying to solve.\\n\\n<br/>\\n\\n**What is the confusion table? What are the cells in this table? üë∂**\\n\\nConfusion table (or confusion matrix) shows how many True positives (TP), True Negative (TN), False Positive (FP) and False Negative (FN) model has made.\\n\\n||                |     Actual   |        Actual |\\n|:---:|   :---:        |     :---:    |:---:          |\\n||                | Positive (1) | Negative (0)  |\\n|Predicted|   Positive (1) | TP           | FP            |\\n|Predicted|   Negative (0) | FN           | TN            |\\n\\n* True Positives (TP): When the actual class of the observation is 1 (True) and the prediction is 1 (True)\\n* True Negative (TN): When the actual class of the observation is 0 (False) and the prediction is 0 (False)\\n* False Positive (FP): When the actual class of the observation is 0 (False) and the prediction is 1 (True)\\n* False Negative (FN): When the actual class of the observation is 1 (True) and the prediction is 0 (False)\\n\\nMost of the performance metrics for classification models are based on the values of the confusion matrix.\\n\\n<br/>\\n\\n**What are precision, recall, and F1-score? üë∂**\\n\\n* Precision and recall are classification evaluation metrics:\\n* P = TP / (TP + FP) and R = TP / (TP + FN).\\n* Where TP is true positives, FP is false positives and FN is false negatives\\n* In both cases the score of 1 is the best: we get no false positives or false negatives and only true positives.\\n* F1 is a combination of both precision and recall in one score (harmonic mean):\\n* F1 = 2 * PR / (P + R).\\n* Max F score is 1 and min is 0, with 1 being the best.\\n\\n<br/>\\n\\n**Precision-recall trade-off \\u200d‚≠êÔ∏è**\\n\\nTradeoff means increasing one parameter would lead to decreasing of other. Precision-recall tradeoff occur due to increasing one of the parameter(precision or recall) while keeping the model same. \\n\\nIn an ideal scenario where there is a perfectly separable data, both precision and recall can get maximum value of 1.0. But in most of the practical situations, there is noise in the dataset and the dataset is not perfectly separable. There might be some points of positive class closer to the negative class and vice versa. In such cases, shifting the decision boundary can either increase the precision or recall but not both. Increasing one parameter leads to decreasing of the other. \\n\\n<br/>\\n\\n**What is the ROC curve? When to use it? \\u200d‚≠êÔ∏è**\\n\\nROC stands for *Receiver Operating Characteristics*. The diagrammatic representation that shows the contrast between true positive rate vs false positive rate. It is used when we need to predict the probability of the binary outcome.\\n\\n<br/>\\n\\n**What is AUC (AU ROC)? When to use it? \\u200d‚≠êÔ∏è**\\n\\nAUC stands for *Area Under the ROC Curve*. ROC is a probability curve and AUC represents degree or measure of separability. It\\'s used when we need to value how much model is capable of distinguishing between classes.  The value is between 0 and 1, the higher the better.\\n\\n<br/>\\n\\n**How to interpret the AU ROC score? \\u200d‚≠êÔ∏è**\\n\\nAUC score is the value of *Area Under the ROC Curve*. \\n\\nIf we assume ROC curve consists of dots, $`(x_1, y_1), (x_2, y_2), \\\\cdots, (x_m,y_m)`$, then\\n\\n$`AUC = \\\\frac{1}{2} \\\\sum_{i=1}^{m-1}(x_{i+1}-x_i)\\\\cdot (y_i+y_{i+1})`$\\n\\nAn excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. When AUC score is 0.5, it means model has no class separation capacity whatsoever. \\n\\n<br/>\\n\\n**What is the PR (precision-recall) curve? \\u200d‚≠êÔ∏è**\\n\\nA *precision*-*recall curve* (or PR Curve) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. Precision-recall curves (PR curves) are recommended for highly skewed domains where ROC curves may provide an excessively optimistic view of the performance.\\n\\n<br/>\\n\\n**What is the area under the PR curve? Is it a useful metric? \\u200d‚≠êÔ∏èI**\\n\\nThe Precision-Recall AUC is just like the ROC AUC, in that it summarizes the curve with a range of threshold values as a single score.\\n\\nA high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.\\n\\n<br/>\\n\\n**In which cases AU PR is better than AU ROC? \\u200d‚≠êÔ∏è**\\n\\nWhat is different however is that AU ROC looks at a true positive rate TPR and false positive rate FPR while AU PR looks at positive predictive value PPV and true positive rate TPR.\\n\\nTypically, if true negatives are not meaningful to the problem or you care more about the positive class, AU PR is typically going to be more useful; otherwise, If you care equally about the positive and negative class or your dataset is quite balanced, then going with AU ROC is a good idea.\\n\\n<br/>\\n\\n**What do we do with categorical variables? \\u200d‚≠êÔ∏è**\\n\\nCategorical variables must be encoded before they can be used as features to train a machine learning model. There are various encoding techniques, including:\\n- One-hot encoding\\n- Label encoding\\n- Ordinal encoding\\n- Target encoding\\n\\n<br/>\\n\\n**Why do we need one-hot encoding? \\u200d‚≠êÔ∏è**\\n\\nIf we simply encode categorical variables with a Label encoder, they become ordinal which can lead to undesirable consequences. In this case, linear models will treat category with id 4 as twice better than a category with id 2. One-hot encoding allows us to represent a categorical variable in a numerical vector space which ensures that vectors of each category have equal distances between each other. This approach is not suited for all situations, because by using it with categorical variables of high cardinality (e.g. customer id) we will encounter problems that come into play because of the curse of dimensionality.\\n\\n<br/>\\n\\n**What is \"curse of dimensionality\"? \\u200d‚≠êÔ∏è**\\n\\nThe curse of dimensionality is an issue that arises when working with high-dimensional data. It is often said that \"the curse of dimensionality\" is one of the main problems with machine learning. The curse of dimensionality refers to the fact that, as the number of dimensions (features) in a data set increases, the number of data points required to accurately learn the relationships between those features increases exponentially.\\n\\nA simple example where we have a data set with two features, x1 and x2. If we want to learn the relationship between these two features, we need to have enough data points so that we can accurately estimate the parameters of that relationship. However, if we add a third feature, x3, then the number of data points required to accurately learn the relationships between all three features increases exponentially. This is because there are now more parameters to estimate, and the number of data points needed to accurately estimate those parameters increases exponentially with the number of parameters.\\n\\nSimply put, the curse of dimensionality basically means that the error increases with the increase in the number of features.\\n\\n<br/>\\n\\n',\n",
       " \"## Regularization\\n\\n**What happens to our linear regression model if we have three columns in our data: x, y, z \\u200a‚Äî\\u200a and z is a sum of x and y? \\u200d‚≠êÔ∏è**\\n\\nWe would not be able to perform the regression. Because z is linearly dependent on x and y so when performing the regression $`{X}^{T}{X}`$ would be a singular (not invertible) matrix.\\n<br/>\\n\\n**What happens to our linear regression model if the column z in the data is a sum of columns x and y and some random noise? \\u200d‚≠êÔ∏è**\\n\\nIt creates a situation known as multicollinearity. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, leading to issues in the estimation of the regression coefficients. The issue arises because the variables X and Y will be highly correlated, making it difficult for the model to distinguish the individual effects of X and Y on the dependent variable Z. To address it, feature selection, PCA or regularization techniques (L2) may be used.\\n\\n<br/>\\n\\n**What is regularization? Why do we need it? üë∂**\\n\\nRegularization is used to reduce overfitting in machine learning models. It helps the models to generalize well and make them robust to outliers and noise in the data.\\n\\n<br/>\\n\\n**Which regularization techniques do you know? \\u200d‚≠êÔ∏è**\\n\\nThere are mainly two types of regularization,\\n1. L1 Regularization (Lasso regularization) - Adds the sum of absolute values of the coefficients to the cost function. $`\\\\lambda\\\\sum_{i=1}^{n} \\\\left | w_i \\\\right |`$\\n2. L2 Regularization (Ridge regularization) - Adds the sum of squares of coefficients to the cost function. $`\\\\lambda\\\\sum_{i=1}^{n} {w_{i}}^{2}`$\\n\\n* Where $`\\\\lambda`$ determines the amount of regularization.\\n\\n<br/>\\n\\n**What kind of regularization techniques are applicable to linear models? \\u200d‚≠êÔ∏è**\\n\\nAIC/BIC, Ridge regression, Lasso, Elastic Net, Basis pursuit denoising, Rudin‚ÄìOsher‚ÄìFatemi model (TV), Potts model, RLAD,\\nDantzig Selector,SLOPE\\n\\n<br/>\\n\\n**How does L2 regularization look like in a linear model? \\u200d‚≠êÔ∏è**\\n\\nL2 regularization adds a penalty term to our cost function which is equal to the sum of squares of models coefficients multiplied by a lambda hyperparameter. This technique makes sure that the coefficients are close to zero and is widely used in cases when we have a lot of features that might correlate with each other.\\n\\n<br/>\\n\\n**How do we select the right regularization parameters? üë∂**\\n\\nRegularization parameters can be chosen using a grid search, for example https://scikit-learn.org/stable/modules/linear_model.html has one formula for the implementing for regularization, alpha in the formula mentioned can be found by doing a RandomSearch or a GridSearch on a set of values and selecting the alpha which gives the least cross validation or validation error.\\n\\n\\n<br/>\\n\\n**What‚Äôs the effect of L2 regularization on the weights of a linear model? \\u200d‚≠êÔ∏è**\\n\\nL2 regularization penalizes larger weights more severely (due to the squared penalty term), which encourages weight values to decay toward zero.\\n\\n<br/>\\n\\n**How L1 regularization looks like in a linear model? \\u200d‚≠êÔ∏è**\\n\\nL1 regularization adds a penalty term to our cost function which is equal to the sum of modules of models coefficients multiplied by a lambda hyperparameter. For example, cost function with L1 regularization will look like: $`\\\\sum_{i=0}^{N} (y_i - \\\\sum_{j=0}^{M} x_{ij} * w_j)+\\\\lambda\\\\sum_{j=0}^{M} \\\\left | w_j \\\\right |`$\\n\\n<br/>\\n\\n**What‚Äôs the difference between L2 and L1 regularization? \\u200d‚≠êÔ∏è**\\n\\n- Penalty terms: L1 regularization uses the sum of the absolute values of the weights, while L2 regularization uses the sum of the weights squared.\\n- Feature selection: L1 performs feature selection by reducing the coefficients of some predictors to 0, while L2 does not.\\n- Computational efficiency: L2 has an analytical solution, while L1 does not.\\n- Multicollinearity: L2 addresses multicollinearity by constraining the coefficient norm.\\n\\n<br/>\\n\\n**Can we have both L1 and L2 regularization components in a linear model? \\u200d‚≠êÔ∏è**\\n\\nYes, elastic net regularization combines L1 and L2 regularization. \\n\\n<br/>\\n\\n**What‚Äôs the interpretation of the bias term in linear models? \\u200d‚≠êÔ∏è**\\n\\nBias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias.\\n\\n<br/>\\n\\n**How do we interpret weights in linear models? \\u200d‚≠êÔ∏è**\\n\\nWithout normalizing weights or variables, if you increase the corresponding predictor by one unit, the coefficient represents on average how much the output changes. By the way, this interpretation still works for logistic regression - if you increase the corresponding predictor by one unit, the weight represents the change in the log of the odds.\\n\\nIf the variables are normalized, we can interpret weights in linear models like the importance of this variable in the predicted result.\\n\\n<br/>\\n\\n**If a weight for one variable is higher than for another \\u200a‚Äî\\u200a can we say that this variable is more important? \\u200d‚≠êÔ∏è**\\n\\nYes - if your predictor variables are normalized.\\n\\nWithout normalization, the weight represents the change in the output per unit change in the predictor. If you have a predictor with a huge range and scale that is used to predict an output with a very small range - for example, using each nation's GDP to predict maternal mortality rates - your coefficient should be very small. That does not necessarily mean that this predictor variable is not important compared to the others.\\n\\n<br/>\\n\\n**When do we need to perform feature normalization for linear models? When it‚Äôs okay not to do it? \\u200d‚≠êÔ∏è**\\n\\nFeature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This can't be done effectively if every feature is scaled differently. \\n\\nLinear regression without regularization techniques can be used without feature normalization. Also, regularization can help to make the analytical solution more stable, ‚Äî it adds the regularization matrix to the feature matrix before inverting it. \\n\\n<br/>\\n\\n\",\n",
       " '## Feature selection\\n\\n**What is feature selection? Why do we need it? üë∂**\\n\\nFeature Selection is a method used to select the relevant features for the model to train on. We need feature selection to remove the irrelevant features which leads the model to under-perform.  \\n\\n<br/>\\n\\n**Is feature selection important for linear models? \\u200d‚≠êÔ∏è**\\n\\nYes, It is. It can make model performance better through selecting the most importance features and remove irrelevant features in order to make a prediction and it can also avoid overfitting, underfitting and bias-variance tradeoff. \\n\\n<br/>\\n\\n**Which feature selection techniques do you know? \\u200d‚≠êÔ∏è**\\n\\nHere are some of the feature selections:\\n- Principal Component Analysis\\n- Neighborhood Component Analysis\\n- ReliefF Algorithm\\n\\n<br/>\\n\\n**Can we use L1 regularization for feature selection? \\u200d‚≠êÔ∏è**\\n\\nYes, because the nature of L1 regularization will lead to sparse coefficients of features. Feature selection can be done by keeping only features with non-zero coefficients.\\n\\n<br/>\\n\\n**Can we use L2 regularization for feature selection? \\u200d‚≠êÔ∏è**\\n\\nNo, Because L2 regularization does not make the weights zero but only makes them very very small. L2 regularization can be used to solve multicollinearity since it stabilizes the model.\\n\\n<br/>\\n\\n',\n",
       " '## Decision trees\\n\\n**What are the decision trees? üë∂**\\n\\nThis is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. \\n\\nIn this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible.\\n\\nA decision tree is a flowchart-like tree structure, where each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a value for the target variable.\\n\\nVarious techniques : like Gini, Information Gain, Chi-square, entropy.\\n\\n<br/>\\n\\n**How do we train decision trees? \\u200d‚≠êÔ∏è**\\n\\n1. Start at the root node.\\n2. For each variable X, find the set S_1 that minimizes the sum of the node impurities in the two child nodes and choose the split {X*,S*} that gives the minimum over all X and S.\\n3. If a stopping criterion is reached, exit. Otherwise, apply step 2 to each child node in turn.\\n\\n<br/>\\n\\n**What are the main parameters of the decision tree model? üë∂**\\n\\n* maximum tree depth\\n* minimum samples per leaf node\\n* impurity criterion\\n\\n<br/>\\n\\n**How do we handle categorical variables in decision trees? \\u200d‚≠êÔ∏è**\\n\\nSome decision tree algorithms can handle categorical variables out of the box, others cannot. However, we can transform categorical variables, e.g. with a binary or a one-hot encoder.\\n\\n<br/>\\n\\n**What are the benefits of a single decision tree compared to more complex models? \\u200d‚≠êÔ∏è**\\n\\n* easy to implement\\n* fast training\\n* fast inference\\n* good explainability\\n\\n<br/>\\n\\n**How can we know which features are more important for the decision tree model? \\u200d‚≠êÔ∏è**\\n\\nOften, we want to find a split such that it minimizes the sum of the node impurities. The impurity criterion is a parameter of decision trees. Popular methods to measure the impurity are the Gini impurity and the entropy describing the information gain.\\n\\n<br/>\\n\\n',\n",
       " '## Random forest\\n\\n**What is random forest? üë∂**\\n\\nRandom Forest is a machine learning method for regression and classification which is composed of many decision trees. Random Forest belongs to a larger class of ML algorithms called ensemble methods (in other words, it involves the combination of several models to solve a single prediction problem).\\n\\n<br/>\\n\\n**Why do we need randomization in random forest? \\u200d‚≠êÔ∏è**\\n\\nRandom forest in an extension of the **bagging** algorithm which takes *random data samples from the training dataset* (with replacement), trains several models and averages predictions. In addition to that, each time a split in a tree is considered, random forest takes a *random sample of m features from full set of n features* (without replacement) and uses this subset of features as candidates for the split (for example, `m = sqrt(n)`).\\n\\nTraining decision trees on random data samples from the training dataset *reduces variance*. Sampling features for each split in a decision tree *decorrelates trees*.\\n\\n<br/>\\n\\n**What are the main parameters of the random forest model? \\u200d‚≠êÔ∏è**\\n\\n- `max_depth`: Longest Path between root node and the leaf\\n- `min_sample_split`: The minimum number of observations needed to split a given node\\n- `max_leaf_nodes`: Conditions the splitting of the tree and hence, limits the growth of the trees\\n- `min_samples_leaf`: minimum number of samples in the leaf node\\n- `n_estimators`: Number of trees\\n- `max_sample`: Fraction of original dataset given to any individual tree in the given model\\n- `max_features`: Limits the maximum number of features provided to trees in random forest model\\n\\n<br/>\\n\\n**How do we select the depth of the trees in random forest? \\u200d‚≠êÔ∏è**\\n\\nThe greater the depth, the greater amount of information is extracted from the tree, however, there is a limit to this, and the algorithm even if defensive against overfitting may learn complex features of noise present in data and as a result, may overfit on noise. Hence, there is no hard thumb rule in deciding the depth, but literature suggests a few tips on tuning the depth of the tree to prevent overfitting:\\n\\n- limit the maximum depth of a tree\\n- limit the number of test nodes\\n- limit the minimum number of objects at a node required to split\\n- do not split a node when, at least, one of the resulting subsample sizes is below a given threshold\\n- stop developing a node if it does not sufficiently improve the fit.\\n\\n<br/>\\n\\n**How do we know how many trees we need in random forest? \\u200d‚≠êÔ∏è**\\n\\nThe number of trees in random forest is worked by n_estimators, and a random forest reduces overfitting by increasing the number of trees. There is no fixed thumb rule to decide the number of trees in a random forest, it is rather fine tuned with the data, typically starting off by taking the square of the number of features (n) present in the data followed by tuning until we get the optimal results.\\n\\n<br/>\\n\\n**Is it easy to parallelize training of a random forest model? How can we do it? \\u200d‚≠êÔ∏è**\\n\\nYes, R provides a simple way to parallelize training of random forests on large scale data.\\nIt makes use of a parameter called multicombine which can be set to TRUE for parallelizing random forest computations.\\n\\n```R\\nrf <- foreach(ntree=rep(25000, 6), .combine=randomForest::combine,\\n              .multicombine=TRUE, .packages=\\'randomForest\\') %dopar% {\\n    randomForest(x, y, ntree=ntree)\\n}\\n```\\n\\n\\n<br/>\\n\\n**What are the potential problems with many large trees? \\u200d‚≠êÔ∏è**\\n\\n- Overfitting: A large number of large trees can lead to overfitting, where the model becomes too complex and is able to memorize the training data but doesn\\'t generalize well to new, unseen data.\\n\\n- Slow prediction time: As the number of trees in the forest increases, the prediction time for new data points can become quite slow. This can be a problem when you need to make predictions in real-time or on a large dataset.\\n\\n- Memory consumption: Random Forest models with many large trees can consume a lot of memory, which can be a problem when working with large datasets or on a limited hardware.\\n\\n- Lack of interpretability: Random Forest models with many large trees can be difficult to interpret, making it harder to understand how the model is making predictions or what features are most important.\\n\\n- Difficulty in tuning : With an increasing number of large trees the tuning process becomes more complex and computationally expensive.\\n\\nIt\\'s important to keep in mind that the number of trees in a Random Forest should be chosen based on the specific problem and dataset, rather than using a large number of trees by default. In practice, the number of trees in a random forest is chosen based on the trade-off between the computational cost and the performance.\\n\\n<br/>\\n\\n**What if instead of finding the best split, we randomly select a few splits and just select the best from them. Will it work? üöÄ**\\n\\nAnswer here\\n\\n<br/>\\n\\n**What happens when we have correlated features in our data? \\u200d‚≠êÔ∏è**\\n\\nIn random forest, since random forest samples some features to build each tree, the information contained in correlated features is twice as much likely to be picked than any other information contained in other features. \\n\\nIn general, when you are adding correlated features, it means that they linearly contains the same information and thus it will reduce the robustness of your model. Each time you train your model, your model might pick one feature or the other to \"do the same job\" i.e. explain some variance, reduce entropy, etc.\\n\\n<br/>\\n\\n',\n",
       " '## Gradient boosting\\n\\n**What is gradient boosting trees? \\u200d‚≠êÔ∏è**\\n\\nGradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\\n\\n<br/>\\n\\n**What‚Äôs the difference between random forest and gradient boosting? \\u200d‚≠êÔ∏è**\\n\\n   1. Random Forests builds each tree independently while Gradient Boosting builds one tree at a time.\\n   2. Random Forests combine results at the end of the process (by averaging or \"majority rules\") while Gradient Boosting combines     results along the way.\\n\\n<br/>\\n\\n**Is it possible to parallelize training of a gradient boosting model? How to do it? \\u200d‚≠êÔ∏è**\\n\\nYes, different frameworks provide different options to make training faster, using GPUs to speed up the process by making it highly parallelizable.For example, for XGBoost <i>tree_method = \\'gpu_hist\\'</i> option makes training faster by use of GPUs. \\n\\n<br/>\\n\\n**Feature importance in gradient boosting trees \\u200a‚Äî\\u200a what are possible options? \\u200d‚≠êÔ∏è**\\n\\nWith CatBoost you can use implemented method get_feature_importance for getting SHAP values. https://arxiv.org/abs/1905.04610v1\\n\\nIt allows to understand how excluding features helps to provide better results. Higher value is better.\\n\\nAlso you can add random noise column to your data (with normal distribution), calculate feature importance and exclude all features below noise importance.\\n\\n<br/>\\n\\n**Are there any differences between continuous and discrete variables when it comes to feature importance of gradient boosting models? üöÄ**\\n\\nAnswer here\\n\\n<br/>\\n\\n**What are the main parameters in the gradient boosting model? \\u200d‚≠êÔ∏è**\\n\\nThere are many parameters, but below are a few key defaults.\\n* learning_rate=0.1 (shrinkage).\\n* n_estimators=100 (number of trees).\\n* max_depth=3.\\n* min_samples_split=2.\\n* min_samples_leaf=1.\\n* subsample=1.0.\\n\\n<br/>\\n\\n**How do you approach tuning parameters in XGBoost or LightGBM? üöÄ**\\n\\nDepending upon the dataset, parameter tuning can be done manually or using hyperparameter optimization frameworks such as optuna and hyperopt. In manual parameter tuning, we need to be aware of max-depth, min_samples_leaf and min_samples_split so that our model does not overfit the data but try to predict generalized characteristics of data (basically keeping variance and bias low for our model).\\n\\n<br/>\\n\\n**How do you select the number of trees in the gradient boosting model? \\u200d‚≠êÔ∏è**\\n\\nMost implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds or thousands. Using scikit-learn we can perform a grid search of the n_estimators model parameter\\n\\n<br/>\\n\\n\\n',\n",
       " '## Parameter tuning\\n\\n**Which hyper-parameter tuning strategies (in general) do you know? \\u200d‚≠êÔ∏è**\\n\\nThere are several strategies for hyper-tuning but I would argue that the three most popular nowadays are the following:\\n* <b>Grid Search</b> is an exhaustive approach such that for each hyper-parameter, the user needs to <i>manually</i> give a list of values for the algorithm to try. After these values are selected, grid search then evaluates the algorithm using each and every combination of hyper-parameters and returns the combination that gives the optimal result (i.e. lowest MAE). Because grid search evaluates the given algorithm using all combinations, it\\'s easy to see that this can be quite computationally expensive and can lead to sub-optimal results specifically since the user needs to specify specific values for these hyper-parameters, which is prone for error and requires domain knowledge.\\n\\n* <b>Random Search</b> is similar to grid search but differs in the sense that rather than specifying which values to try for each hyper-parameter, an upper and lower bound of values for each hyper-parameter is given instead. With uniform probability, random values within these bounds are then chosen and similarly, the best combination is returned to the user. Although this seems less intuitive, no domain knowledge is necessary and theoretically much more of the parameter space can be explored.\\n\\n* In a completely different framework, <b>Bayesian Optimization</b> is thought of as a more statistical way of optimization and is commonly used when using neural networks, specifically since one evaluation of a neural network can be computationally costly. In numerous research papers, this method heavily outperforms Grid Search and Random Search and is currently used on the Google Cloud Platform as well as AWS. Because an in-depth explanation requires a heavy background in bayesian statistics and gaussian processes (and maybe even some game theory), a \"simple\" explanation is that a much simpler/faster <i>acquisition function</i> intelligently chooses (using a <i>surrogate function</i> such as probability of improvement or GP-UCB) which hyper-parameter values to try on the computationally expensive, original algorithm. Using the result of the initial combination of values on the expensive/original function, the acquisition function takes the result of the expensive/original algorithm into account and uses it as its prior knowledge to again come up with another set of hyper-parameters to choose during the next iteration. This process continues either for a specified number of iterations or for a specified amount of time and similarly the combination of hyper-parameters that performs the best on the expensive/original algorithm is chosen.\\n\\n\\n<br/>\\n\\n**What‚Äôs the difference between grid search parameter tuning strategy and random search? When to use one or another? \\u200d‚≠êÔ∏è**\\n\\nFor specifics, refer to the above answer.\\n\\n<br/>\\n\\n',\n",
       " '## Neural networks\\n\\n**What kind of problems neural nets can solve? üë∂**\\n\\nNeural nets are good at solving non-linear problems. Some good examples are problems that are relatively easy for humans (because of experience, intuition, understanding, etc), but difficult for traditional regression models: speech recognition, handwriting recognition, image identification, etc.\\n\\n<br/>\\n\\n**How does a usual fully-connected feed-forward neural network work? \\u200d‚≠êÔ∏è**\\n\\nIn a usual fully-connected feed-forward network, each neuron receives input from every element of the previous layer and thus the receptive field of a neuron is the entire previous layer. They are usually used to represent feature vectors for input data in classification problems but can be expensive to train because of the number of computations involved.\\n\\n<br/>\\n\\n**Why do we need activation functions? üë∂**\\n\\nThe main idea of using neural networks is to learn complex nonlinear functions. If we are not using an activation function in between different layers of a neural network, we are just stacking up multiple linear layers one on top of another and this leads to learning a linear function. The Nonlinearity comes only with the activation function, this is the reason we need activation functions.\\n\\n<br/>\\n\\n**What are the problems with sigmoid as an activation function? \\u200d‚≠êÔ∏è**\\n\\nThe derivative of the sigmoid function for large positive or negative numbers is almost zero. From this comes the problem of vanishing gradient ‚Äî during the backpropagation our net will not learn (or will learn drastically slow). One possible way to solve this problem is to use ReLU activation function.\\n\\n<br/>\\n\\n**What is ReLU? How is it better than sigmoid or tanh? \\u200d‚≠êÔ∏è**\\n\\nReLU is an abbreviation for Rectified Linear Unit. It is an activation function which has the value 0 for all negative values and the value f(x) = x for all positive values. The ReLU has a simple activation function which makes it fast to compute and while the sigmoid and tanh activation functions saturate at higher values, the ReLU has a potentially infinite activation, which addresses the problem of vanishing gradients. \\n\\n<br/>\\n\\n**How we can initialize the weights of a neural network? \\u200d‚≠êÔ∏è**\\n\\nProper initialization of weight matrix in neural network is very necessary.\\nSimply we can say there are two ways for initializations.\\n   1. Initializing weights with zeroes.\\n      Setting weights to zero makes your network no better than a linear model. It is important to note that setting biases to 0 will not create any troubles as non zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron are still different.  \\n   2. Initializing weights randomly.\\n      Assigning random values to weights is better than just 0 assignment. \\n* a) If weights are initialized with very high values the term np.dot(W,X)+b becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where the slope of gradient changes slowly and learning takes a lot of time.\\n* b) If weights are initialized with low values it gets mapped to 0, where the case is the same as above. This problem is often referred to as the vanishing gradient.\\n      \\n<br/>\\n\\n**What if we set all the weights of a neural network to 0? \\u200d‚≠êÔ∏è**\\n\\nIf all the weights of a neural network are set to zero, the output of each connection is same (W*x = 0). This means the gradients which are backpropagated to each connection in a layer is same. This means all the connections/weights learn the same thing, and the model never converges. \\n\\n<br/>\\n\\n**What regularization techniques for neural nets do you know? \\u200d‚≠êÔ∏è**\\n\\n* L1 Regularization - Defined as the sum of absolute values of the individual parameters. The L1 penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded. \\n* L2 Regularization - Defined as the sum of square of individual parameters. Often supported by regularization hyperparameter alpha. It results in weight decay. \\n* Data Augmentation - This requires some fake data to be created as a part of training set. \\n* Drop Out : This is most effective regularization technique for neural nets. Few random nodes in each layer is deactivated in forward pass. This allows the algorithm to train on different set of nodes in each iterations.\\n<br/>\\n\\n**What is dropout? Why is it useful? How does it work? \\u200d‚≠êÔ∏è**\\n\\nDropout is a technique that at each training step turns off each neuron with a certain probability of *p*. This way at each iteration we train only *1-p* of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter *p*.  \\n\\n<br/>\\n\\n',\n",
       " '## Optimization in neural\\xa0networks\\n\\n**What is backpropagation? How does it work? Why do we need it? \\u200d‚≠êÔ∏è**\\n\\nThe Backpropagation algorithm looks for the minimum value of the error function in weight space using a technique called the delta rule or gradient descent. \\nThe weights that minimize the error function is then considered to be a solution to the learning problem. \\n\\nWe need backpropogation because,\\n* Calculate the error ‚Äì How far is your model output from the actual output.\\n* Minimum Error ‚Äì Check whether the error is minimized or not.\\n* Update the parameters ‚Äì If the error is huge then, update the parameters (weights and biases). After that again check the error.  \\nRepeat the process until the error becomes minimum.\\n* Model is ready to make a prediction ‚Äì Once the error becomes minimum, you can feed some inputs to your model and it will produce the output.\\n\\n<br/>\\n\\n**Which optimization techniques for training neural nets do you know? \\u200d‚≠êÔ∏è**\\n\\n* Gradient Descent\\n* Stochastic Gradient Descent\\n* Mini-Batch Gradient Descent(best among gradient descents)\\n* Nesterov Accelerated Gradient\\n* Momentum\\n* Adagrad \\n* AdaDelta\\n* Adam(best one. less time, more efficient)\\n\\n<br/>\\n\\n**How do we use SGD (stochastic gradient descent) for training a neural net? \\u200d‚≠êÔ∏è**\\n\\nSGD approximates the expectation with few randomly selected samples (instead of the full data). In comparison to batch gradient descent, we can efficiently approximate the expectation in large data sets using SGD. For neural networks this reduces the training time a lot even considering that it will converge later as the random sampling adds noise to the gradient descent.\\n\\n<br/>\\n\\n**What‚Äôs the learning rate? üë∂**\\n\\nThe learning rate is an important hyperparameter that controls how quickly the model is adapted to the problem during the training. It can be seen as the \"step width\" during the parameter updates, i.e. how far the weights are moved into the direction of the minimum of our optimization problem.\\n\\n<br/>\\n\\n**What happens when the learning rate is too large? Too small? üë∂**\\n\\nA large learning rate can accelerate the training. However, it is possible that we \"shoot\" too far and miss the minimum of the function that we want to optimize, which will not result in the best solution. On the other hand, training with a small learning rate takes more time but it is possible to find a more precise minimum. The downside can be that the solution is stuck in a local minimum, and the weights won\\'t update even if it is not the best possible global solution.\\n\\n<br/>\\n\\n**How to set the learning rate? \\u200d‚≠êÔ∏è**\\n\\nThere is no straightforward way of finding an optimum learning rate for a model. It involves a lot of hit and trial. Usually starting with a small values such as 0.01 is a good starting point for setting a learning rate and further tweaking it so that it doesn\\'t overshoot or converge too slowly.\\n\\n<br/>\\n\\n**What is Adam? What‚Äôs the main difference between Adam and SGD? \\u200d‚≠êÔ∏è**\\n\\nAdam (Adaptive Moment Estimation) is a optimization technique for training neural networks. on an average, it is the best optimizer .It works with momentums of first and second order. The intuition behind the Adam is that we don‚Äôt want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.\\n\\nAdam tends to converge faster, while SGD often converges to more optimal solutions.\\nSGD\\'s high variance disadvantages gets rectified by Adam (as advantage for Adam).\\n\\n<br/>\\n\\n**When would you use Adam and when SGD? \\u200d‚≠êÔ∏è**\\n\\nAdam tends to converge faster, while SGD often converges to more optimal solutions.\\n\\n<br/>\\n\\n**Do we want to have a constant learning rate or we better change it throughout training? \\u200d‚≠êÔ∏è**\\n\\nGenerally, it is recommended to start learning rate with relatively high value and then gradually decrease learning rate so the model does not overshoot the minima and at the same time we don\\'t want to start with very low learning rate as the model will take too long to converge. There are many available techniques to do decay the learning rate. For example, in PyTorch you can use\\na function called **StepLR** which decays the learning rate of each parameter by value **gamma**-which we have to pass through argument- after n number of epoch which you can also set through function argument named **epoch_size**. \\n\\n<br/>\\n\\n**How do we decide when to stop training a neural net? üë∂**\\n\\nSimply stop training when the validation error is the minimum.\\n\\n<br/>\\n\\n**What is model checkpointing? \\u200d‚≠êÔ∏è**\\n\\nSaving the weights learned by a model mid training for long running processes is known as model checkpointing so that you can resume your training from a certain checkpoint.\\n\\n<br/>\\n\\n**Can you tell us how you approach the model training process? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n',\n",
       " \"## Neural networks for computer\\xa0vision\\n\\n**How we can use neural nets for computer vision? \\u200d‚≠êÔ∏è**\\n\\nNeural nets used in the area of computer vision are generally Convolutional Neural Networks(CNN's). You can learn about convolutions below. It appears that convolutions are quite powerful when it comes to working with images and videos due to their ability to extract and learn complex features. Thus CNN's are a go-to method for any problem in computer vision.    \\n\\n<br/>\\n\\n**What‚Äôs a convolutional layer? \\u200d‚≠êÔ∏è**\\n\\nThe idea of the convolutional layer is the assumption that the information needed for making a decision often is spatially close and thus, it only takes the weighted sum over nearby inputs. It also assumes that the networks‚Äô kernels can be reused for all nodes, hence the number of weights can be drastically reduced. To counteract only one feature being learnt per layer, multiple kernels are applied to the input which creates parallel channels in the output. Consecutive layers can also be stacked to allow the network to find more high-level features.\\n\\n<br/>\\n\\n**Why do we actually need convolutions? Can‚Äôt we use fully-connected layers for that? \\u200d‚≠êÔ∏è**\\n\\nA fully-connected layer needs one weight per inter-layer connection, which means the number of weights which needs to be computed quickly balloons as the number of layers and nodes per layer is increased. \\n\\n<br/>\\n\\n**What‚Äôs pooling in CNN? Why do we need it? \\u200d‚≠êÔ∏è**\\n\\nPooling is a technique to downsample the feature map. It allows layers which receive relatively undistorted versions of the input to learn low level features such as lines, while layers deeper in the model can learn more abstract features such as texture.\\n\\n<br/>\\n\\n**How does max pooling work? Are there other pooling techniques? \\u200d‚≠êÔ∏è**\\n\\nMax pooling is a technique where the maximum value of a receptive field is passed on in the next feature map. The most commonly used receptive field is 2 x 2 with a stride of 2, which means the feature map is downsampled from N x N to N/2 x N/2. Receptive fields larger than 3 x 3 are rarely employed as too much information is lost. \\n\\nOther pooling techniques include:\\n\\n* Average pooling, the output is the average value of the receptive field.\\n* Min pooling, the output is the minimum value of the receptive field.\\n* Global pooling, where the receptive field is set to be equal to the input size, this means the output is equal to a scalar and can be used to reduce the dimensionality of the feature map. \\n\\n<br/>\\n\\n**Are CNNs resistant to rotations? What happens to the predictions of a CNN if an image is rotated? üöÄ**\\n\\nCNNs are not resistant to rotation by design. However, we can make our models resistant by augmenting our datasets with different rotations of the raw data. The predictions of a CNN will change if an image is rotated and we did not augment our dataset accordingly. A demonstration of this occurence can be seen in [this video](https://www.youtube.com/watch?v=VO1bQo4PXV4), where a CNN changes its predicted class between a duck and a rabbit based on the rotation of the image.\\n\\n<br/>\\n\\n**What are augmentations? Why do we need them? üë∂**\\n\\nAugmentations are an artifical way of expanding the existing datasets by performing some transformations, color shifts or many other things on the data. It helps in diversifying the data and even increasing the data when there is scarcity of data for a model to train on.  \\n\\n<br/>\\n\\n**What kind of augmentations do you know? üë∂**\\n\\nThere are many kinds of augmentations which can be used according to the type of data you are working on some of which are geometric and numerical transformation, PCA, cropping, padding, shifting, noise injection etc.\\n\\n<br/>\\n\\n**How to choose which augmentations to use? \\u200d‚≠êÔ∏è**\\n\\nAugmentations really depend on the type of output classes and the features you want your model to learn. For eg. if you have mostly properly illuminated images in your dataset and want your model to predict poorly illuminated images too, you can apply channel shifting on your data and include the resultant images in your dataset for better results.\\n\\n<br/>\\n\\n**What kind of CNN architectures for classification do you know? üöÄ**\\n\\nImage Classification\\n* Inception v3\\n* Xception \\n* DenseNet\\n* AlexNet\\n* VGG16\\n* ResNet\\n* SqueezeNet\\n* EfficientNet\\n* MobileNet\\n\\nThe last three are designed so they use smaller number of parameters which is helpful for edge AI. \\n\\n<br/>\\n\\n**What is transfer learning? How does it work? \\u200d‚≠êÔ∏è**\\n\\nGiven a source domain D_S and learning task T_S, a target domain D_T and learning task T_T, transfer learning aims to help improve the learning of the target predictive function f_T in D_T using the knowledge in D_S and T_S, where D_S ‚â† D_T,or T_S ‚â† T_T. In other words, transfer learning enables to reuse knowledge coming from other domains or learning tasks.\\n\\nIn the context of CNNs, we can use networks that were pre-trained on popular datasets such as ImageNet. We then can use the weights of the layers that learn to represent features and combine them with a new set of layers that learns to map the feature representations to the given classes. Two popular strategies are either to freeze the layers that learn the feature representations completely, or to give them a smaller learning rate.\\n\\n<br/>\\n\\n**What is object detection? Do you know any architectures for that? üöÄ**\\n\\nObject detection is finding Bounding Boxes around objects in an image. \\nArchitectures :\\nYOLO, Faster RCNN, Center Net\\n\\n<br/>\\n\\n**What is object segmentation? Do you know any architectures for that? üöÄ**\\n\\nObject Segmentation is predicting masks. It does not differentiate objects. \\nArchitectures :\\nMask RCNN, UNet\\n\\n<br/>\\n\\n\",\n",
       " '## Text classification\\n\\n**How can we use machine learning for text classification? \\u200d‚≠êÔ∏è**\\n\\nMachine learning classification algorithms predict a class based on a numerical feature representation. This means that in order to use machine learning for text classification, we need to extract numerical features from our text data first before we can apply machine learning algorithms. Common approaches to extract numerical features from text data are bag of words, N-grams or word embeddings.\\n\\n<br/>\\n\\n**What is bag of words? How we can use it for text classification? \\u200d‚≠êÔ∏è**\\n\\nBag of Words is a representation of text that describes the occurrence of words within a document. The order or structure of the words is not considered. For text classification, we look at the histogram of the words within the text and consider each word count as a feature.\\n\\n<br/>\\n\\n**What are the advantages and disadvantages of bag of words? \\u200d‚≠êÔ∏è**\\n\\nAdvantages:\\n1. Simple to understand and implement.\\n\\nDisadvantages:\\n1. The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\\n2. Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons\\n3. Discarding word order ignores the context, and in turn meaning of words in the document. Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (‚Äúthis is interesting‚Äù vs ‚Äúis this interesting‚Äù), synonyms (‚Äúold bike‚Äù vs ‚Äúused bike‚Äù).\\n\\n<br/>\\n\\n**What are N-grams? How can we use them? \\u200d‚≠êÔ∏è**\\n\\nThe function to tokenize into consecutive sequences of words is called n-grams. It can be used to find out N most co-occurring words (how often word X is followed by word Y) in a given sentence.\\n\\n<br/>\\n\\n**How large should be N for our bag of words when using N-grams? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n**What is TF-IDF? How is it useful for text classification? \\u200d‚≠êÔ∏è**\\n\\nTerm Frequency (TF) is a scoring of the frequency of the word in the current document. Inverse Document Frequency(IDF) is a scoring of how rare the word is across documents. It is used in scenario where highly recurring words may not contain as much informational content as the domain specific words. For example, words like ‚Äúthe‚Äù that are frequent across all documents therefore need to be less weighted. The TF-IDF score highlights words that are distinct (contain useful information) in a given document.  \\n\\n<br/>\\n\\n**Which model would you use for text classification with bag of words features? \\u200d‚≠êÔ∏è**\\n\\n1. Bag Of Words model\\n2. Word2Vec Embeddings\\n3. fastText Embeddings\\n4. Convolutional Neural Networks (CNN)\\n5. Long Short-Term Memory (LSTM)\\n6. Bidirectional Encoder Representations from Transformers (BERT)\\n\\n<br/>\\n\\n**Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words? \\u200d‚≠êÔ∏è**\\n\\nUsually logistic regression is better because bag of words creates a matrix with large number of columns. For a huge number of columns logistic regression is usually faster than gradient boosting trees.\\n\\n<br/>\\n\\n**What are word embeddings? Why are they useful? Do you know Word2Vec? \\u200d‚≠êÔ∏è**\\n\\nWord Embeddings are vector representations for words. Each word is mapped to one vector, this vector tries to capture some characteristics of the word, allowing similar words to have similar vector representations.  \\nWord Embeddings helps in capturing the inter-word semantics and represents it in real-valued vectors.  \\n\\n<br/>\\n\\nWord2Vec is a method to construct such an embedding. It takes a text corpus as input and outputs a set of vectors which represents words in that corpus.  \\n\\nIt can be generated using two methods:\\n\\n- Common Bag of Words (CBOW)\\n- Skip-Gram\\n\\n<br/>\\n\\n**Do you know any other ways to get word embeddings? üöÄ**\\n\\n- GloVe\\n- BERT\\n<br/>\\n\\n**If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it? \\u200d‚≠êÔ∏è**\\n\\nApproaches ranked from simple to more complex:\\n\\n1. Take an average over all words\\n2. Take a weighted average over all words. Weighting can be done by inverse document frequency (idf part of tf-idf).\\n3. Use ML model like LSTM or Transformer.\\n\\n<br/>\\n\\n**Would you prefer gradient boosting trees model or logistic regression when doing text classification with embeddings? \\u200d‚≠êÔ∏è**\\n\\nGradient boosting trees (GBTs) are generally a better choice than logistic regression for text classification with embeddings. This is because GBTs are able to learn more complex relationships between the features in the data, including the features extracted from the embeddings.\\n\\nLogistic regression is a linear model, which means that it can only learn linear relationships between the features. This can be a limitation for text classification, where the relationships between the features are often complex and non-linear.\\n\\nGBTs, on the other hand, are able to learn non-linear relationships between the features by combining multiple decision trees. This allows GBTs to learn more complex patterns in the data, which can lead to better performance on text classification tasks.\\n\\nIn addition, GBTs are more robust to outliers and noise in the data than logistic regression. This can be important for text classification tasks, where the data can be noisy and imbalanced.\\n\\nOverall, GBTs are a better choice than logistic regression for text classification with embeddings, especially when the data is noisy or imbalanced. However, it is important to consider the computational cost and interpretability of GBTs before using them.\\n\\n<br/>\\n\\n**How can you use neural nets for text classification? üöÄ**\\n\\nHere is a general overview of how to use neural nets for text classification:\\n\\nPreprocess the text: This includes cleaning the text by removing stop words, punctuation, and other irrelevant symbols. It may also involve converting the text to lowercase and stemming or lemmatizing the words.\\nRepresent the text as a vector: This can be done using a variety of methods, such as one-hot encoding or word embeddings.\\nBuild the neural net: The neural net architecture will depend on the specific text classification task. However, a typical architecture will include an embedding layer, one or more hidden layers, and an output layer.\\nTrain the neural net: The neural net is trained by feeding it labeled examples of text data. The neural net will learn to adjust its parameters in order to minimize the loss function, which is typically the cross-entropy loss function.\\nEvaluate the neural net: Once the neural net is trained, it can be evaluated on a held-out test set to assess its performance.\\nHere are some specific examples of how neural nets can be used for text classification:\\nSentiment analysis, Spam detection, Topic classification, Language identification\\n\\nNeural nets have achieved state-of-the-art results on many text classification tasks. However, they can be computationally expensive to train and deploy. \\n\\n<br/>\\n\\n**How can we use CNN for text classification? üöÄ**\\n\\nHere are some specific examples of how CNNs can be used for text classification:\\n\\nSentiment analysis: CNNs can be used to classify text as positive, negative, or neutral sentiment. This is a common task in social media analysis and customer service.\\nSpam detection: CNNs can be used to classify emails as spam or not spam. This is a common task in email filtering systems.\\nTopic classification: CNNs can be used to classify text documents into different topics. This is a common task in news and social media analysis.\\nLanguage identification: CNNs can be used to identify the language of a text document. This is a common task in translation systems.\\n\\n<br/>\\n\\n',\n",
       " '## Clustering\\n\\n**What is unsupervised learning? üë∂**\\n\\nUnsupervised learning aims to detect patterns in data where no labels are given.\\n\\n<br/>\\n\\n**What is clustering? When do we need it? üë∂**\\n\\nClustering algorithms group objects such that similar feature points are put into the same groups (clusters) and dissimilar feature points are put into different clusters.\\n\\n<br/>\\n\\n**Do you know how K-means works? \\u200d‚≠êÔ∏è**\\n\\n1. Partition points into k subsets.\\n2. Compute the seed points as the new centroids of the clusters of the current partitioning.\\n3. Assign each point to the cluster with the nearest seed point.\\n4. Go back to step 2 or stop when the assignment does not change.\\n\\n<br/>\\n\\n**How to select K for K-means? \\u200d‚≠êÔ∏è**\\n\\n* Domain knowledge, i.e. an expert knows the value of k\\n* Elbow method: compute the clusters for different values of k, for each k, calculate the total within-cluster sum of square, plot the sum according to the number of clusters and use the band as the number of clusters.\\n* Average silhouette method: compute the clusters for different values of k, for each k, calculate the average silhouette of observations, plot the silhouette according to the number of clusters and select the maximum as the number of clusters.\\n\\n<br/>\\n\\n**What are the other clustering algorithms do you know? \\u200d‚≠êÔ∏è**\\n\\n* k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise.\\n* Agglomerative Hierarchical Clustering (AHC): hierarchical clusters combining the nearest clusters starting with each point as its own cluster.\\n* DIvisive ANAlysis Clustering (DIANA): hierarchical clustering starting with one cluster containing all points and splitting the clusters until each point describes its own cluster.\\n* Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Cluster defined as maximum set of density-connected points.\\n\\n<br/>\\n\\n**Do you know how DBScan works? \\u200d‚≠êÔ∏è**\\n\\n* Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood)\\n* Cluster defined as maximum set of density-connected points.\\n* Points p_j and p_i are density-connected w.r.t. epsilon and minPts if there is a point o such that both, i and j are density-reachable from o w.r.t. epsilon and minPts.\\n* p_j is density-reachable from p_i w.r.t. epsilon, minPts if there is a chain of points p_i -> p_i+1 -> p_i+x = p_j such that p_i+x is directly density-reachable from p_i+x-1.\\n* p_j is a directly density-reachable point of the neighborhood of p_i if dist(p_i,p_j) <= epsilon.\\n\\n<br/>\\n\\n**When would you choose K-means and when DBScan? \\u200d‚≠êÔ∏è**\\n\\n* DBScan is more robust to noise.\\n* DBScan is better when the amount of clusters is difficult to guess.\\n* K-means has a lower complexity, i.e. it will be much faster, especially with a larger amount of points.\\n\\n<br/>\\n\\n',\n",
       " '## Dimensionality reduction\\n**What is the curse of dimensionality? Why do we care about it? \\u200d‚≠êÔ∏è**\\n\\nData in only one dimension is relatively tightly packed. Adding a dimension stretches the points across that dimension, pushing them further apart. Additional dimensions spread the data even further making high dimensional data extremely sparse. We care about it, because it is difficult to use machine learning in sparse spaces.\\n\\n<br/>\\n\\n**Do you know any dimensionality reduction techniques? \\u200d‚≠êÔ∏è**\\n\\n* Singular Value Decomposition (SVD)\\n* Principal Component Analysis (PCA)\\n* Linear Discriminant Analysis (LDA)\\n* T-distributed Stochastic Neighbor Embedding (t-SNE)\\n* Autoencoders\\n* Fourier and Wavelet Transforms\\n\\n<br/>\\n\\n**What‚Äôs singular value decomposition? How is it typically used for machine learning? \\u200d‚≠êÔ∏è**\\n\\n* Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Œ£ (diagonal matrix) and R^T (right singular values).\\n* For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive.\\n* Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction.\\n\\n<br/>\\n\\n',\n",
       " '## Ranking and\\xa0search\\n\\n**What is the ranking problem? Which models can you use to solve them? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n**What are good unsupervised baselines for text information retrieval? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n**How would you evaluate your ranking algorithms? Which offline metrics would you use? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n**What is precision and recall at k? \\u200d‚≠êÔ∏è**\\n\\nPrecision at k and recall at k are evaluation metrics for ranking algorithms. Precision at k shows the share of relevant items in the first *k* results of the ranking algorithm. And Recall at k indicates the share of relevant items returned in top *k* results out of all correct answers for a given query.\\n\\nExample:\\nFor a search query \"Car\" there are 3 relevant products in your shop. Your search algorithm returns 2 of those relevant products in the first 5 search results.\\nPrecision at 5 = # num of relevant products in search result / k = 2/5 = 40%\\nRecall at 5 = # num of relevant products in search result / # num of all relevant products = 2/3 = 66.6%\\n\\n<br/>\\n\\n**What is mean average precision at k? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n**How can we use machine learning for search? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n**How can we get training data for our ranking algorithms? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n**Can we formulate the search problem as a classification problem? How? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n**How can we use clicks data as the training data for ranking algorithms? üöÄ**\\n\\nAnswer here\\n\\n<br/>\\n\\n**Do you know how to use gradient boosting trees for ranking? üöÄ**\\n\\nAnswer here\\n\\n<br/>\\n\\n**How do you do an online evaluation of a new ranking algorithm? \\u200d‚≠êÔ∏è**\\n\\nAnswer here\\n\\n<br/>\\n\\n',\n",
       " \"## Recommender systems\\n\\n**What is a recommender system? üë∂**\\n\\nRecommender systems are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user.\\n\\n<br/>\\n\\n**What are good baselines when building a recommender system? \\u200d‚≠êÔ∏è**\\n\\n* A good recommer system should give relevant and personalized information.\\n* It should not recommend items the user knows well or finds easily.\\n* It should make diverse suggestions.\\n* A user should explore new items.\\n\\n<br/>\\n\\n**What is collaborative filtering? \\u200d‚≠êÔ∏è**\\n\\n* Collaborative filtering is the most prominent approach to generate recommendations.\\n* It uses the wisdom of the crowd, i.e. it gives recommendations based on the experience of others.\\n* A recommendation is calculated as the average of other experiences.\\n* Say we want to give a score that indicates how much user u will like an item i. Then we can calculate it with the experience of N other users U as r_ui = 1/N * sum(v in U) r_vi.\\n* In order to rate similar experiences with a higher weight, we can introduce a similarity between users that we use as a multiplier for each rating.\\n* Also, as users have an individual profile, one user may have an average rating much larger than another user, so we use normalization techniques (e.g. centering or Z-score normalization) to remove the users' biases.\\n* Collaborative filtering does only need a rating matrix as input and improves over time. However, it does not work well on sparse data, does not work for cold starts (see below) and usually tends to overfit.\\n\\n<br/>\\n\\n**How we can incorporate implicit feedback (clicks, etc) into our recommender systems? \\u200d‚≠êÔ∏è**\\n\\nIn comparison to explicit feedback, implicit feedback datasets lack negative examples. For example, explicit feedback can be a positive or a negative rating, but implicit feedback may be the number of purchases or clicks. One popular approach to solve this problem is named weighted alternating least squares (wALS) [Hu, Y., Koren, Y., & Volinsky, C. (2008, December). Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on (pp. 263-272). IEEE.]. Instead of modeling the rating matrix directly, the numbers (e.g. amount of clicks) describe the strength in observations of user actions. The model tries to find latent factors that can be used to predict the expected preference of a user for an item.\\n\\n<br/>\\n\\n**What is the cold start problem? \\u200d‚≠êÔ∏è**\\n\\nCollaborative filterung incorporates crowd knowledge to give recommendations for certain items. Say we want to recommend how much a user will like an item, we then will calculate the score using the recommendations of other users for this certain item. We can distinguish between two different ways of a cold start problem now. First, if there is a new item that has not been rated yet, we cannot give any recommendation. Also, when there is a new user, we cannot calculate a similarity to any other user.\\n\\n<br/>\\n\\n**Possible approaches to solving the cold start problem? \\u200d‚≠êÔ∏èüöÄ**\\n\\n* Content-based filtering incorporates features about items to calculate a similarity between them. In this way, we can recommend items that have a high similarity to items that a user liked already. In this way, we are not dependent on the ratings of other users for a given item anymore and solve the cold start problem for new items.\\n* Demographic filtering incorporates user profiles to calculate a similarity between them and solves the cold start problem for new users.\\n\\n<br/>\\n\\n\",\n",
       " \"## Time series\\n\\n**What is a time series? üë∂**\\n\\nA time series is a set of observations ordered in time usually collected at regular intervals.\\n\\n<br/>\\n\\n**How is time series different from the usual regression problem? üë∂**\\n\\nThe principle behind causal forecasting is that the value that has to be predicted is dependant on the input features (causal factors). In time series forecasting, the to be predicted value is expected to follow a certain pattern over time.\\n\\n<br/>\\n\\n**Which models do you know for solving time series problems? \\u200d‚≠êÔ∏è**\\n\\n* Simple Exponential Smoothing: approximate the time series with an exponential function\\n* Trend-Corrected Exponential Smoothing (Holt‚Äòs Method): exponential smoothing that also models the trend\\n* Trend- and Seasonality-Corrected Exponential Smoothing (Holt-Winter‚Äòs Method): exponential smoothing that also models trend and seasonality\\n* Time Series Decomposition: decomposed a time series into the four components trend, seasonal variation, cycling variation and irregular component\\n* Autoregressive models: similar to multiple linear regression, except that the dependent variable y_t depends on its own previous values rather than other independent variables.\\n* Deep learning approaches (RNN, LSTM, etc.)\\n\\n<br/>\\n\\n**If there‚Äôs a trend in our series, how we can remove it? And why would we want to do it? \\u200d‚≠êÔ∏è**\\n\\nWe can explicitly model the trend (and/or seasonality) with approaches such as Holt's Method or Holt-Winter's Method. We want to explicitly model the trend to reach the stationarity property for the data. Many time series approaches require stationarity. Without stationarity,the interpretation of the results of these analyses is problematic [Manuca, Radu & Savit, Robert. (1996). Stationarity and nonstationarity in time series analysis. Physica D: Nonlinear Phenomena. 99. 134-161. 10.1016/S0167-2789(96)00139-X. ].\\n\\n<br/>\\n\\n**You have a series with only one variable ‚Äúy‚Äù measured at time t. How do predict ‚Äúy‚Äù at time t+1? Which approaches would you use? \\u200d‚≠êÔ∏è**\\n\\nWe want to look at the correlation between different observations of y. This measure of correlation is called autocorrelation. Autoregressive models are multiple regression models where the time-lag series of the original time series are treated like multiple independent variables.\\n\\n<br/>\\n\\n**You have a series with a variable ‚Äúy‚Äù and a set of features. How do you predict ‚Äúy‚Äù at t+1? Which approaches would you use? \\u200d‚≠êÔ∏è**\\n\\nGiven the assumption that the set of features gives a meaningful causation to y, a causal forecasting approach such as linear regression or multiple nonlinear regression might be useful. In case there is a lot of data and the explainability of the results is not a high priority, we can also consider deep learning approaches.\\n\\n<br/>\\n\\n**What are the problems with using trees for solving time series problems? \\u200d‚≠êÔ∏è**\\n\\nRandom Forest models are not able to extrapolate time series data and understand increasing/decreasing trends. It will provide us with average data points if the validation data has values greater than the training data points.\\n\\n<br/>\\n\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77920718",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pattern = r'\\*\\*(.*?)\\*\\*\\n\\n([\\s\\S]*?)(?=\\n<br/>\\n|\\Z)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec6fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4813f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in sections:\n",
    "    qa_pairs = re.findall(qa_pattern, section)\n",
    "    qa.extend(qa_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5103b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What is supervised machine learning? üë∂', 'Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the\\xa0input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.\\n\\nk-Nearest Neighbors(KNN):Looking at the k closest labeled data points \\n'), ('What is regression? Which models can you use to solve a regression problem? üë∂', 'Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor).\\nHere are some common regression models\\n\\n- *Linear Regression* establishes a linear relationship between target and predictor (s). It predicts a numeric value and has a shape of a straight line.\\n- *Polynomial Regression* has a regression equation with the power of independent variable more than 1. It is a curve that fits into the data points.\\n- *Ridge Regression* helps when predictors are highly correlated (multicollinearity problem). It penalizes the squares of regression coefficients but doesn‚Äôt allow the coefficients to reach zeros (uses L2 regularization).\\n- *Lasso Regression* penalizes the absolute values of regression coefficients and allows some of the coefficients to reach absolute zero (thereby allowing feature selection).\\n'), ('What is linear regression? When do we use it? üë∂', 'Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).\\n\\nWith a simple equation:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```\\n\\nB is regression coefficients, x values are the independent (explanatory) variables  and y is dependent variable.\\n\\nThe case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\\n\\nSimple linear regression:\\n\\n```\\ny = B0 + B1*x1\\n```\\n\\nMultiple linear regression:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```\\n'), ('What are the main assumptions of linear regression? ‚≠ê', 'There are several assumptions of linear regression. If any of them is violated, model predictions and interpretation may be worthless or misleading.\\n\\n1. **Linear relationship** between features and target variable.\\n2. **Additivity** means that the effect of changes in one of the features on the target variable does not depend on values of other features. For example, a model for predicting revenue of a company have of two features - the number of items _a_ sold and the number of items _b_ sold. When company sells more items _a_ the revenue increases and this is independent of the number of items _b_ sold. But, if customers who buy _a_ stop buying _b_, the additivity assumption is violated.\\n3. Features are not correlated (no **collinearity**) since it can be difficult to separate out the individual effects of collinear features on the target variable.\\n4. Errors are independently and identically normally distributed (y<sub>i</sub> = B0 + B1*x1<sub>i</sub> + ... + error<sub>i</sub>):\\n   1. No correlation between errors (consecutive errors in the case of time series data).\\n   2. Constant variance of errors - **homoscedasticity**. For example, in case of time series, seasonal patterns can increase errors in seasons with higher activity.\\n   3. Errors are normally distributed, otherwise some features will have more influence on the target variable than to others. If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow.\\n'), ('What‚Äôs the normal distribution? Why do we care about it? üë∂', 'The normal distribution is a continuous probability distribution whose probability density function takes the following formula:\\n\\n![formula](https://mathworld.wolfram.com/images/equations/NormalDistribution/NumberedEquation1.gif)\\n\\nwhere Œº is the mean and œÉ is the standard deviation of the distribution.\\n\\nThe normal distribution derives its importance from the **Central Limit Theorem**, which states that if we draw a large enough number of samples, their mean will follow a normal distribution regardless of the initial distribution of the sample, i.e **the distribution of the mean of the samples is normal**. It is important that each sample is independent from the other.\\n\\nThis is powerful because it helps us study processes whose population distribution is unknown to us.\\n\\n'), ('How do we check if a variable follows the normal distribution? \\u200d‚≠êÔ∏è', '1. Plot a histogram out of the sampled data. If you can fit the bell-shaped \"normal\" curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected.\\n2. Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution.\\n3. Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously.\\n4. Check for Quantile-Quantile plot. It is a scatterplot created by plotting two sets of quantiles against one another. Normal Q-Q plot place the data points in a roughly straight line.\\n'), ('What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices? \\u200d‚≠êÔ∏è', \"Data is not normal. Specially, real-world datasets or uncleaned datasets always have certain skewness. Same goes for the price prediction. Price of houses or any other thing under consideration depends on a number of factors. So, there's a great chance of presence of some skewed values i.e outliers if we talk in data science terms. \\n\\nYes, you may need to do pre-processing. Most probably, you will need to remove the outliers to make your distribution near-to-normal.\\n\"), ('What methods for solving linear regression do you know? \\u200d‚≠êÔ∏è', \"To solve linear regression, you need to find the coefficients $`\\\\beta`$ which minimize the sum of squared errors.\\n\\nMatrix Algebra method: Let's say you have `X`, a matrix of features, and `y`, a vector with the values you want to predict. After going through the matrix algebra and minimization problem, you get this solution: $`\\\\beta = (X^{T}X)^{-1}X^{T}y`$. \\n\\nBut solving this requires you to find an inverse, which can be time-consuming, if not impossible. Luckily, there are methods like Singular Value Decomposition (SVD) or QR Decomposition that can reliably calculate this part $`(X^{T}X)^{-1}X^{T}`$ (called the pseudo-inverse) without actually needing to find an inverse. The popular python ML library `sklearn` uses SVD to solve least squares.\\n\\nAlternative method: Gradient Descent. See explanation below.\\n\"), ('What is gradient descent? How does it work? \\u200d‚≠êÔ∏è', 'Gradient descent is an algorithm that uses calculus concept of gradient to try and reach local or global minima. It works by taking the negative of the gradient in a point of a given function, and updating that point repeatedly using the calculated negative gradient, until the algorithm reaches a local or global minimum, which will cause future iterations of the algorithm to return values that are equal or too close to the current point. It is widely used in machine learning applications.\\n'), ('What is the normal equation? \\u200d‚≠êÔ∏è', 'Normal equations are equations obtained by setting equal to zero the partial derivatives of the sum of squared errors (least squares); normal equations allow one to estimate the parameters of a multiple linear regression.\\n'), ('What is SGD \\u200a‚Äî\\u200a stochastic gradient descent? What‚Äôs the difference with the usual gradient descent? \\u200d‚≠êÔ∏è', 'In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function.\\n\\nThe difference lies in how the gradient of the loss function is estimated. In the usual GD, you have to run through ALL the samples in your training set in order to estimate the gradient and do a single update for a parameter in a particular iteration. In SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to estimate the gradient and do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent.\\n'), ('Which metrics for evaluating regression models do you know? üë∂', '1. Mean Squared Error(MSE)\\n2. Root Mean Squared Error(RMSE)\\n3. Mean Absolute Error(MAE)\\n4. R¬≤ or Coefficient of Determination\\n5. Adjusted R¬≤\\n'), ('What are MSE and RMSE? üë∂', 'MSE stands for <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror while RMSE stands for <strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror. They are metrics with which we can evaluate models.\\n'), ('What is the bias-variance trade-off? üë∂', '**Bias** is the error introduced by approximating the true underlying function, which can be quite complex, by a simpler model. **Variance** is a model sensitivity to changes in the training dataset.\\n\\n**Bias-variance trade-off** is a relationship between the expected test error and the variance and the bias - both contribute to the level of the test error and ideally should be as small as possible:\\n\\n```\\nExpectedTestError = Variance + Bias¬≤ + IrreducibleError\\n```\\n\\nBut as a model complexity increases, the bias decreases and the variance increases which leads to *overfitting*. And vice versa, model simplification helps to decrease the variance but it increases the bias which leads to *underfitting*.\\n'), ('What is overfitting? üë∂', \"When your model perform very well on your training set but can't generalize the test set, because it adjusted a lot to the training set.\\n\"), ('How to validate your models? üë∂', 'One of the most common approaches is splitting data into train, validation and test parts.\\nModels are trained on train data, hyperparameters (for example early stopping) are selected based on the validation data, the final measurement is done on test dataset.\\nAnother approach is cross-validation: split dataset into K folds and each time train models on training folds and measure the performance on the validation folds.\\nAlso you could combine these approaches: make a test/holdout dataset and do cross-validation on the rest of the data. The final quality is measured on test dataset.\\n'), ('Why do we need to split our data into three parts: train, validation, and test? üë∂', 'The training set is used to fit the model, i.e. to train the model with the data. The validation set is then used to provide an unbiased evaluation of a model while fine-tuning hyperparameters. This improves the generalization of the model. Finally, a test data set which the model has never \"seen\" before should be used for the final evaluation of the model. This allows for an unbiased evaluation of the model. The evaluation should never be performed on the same data that is used for training. Otherwise the model performance would not be representative.\\n'), ('Can you explain how cross-validation works? üë∂', 'Cross-validation is the process to separate your total training set into two subsets: training and validation set, and evaluate your model to choose the hyperparameters. But you do this process iteratively, selecting different training and validation set, in order to reduce the bias that you would have by selecting only one validation set.\\n'), ('What is K-fold cross-validation? üë∂', 'K fold cross validation is a method of cross validation where we select a hyperparameter k. The dataset is now divided into k parts. Now, we take the 1st part as validation set and remaining k-1 as training set. Then we take the 2nd part as validation set and remaining k-1 parts as training set. Like this, each part is used as validation set once and the remaining k-1 parts are taken together and used as training set.\\nIt should not be used in a time series data.\\n'), ('How do we choose K in K-fold cross-validation? What‚Äôs your favorite K? üë∂', 'There are two things to consider while deciding K: the number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained. <br/>\\nI tend to use 4 for small datasets and 5 for large ones as K.\\n'), ('What is classification? Which models would you use to solve a classification problem? üë∂', 'Classification problems are problems in which our prediction space is discrete, i.e. there is a finite number of values the output variable can be. Some models which can be used to solve classification problems are: logistic regression, decision tree, random forests, multi-layer perceptron, one-vs-all, amongst others.\\n'), ('What is logistic regression? When do we need to use it? üë∂', 'Logistic regression is a Machine Learning algorithm that is used for binary classification. You should use logistic regression when your Y variable takes only two values, e.g. True and False, \"spam\" and \"not spam\", \"churn\" and \"not churn\" and so on. The variable is said to be a \"binary\" or \"dichotomous\".\\n'), ('Is logistic regression a linear model? Why? üë∂', 'Yes, Logistic Regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters.\\n'), ('What is sigmoid? What does it do? üë∂', 'A sigmoid function is a type of activation function, and more specifically defined as a squashing function. Squashing functions limit the output to a range between 0 and 1, making these functions useful in the prediction of probabilities.\\n\\nSigmod(x) = 1/(1+e^{-x})\\n'), ('How do we evaluate classification models? üë∂', 'Depending on the classification problem, we can use the following evaluation metrics:\\n\\n1. Accuracy\\n2. Precision\\n3. Recall\\n4. F1 Score\\n5. Logistic loss (also known as Cross-entropy loss)\\n6. Jaccard similarity coefficient score\\n'), ('What is accuracy? üë∂', 'Accuracy is a metric for evaluating classification models. It is calculated by dividing the number of correct predictions by the number of total predictions.\\n'), ('Is accuracy always a good metric? üë∂', 'Accuracy is not a good performance metric when there is imbalance in the dataset. For example, in binary classification with 95% of A class and 5% of B class, a constant prediction of A class would have an accuracy of 95%. In case of imbalance dataset, we need to choose Precision, recall, or F1 Score depending on the problem we are trying to solve.\\n'), ('What is the confusion table? What are the cells in this table? üë∂', 'Confusion table (or confusion matrix) shows how many True positives (TP), True Negative (TN), False Positive (FP) and False Negative (FN) model has made.\\n\\n||                |     Actual   |        Actual |\\n|:---:|   :---:        |     :---:    |:---:          |\\n||                | Positive (1) | Negative (0)  |\\n|Predicted|   Positive (1) | TP           | FP            |\\n|Predicted|   Negative (0) | FN           | TN            |\\n\\n* True Positives (TP): When the actual class of the observation is 1 (True) and the prediction is 1 (True)\\n* True Negative (TN): When the actual class of the observation is 0 (False) and the prediction is 0 (False)\\n* False Positive (FP): When the actual class of the observation is 0 (False) and the prediction is 1 (True)\\n* False Negative (FN): When the actual class of the observation is 1 (True) and the prediction is 0 (False)\\n\\nMost of the performance metrics for classification models are based on the values of the confusion matrix.\\n'), ('What are precision, recall, and F1-score? üë∂', '* Precision and recall are classification evaluation metrics:\\n* P = TP / (TP + FP) and R = TP / (TP + FN).\\n* Where TP is true positives, FP is false positives and FN is false negatives\\n* In both cases the score of 1 is the best: we get no false positives or false negatives and only true positives.\\n* F1 is a combination of both precision and recall in one score (harmonic mean):\\n* F1 = 2 * PR / (P + R).\\n* Max F score is 1 and min is 0, with 1 being the best.\\n'), ('Precision-recall trade-off \\u200d‚≠êÔ∏è', 'Tradeoff means increasing one parameter would lead to decreasing of other. Precision-recall tradeoff occur due to increasing one of the parameter(precision or recall) while keeping the model same. \\n\\nIn an ideal scenario where there is a perfectly separable data, both precision and recall can get maximum value of 1.0. But in most of the practical situations, there is noise in the dataset and the dataset is not perfectly separable. There might be some points of positive class closer to the negative class and vice versa. In such cases, shifting the decision boundary can either increase the precision or recall but not both. Increasing one parameter leads to decreasing of the other. \\n'), ('What is the ROC curve? When to use it? \\u200d‚≠êÔ∏è', 'ROC stands for *Receiver Operating Characteristics*. The diagrammatic representation that shows the contrast between true positive rate vs false positive rate. It is used when we need to predict the probability of the binary outcome.\\n'), ('What is AUC (AU ROC)? When to use it? \\u200d‚≠êÔ∏è', \"AUC stands for *Area Under the ROC Curve*. ROC is a probability curve and AUC represents degree or measure of separability. It's used when we need to value how much model is capable of distinguishing between classes.  The value is between 0 and 1, the higher the better.\\n\"), ('How to interpret the AU ROC score? \\u200d‚≠êÔ∏è', 'AUC score is the value of *Area Under the ROC Curve*. \\n\\nIf we assume ROC curve consists of dots, $`(x_1, y_1), (x_2, y_2), \\\\cdots, (x_m,y_m)`$, then\\n\\n$`AUC = \\\\frac{1}{2} \\\\sum_{i=1}^{m-1}(x_{i+1}-x_i)\\\\cdot (y_i+y_{i+1})`$\\n\\nAn excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. When AUC score is 0.5, it means model has no class separation capacity whatsoever. \\n'), ('What is the PR (precision-recall) curve? \\u200d‚≠êÔ∏è', 'A *precision*-*recall curve* (or PR Curve) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. Precision-recall curves (PR curves) are recommended for highly skewed domains where ROC curves may provide an excessively optimistic view of the performance.\\n'), ('What is the area under the PR curve? Is it a useful metric? \\u200d‚≠êÔ∏èI', 'The Precision-Recall AUC is just like the ROC AUC, in that it summarizes the curve with a range of threshold values as a single score.\\n\\nA high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.\\n'), ('In which cases AU PR is better than AU ROC? \\u200d‚≠êÔ∏è', 'What is different however is that AU ROC looks at a true positive rate TPR and false positive rate FPR while AU PR looks at positive predictive value PPV and true positive rate TPR.\\n\\nTypically, if true negatives are not meaningful to the problem or you care more about the positive class, AU PR is typically going to be more useful; otherwise, If you care equally about the positive and negative class or your dataset is quite balanced, then going with AU ROC is a good idea.\\n'), ('What do we do with categorical variables? \\u200d‚≠êÔ∏è', 'Categorical variables must be encoded before they can be used as features to train a machine learning model. There are various encoding techniques, including:\\n- One-hot encoding\\n- Label encoding\\n- Ordinal encoding\\n- Target encoding\\n'), ('Why do we need one-hot encoding? \\u200d‚≠êÔ∏è', 'If we simply encode categorical variables with a Label encoder, they become ordinal which can lead to undesirable consequences. In this case, linear models will treat category with id 4 as twice better than a category with id 2. One-hot encoding allows us to represent a categorical variable in a numerical vector space which ensures that vectors of each category have equal distances between each other. This approach is not suited for all situations, because by using it with categorical variables of high cardinality (e.g. customer id) we will encounter problems that come into play because of the curse of dimensionality.\\n'), ('What is \"curse of dimensionality\"? \\u200d‚≠êÔ∏è', 'The curse of dimensionality is an issue that arises when working with high-dimensional data. It is often said that \"the curse of dimensionality\" is one of the main problems with machine learning. The curse of dimensionality refers to the fact that, as the number of dimensions (features) in a data set increases, the number of data points required to accurately learn the relationships between those features increases exponentially.\\n\\nA simple example where we have a data set with two features, x1 and x2. If we want to learn the relationship between these two features, we need to have enough data points so that we can accurately estimate the parameters of that relationship. However, if we add a third feature, x3, then the number of data points required to accurately learn the relationships between all three features increases exponentially. This is because there are now more parameters to estimate, and the number of data points needed to accurately estimate those parameters increases exponentially with the number of parameters.\\n\\nSimply put, the curse of dimensionality basically means that the error increases with the increase in the number of features.\\n'), ('What happens to our linear regression model if we have three columns in our data: x, y, z \\u200a‚Äî\\u200a and z is a sum of x and y? \\u200d‚≠êÔ∏è', 'We would not be able to perform the regression. Because z is linearly dependent on x and y so when performing the regression $`{X}^{T}{X}`$ would be a singular (not invertible) matrix.'), ('What happens to our linear regression model if the column z in the data is a sum of columns x and y and some random noise? \\u200d‚≠êÔ∏è', 'It creates a situation known as multicollinearity. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, leading to issues in the estimation of the regression coefficients. The issue arises because the variables X and Y will be highly correlated, making it difficult for the model to distinguish the individual effects of X and Y on the dependent variable Z. To address it, feature selection, PCA or regularization techniques (L2) may be used.\\n'), ('What is regularization? Why do we need it? üë∂', 'Regularization is used to reduce overfitting in machine learning models. It helps the models to generalize well and make them robust to outliers and noise in the data.\\n'), ('Which regularization techniques do you know? \\u200d‚≠êÔ∏è', 'There are mainly two types of regularization,\\n1. L1 Regularization (Lasso regularization) - Adds the sum of absolute values of the coefficients to the cost function. $`\\\\lambda\\\\sum_{i=1}^{n} \\\\left | w_i \\\\right |`$\\n2. L2 Regularization (Ridge regularization) - Adds the sum of squares of coefficients to the cost function. $`\\\\lambda\\\\sum_{i=1}^{n} {w_{i}}^{2}`$\\n\\n* Where $`\\\\lambda`$ determines the amount of regularization.\\n'), ('What kind of regularization techniques are applicable to linear models? \\u200d‚≠êÔ∏è', 'AIC/BIC, Ridge regression, Lasso, Elastic Net, Basis pursuit denoising, Rudin‚ÄìOsher‚ÄìFatemi model (TV), Potts model, RLAD,\\nDantzig Selector,SLOPE\\n'), ('How does L2 regularization look like in a linear model? \\u200d‚≠êÔ∏è', 'L2 regularization adds a penalty term to our cost function which is equal to the sum of squares of models coefficients multiplied by a lambda hyperparameter. This technique makes sure that the coefficients are close to zero and is widely used in cases when we have a lot of features that might correlate with each other.\\n'), ('How do we select the right regularization parameters? üë∂', 'Regularization parameters can be chosen using a grid search, for example https://scikit-learn.org/stable/modules/linear_model.html has one formula for the implementing for regularization, alpha in the formula mentioned can be found by doing a RandomSearch or a GridSearch on a set of values and selecting the alpha which gives the least cross validation or validation error.\\n\\n'), ('What‚Äôs the effect of L2 regularization on the weights of a linear model? \\u200d‚≠êÔ∏è', 'L2 regularization penalizes larger weights more severely (due to the squared penalty term), which encourages weight values to decay toward zero.\\n'), ('How L1 regularization looks like in a linear model? \\u200d‚≠êÔ∏è', 'L1 regularization adds a penalty term to our cost function which is equal to the sum of modules of models coefficients multiplied by a lambda hyperparameter. For example, cost function with L1 regularization will look like: $`\\\\sum_{i=0}^{N} (y_i - \\\\sum_{j=0}^{M} x_{ij} * w_j)+\\\\lambda\\\\sum_{j=0}^{M} \\\\left | w_j \\\\right |`$\\n'), ('What‚Äôs the difference between L2 and L1 regularization? \\u200d‚≠êÔ∏è', '- Penalty terms: L1 regularization uses the sum of the absolute values of the weights, while L2 regularization uses the sum of the weights squared.\\n- Feature selection: L1 performs feature selection by reducing the coefficients of some predictors to 0, while L2 does not.\\n- Computational efficiency: L2 has an analytical solution, while L1 does not.\\n- Multicollinearity: L2 addresses multicollinearity by constraining the coefficient norm.\\n'), ('Can we have both L1 and L2 regularization components in a linear model? \\u200d‚≠êÔ∏è', 'Yes, elastic net regularization combines L1 and L2 regularization. \\n'), ('What‚Äôs the interpretation of the bias term in linear models? \\u200d‚≠êÔ∏è', 'Bias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias.\\n'), ('How do we interpret weights in linear models? \\u200d‚≠êÔ∏è', 'Without normalizing weights or variables, if you increase the corresponding predictor by one unit, the coefficient represents on average how much the output changes. By the way, this interpretation still works for logistic regression - if you increase the corresponding predictor by one unit, the weight represents the change in the log of the odds.\\n\\nIf the variables are normalized, we can interpret weights in linear models like the importance of this variable in the predicted result.\\n'), ('If a weight for one variable is higher than for another \\u200a‚Äî\\u200a can we say that this variable is more important? \\u200d‚≠êÔ∏è', \"Yes - if your predictor variables are normalized.\\n\\nWithout normalization, the weight represents the change in the output per unit change in the predictor. If you have a predictor with a huge range and scale that is used to predict an output with a very small range - for example, using each nation's GDP to predict maternal mortality rates - your coefficient should be very small. That does not necessarily mean that this predictor variable is not important compared to the others.\\n\"), ('When do we need to perform feature normalization for linear models? When it‚Äôs okay not to do it? \\u200d‚≠êÔ∏è', \"Feature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This can't be done effectively if every feature is scaled differently. \\n\\nLinear regression without regularization techniques can be used without feature normalization. Also, regularization can help to make the analytical solution more stable, ‚Äî it adds the regularization matrix to the feature matrix before inverting it. \\n\"), ('What is feature selection? Why do we need it? üë∂', 'Feature Selection is a method used to select the relevant features for the model to train on. We need feature selection to remove the irrelevant features which leads the model to under-perform.  \\n'), ('Is feature selection important for linear models? \\u200d‚≠êÔ∏è', 'Yes, It is. It can make model performance better through selecting the most importance features and remove irrelevant features in order to make a prediction and it can also avoid overfitting, underfitting and bias-variance tradeoff. \\n'), ('Which feature selection techniques do you know? \\u200d‚≠êÔ∏è', 'Here are some of the feature selections:\\n- Principal Component Analysis\\n- Neighborhood Component Analysis\\n- ReliefF Algorithm\\n'), ('Can we use L1 regularization for feature selection? \\u200d‚≠êÔ∏è', 'Yes, because the nature of L1 regularization will lead to sparse coefficients of features. Feature selection can be done by keeping only features with non-zero coefficients.\\n'), ('Can we use L2 regularization for feature selection? \\u200d‚≠êÔ∏è', 'No, Because L2 regularization does not make the weights zero but only makes them very very small. L2 regularization can be used to solve multicollinearity since it stabilizes the model.\\n'), ('What are the decision trees? üë∂', 'This is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. \\n\\nIn this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible.\\n\\nA decision tree is a flowchart-like tree structure, where each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a value for the target variable.\\n\\nVarious techniques : like Gini, Information Gain, Chi-square, entropy.\\n'), ('How do we train decision trees? \\u200d‚≠êÔ∏è', '1. Start at the root node.\\n2. For each variable X, find the set S_1 that minimizes the sum of the node impurities in the two child nodes and choose the split {X*,S*} that gives the minimum over all X and S.\\n3. If a stopping criterion is reached, exit. Otherwise, apply step 2 to each child node in turn.\\n'), ('What are the main parameters of the decision tree model? üë∂', '* maximum tree depth\\n* minimum samples per leaf node\\n* impurity criterion\\n'), ('How do we handle categorical variables in decision trees? \\u200d‚≠êÔ∏è', 'Some decision tree algorithms can handle categorical variables out of the box, others cannot. However, we can transform categorical variables, e.g. with a binary or a one-hot encoder.\\n'), ('What are the benefits of a single decision tree compared to more complex models? \\u200d‚≠êÔ∏è', '* easy to implement\\n* fast training\\n* fast inference\\n* good explainability\\n'), ('How can we know which features are more important for the decision tree model? \\u200d‚≠êÔ∏è', 'Often, we want to find a split such that it minimizes the sum of the node impurities. The impurity criterion is a parameter of decision trees. Popular methods to measure the impurity are the Gini impurity and the entropy describing the information gain.\\n'), ('What is random forest? üë∂', 'Random Forest is a machine learning method for regression and classification which is composed of many decision trees. Random Forest belongs to a larger class of ML algorithms called ensemble methods (in other words, it involves the combination of several models to solve a single prediction problem).\\n'), ('Why do we need randomization in random forest? \\u200d‚≠êÔ∏è', 'Random forest in an extension of the **bagging** algorithm which takes *random data samples from the training dataset* (with replacement), trains several models and averages predictions. In addition to that, each time a split in a tree is considered, random forest takes a *random sample of m features from full set of n features* (without replacement) and uses this subset of features as candidates for the split (for example, `m = sqrt(n)`).\\n\\nTraining decision trees on random data samples from the training dataset *reduces variance*. Sampling features for each split in a decision tree *decorrelates trees*.\\n'), ('What are the main parameters of the random forest model? \\u200d‚≠êÔ∏è', '- `max_depth`: Longest Path between root node and the leaf\\n- `min_sample_split`: The minimum number of observations needed to split a given node\\n- `max_leaf_nodes`: Conditions the splitting of the tree and hence, limits the growth of the trees\\n- `min_samples_leaf`: minimum number of samples in the leaf node\\n- `n_estimators`: Number of trees\\n- `max_sample`: Fraction of original dataset given to any individual tree in the given model\\n- `max_features`: Limits the maximum number of features provided to trees in random forest model\\n'), ('How do we select the depth of the trees in random forest? \\u200d‚≠êÔ∏è', 'The greater the depth, the greater amount of information is extracted from the tree, however, there is a limit to this, and the algorithm even if defensive against overfitting may learn complex features of noise present in data and as a result, may overfit on noise. Hence, there is no hard thumb rule in deciding the depth, but literature suggests a few tips on tuning the depth of the tree to prevent overfitting:\\n\\n- limit the maximum depth of a tree\\n- limit the number of test nodes\\n- limit the minimum number of objects at a node required to split\\n- do not split a node when, at least, one of the resulting subsample sizes is below a given threshold\\n- stop developing a node if it does not sufficiently improve the fit.\\n'), ('How do we know how many trees we need in random forest? \\u200d‚≠êÔ∏è', 'The number of trees in random forest is worked by n_estimators, and a random forest reduces overfitting by increasing the number of trees. There is no fixed thumb rule to decide the number of trees in a random forest, it is rather fine tuned with the data, typically starting off by taking the square of the number of features (n) present in the data followed by tuning until we get the optimal results.\\n'), ('Is it easy to parallelize training of a random forest model? How can we do it? \\u200d‚≠êÔ∏è', \"Yes, R provides a simple way to parallelize training of random forests on large scale data.\\nIt makes use of a parameter called multicombine which can be set to TRUE for parallelizing random forest computations.\\n\\n```R\\nrf <- foreach(ntree=rep(25000, 6), .combine=randomForest::combine,\\n              .multicombine=TRUE, .packages='randomForest') %dopar% {\\n    randomForest(x, y, ntree=ntree)\\n}\\n```\\n\\n\"), ('What are the potential problems with many large trees? \\u200d‚≠êÔ∏è', \"- Overfitting: A large number of large trees can lead to overfitting, where the model becomes too complex and is able to memorize the training data but doesn't generalize well to new, unseen data.\\n\\n- Slow prediction time: As the number of trees in the forest increases, the prediction time for new data points can become quite slow. This can be a problem when you need to make predictions in real-time or on a large dataset.\\n\\n- Memory consumption: Random Forest models with many large trees can consume a lot of memory, which can be a problem when working with large datasets or on a limited hardware.\\n\\n- Lack of interpretability: Random Forest models with many large trees can be difficult to interpret, making it harder to understand how the model is making predictions or what features are most important.\\n\\n- Difficulty in tuning : With an increasing number of large trees the tuning process becomes more complex and computationally expensive.\\n\\nIt's important to keep in mind that the number of trees in a Random Forest should be chosen based on the specific problem and dataset, rather than using a large number of trees by default. In practice, the number of trees in a random forest is chosen based on the trade-off between the computational cost and the performance.\\n\"), ('What if instead of finding the best split, we randomly select a few splits and just select the best from them. Will it work? üöÄ', 'Answer here\\n'), ('What happens when we have correlated features in our data? \\u200d‚≠êÔ∏è', 'In random forest, since random forest samples some features to build each tree, the information contained in correlated features is twice as much likely to be picked than any other information contained in other features. \\n\\nIn general, when you are adding correlated features, it means that they linearly contains the same information and thus it will reduce the robustness of your model. Each time you train your model, your model might pick one feature or the other to \"do the same job\" i.e. explain some variance, reduce entropy, etc.\\n'), ('What is gradient boosting trees? \\u200d‚≠êÔ∏è', 'Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\\n'), ('What‚Äôs the difference between random forest and gradient boosting? \\u200d‚≠êÔ∏è', '   1. Random Forests builds each tree independently while Gradient Boosting builds one tree at a time.\\n   2. Random Forests combine results at the end of the process (by averaging or \"majority rules\") while Gradient Boosting combines     results along the way.\\n'), ('Is it possible to parallelize training of a gradient boosting model? How to do it? \\u200d‚≠êÔ∏è', \"Yes, different frameworks provide different options to make training faster, using GPUs to speed up the process by making it highly parallelizable.For example, for XGBoost <i>tree_method = 'gpu_hist'</i> option makes training faster by use of GPUs. \\n\"), ('Feature importance in gradient boosting trees \\u200a‚Äî\\u200a what are possible options? \\u200d‚≠êÔ∏è', 'With CatBoost you can use implemented method get_feature_importance for getting SHAP values. https://arxiv.org/abs/1905.04610v1\\n\\nIt allows to understand how excluding features helps to provide better results. Higher value is better.\\n\\nAlso you can add random noise column to your data (with normal distribution), calculate feature importance and exclude all features below noise importance.\\n'), ('Are there any differences between continuous and discrete variables when it comes to feature importance of gradient boosting models? üöÄ', 'Answer here\\n'), ('What are the main parameters in the gradient boosting model? \\u200d‚≠êÔ∏è', 'There are many parameters, but below are a few key defaults.\\n* learning_rate=0.1 (shrinkage).\\n* n_estimators=100 (number of trees).\\n* max_depth=3.\\n* min_samples_split=2.\\n* min_samples_leaf=1.\\n* subsample=1.0.\\n'), ('How do you approach tuning parameters in XGBoost or LightGBM? üöÄ', 'Depending upon the dataset, parameter tuning can be done manually or using hyperparameter optimization frameworks such as optuna and hyperopt. In manual parameter tuning, we need to be aware of max-depth, min_samples_leaf and min_samples_split so that our model does not overfit the data but try to predict generalized characteristics of data (basically keeping variance and bias low for our model).\\n'), ('How do you select the number of trees in the gradient boosting model? \\u200d‚≠êÔ∏è', 'Most implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds or thousands. Using scikit-learn we can perform a grid search of the n_estimators model parameter\\n'), ('Which hyper-parameter tuning strategies (in general) do you know? \\u200d‚≠êÔ∏è', 'There are several strategies for hyper-tuning but I would argue that the three most popular nowadays are the following:\\n* <b>Grid Search</b> is an exhaustive approach such that for each hyper-parameter, the user needs to <i>manually</i> give a list of values for the algorithm to try. After these values are selected, grid search then evaluates the algorithm using each and every combination of hyper-parameters and returns the combination that gives the optimal result (i.e. lowest MAE). Because grid search evaluates the given algorithm using all combinations, it\\'s easy to see that this can be quite computationally expensive and can lead to sub-optimal results specifically since the user needs to specify specific values for these hyper-parameters, which is prone for error and requires domain knowledge.\\n\\n* <b>Random Search</b> is similar to grid search but differs in the sense that rather than specifying which values to try for each hyper-parameter, an upper and lower bound of values for each hyper-parameter is given instead. With uniform probability, random values within these bounds are then chosen and similarly, the best combination is returned to the user. Although this seems less intuitive, no domain knowledge is necessary and theoretically much more of the parameter space can be explored.\\n\\n* In a completely different framework, <b>Bayesian Optimization</b> is thought of as a more statistical way of optimization and is commonly used when using neural networks, specifically since one evaluation of a neural network can be computationally costly. In numerous research papers, this method heavily outperforms Grid Search and Random Search and is currently used on the Google Cloud Platform as well as AWS. Because an in-depth explanation requires a heavy background in bayesian statistics and gaussian processes (and maybe even some game theory), a \"simple\" explanation is that a much simpler/faster <i>acquisition function</i> intelligently chooses (using a <i>surrogate function</i> such as probability of improvement or GP-UCB) which hyper-parameter values to try on the computationally expensive, original algorithm. Using the result of the initial combination of values on the expensive/original function, the acquisition function takes the result of the expensive/original algorithm into account and uses it as its prior knowledge to again come up with another set of hyper-parameters to choose during the next iteration. This process continues either for a specified number of iterations or for a specified amount of time and similarly the combination of hyper-parameters that performs the best on the expensive/original algorithm is chosen.\\n\\n'), ('What‚Äôs the difference between grid search parameter tuning strategy and random search? When to use one or another? \\u200d‚≠êÔ∏è', 'For specifics, refer to the above answer.\\n'), ('What kind of problems neural nets can solve? üë∂', 'Neural nets are good at solving non-linear problems. Some good examples are problems that are relatively easy for humans (because of experience, intuition, understanding, etc), but difficult for traditional regression models: speech recognition, handwriting recognition, image identification, etc.\\n'), ('How does a usual fully-connected feed-forward neural network work? \\u200d‚≠êÔ∏è', 'In a usual fully-connected feed-forward network, each neuron receives input from every element of the previous layer and thus the receptive field of a neuron is the entire previous layer. They are usually used to represent feature vectors for input data in classification problems but can be expensive to train because of the number of computations involved.\\n'), ('Why do we need activation functions? üë∂', 'The main idea of using neural networks is to learn complex nonlinear functions. If we are not using an activation function in between different layers of a neural network, we are just stacking up multiple linear layers one on top of another and this leads to learning a linear function. The Nonlinearity comes only with the activation function, this is the reason we need activation functions.\\n'), ('What are the problems with sigmoid as an activation function? \\u200d‚≠êÔ∏è', 'The derivative of the sigmoid function for large positive or negative numbers is almost zero. From this comes the problem of vanishing gradient ‚Äî during the backpropagation our net will not learn (or will learn drastically slow). One possible way to solve this problem is to use ReLU activation function.\\n'), ('What is ReLU? How is it better than sigmoid or tanh? \\u200d‚≠êÔ∏è', 'ReLU is an abbreviation for Rectified Linear Unit. It is an activation function which has the value 0 for all negative values and the value f(x) = x for all positive values. The ReLU has a simple activation function which makes it fast to compute and while the sigmoid and tanh activation functions saturate at higher values, the ReLU has a potentially infinite activation, which addresses the problem of vanishing gradients. \\n'), ('How we can initialize the weights of a neural network? \\u200d‚≠êÔ∏è', 'Proper initialization of weight matrix in neural network is very necessary.\\nSimply we can say there are two ways for initializations.\\n   1. Initializing weights with zeroes.\\n      Setting weights to zero makes your network no better than a linear model. It is important to note that setting biases to 0 will not create any troubles as non zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron are still different.  \\n   2. Initializing weights randomly.\\n      Assigning random values to weights is better than just 0 assignment. \\n* a) If weights are initialized with very high values the term np.dot(W,X)+b becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where the slope of gradient changes slowly and learning takes a lot of time.\\n* b) If weights are initialized with low values it gets mapped to 0, where the case is the same as above. This problem is often referred to as the vanishing gradient.\\n      '), ('What if we set all the weights of a neural network to 0? \\u200d‚≠êÔ∏è', 'If all the weights of a neural network are set to zero, the output of each connection is same (W*x = 0). This means the gradients which are backpropagated to each connection in a layer is same. This means all the connections/weights learn the same thing, and the model never converges. \\n'), ('What regularization techniques for neural nets do you know? \\u200d‚≠êÔ∏è', '* L1 Regularization - Defined as the sum of absolute values of the individual parameters. The L1 penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded. \\n* L2 Regularization - Defined as the sum of square of individual parameters. Often supported by regularization hyperparameter alpha. It results in weight decay. \\n* Data Augmentation - This requires some fake data to be created as a part of training set. \\n* Drop Out : This is most effective regularization technique for neural nets. Few random nodes in each layer is deactivated in forward pass. This allows the algorithm to train on different set of nodes in each iterations.'), ('What is dropout? Why is it useful? How does it work? \\u200d‚≠êÔ∏è', 'Dropout is a technique that at each training step turns off each neuron with a certain probability of *p*. This way at each iteration we train only *1-p* of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter *p*.  \\n'), ('What is backpropagation? How does it work? Why do we need it? \\u200d‚≠êÔ∏è', 'The Backpropagation algorithm looks for the minimum value of the error function in weight space using a technique called the delta rule or gradient descent. \\nThe weights that minimize the error function is then considered to be a solution to the learning problem. \\n\\nWe need backpropogation because,\\n* Calculate the error ‚Äì How far is your model output from the actual output.\\n* Minimum Error ‚Äì Check whether the error is minimized or not.\\n* Update the parameters ‚Äì If the error is huge then, update the parameters (weights and biases). After that again check the error.  \\nRepeat the process until the error becomes minimum.\\n* Model is ready to make a prediction ‚Äì Once the error becomes minimum, you can feed some inputs to your model and it will produce the output.\\n'), ('Which optimization techniques for training neural nets do you know? \\u200d‚≠êÔ∏è', '* Gradient Descent\\n* Stochastic Gradient Descent\\n* Mini-Batch Gradient Descent(best among gradient descents)\\n* Nesterov Accelerated Gradient\\n* Momentum\\n* Adagrad \\n* AdaDelta\\n* Adam(best one. less time, more efficient)\\n'), ('How do we use SGD (stochastic gradient descent) for training a neural net? \\u200d‚≠êÔ∏è', 'SGD approximates the expectation with few randomly selected samples (instead of the full data). In comparison to batch gradient descent, we can efficiently approximate the expectation in large data sets using SGD. For neural networks this reduces the training time a lot even considering that it will converge later as the random sampling adds noise to the gradient descent.\\n'), ('What‚Äôs the learning rate? üë∂', 'The learning rate is an important hyperparameter that controls how quickly the model is adapted to the problem during the training. It can be seen as the \"step width\" during the parameter updates, i.e. how far the weights are moved into the direction of the minimum of our optimization problem.\\n'), ('What happens when the learning rate is too large? Too small? üë∂', 'A large learning rate can accelerate the training. However, it is possible that we \"shoot\" too far and miss the minimum of the function that we want to optimize, which will not result in the best solution. On the other hand, training with a small learning rate takes more time but it is possible to find a more precise minimum. The downside can be that the solution is stuck in a local minimum, and the weights won\\'t update even if it is not the best possible global solution.\\n'), ('How to set the learning rate? \\u200d‚≠êÔ∏è', \"There is no straightforward way of finding an optimum learning rate for a model. It involves a lot of hit and trial. Usually starting with a small values such as 0.01 is a good starting point for setting a learning rate and further tweaking it so that it doesn't overshoot or converge too slowly.\\n\"), ('What is Adam? What‚Äôs the main difference between Adam and SGD? \\u200d‚≠êÔ∏è', \"Adam (Adaptive Moment Estimation) is a optimization technique for training neural networks. on an average, it is the best optimizer .It works with momentums of first and second order. The intuition behind the Adam is that we don‚Äôt want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.\\n\\nAdam tends to converge faster, while SGD often converges to more optimal solutions.\\nSGD's high variance disadvantages gets rectified by Adam (as advantage for Adam).\\n\"), ('When would you use Adam and when SGD? \\u200d‚≠êÔ∏è', 'Adam tends to converge faster, while SGD often converges to more optimal solutions.\\n'), ('Do we want to have a constant learning rate or we better change it throughout training? \\u200d‚≠êÔ∏è', \"Generally, it is recommended to start learning rate with relatively high value and then gradually decrease learning rate so the model does not overshoot the minima and at the same time we don't want to start with very low learning rate as the model will take too long to converge. There are many available techniques to do decay the learning rate. For example, in PyTorch you can use\\na function called **StepLR** which decays the learning rate of each parameter by value **gamma**-which we have to pass through argument- after n number of epoch which you can also set through function argument named **epoch_size**. \\n\"), ('How do we decide when to stop training a neural net? üë∂', 'Simply stop training when the validation error is the minimum.\\n'), ('What is model checkpointing? \\u200d‚≠êÔ∏è', 'Saving the weights learned by a model mid training for long running processes is known as model checkpointing so that you can resume your training from a certain checkpoint.\\n'), ('Can you tell us how you approach the model training process? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('How we can use neural nets for computer vision? \\u200d‚≠êÔ∏è', \"Neural nets used in the area of computer vision are generally Convolutional Neural Networks(CNN's). You can learn about convolutions below. It appears that convolutions are quite powerful when it comes to working with images and videos due to their ability to extract and learn complex features. Thus CNN's are a go-to method for any problem in computer vision.    \\n\"), ('What‚Äôs a convolutional layer? \\u200d‚≠êÔ∏è', 'The idea of the convolutional layer is the assumption that the information needed for making a decision often is spatially close and thus, it only takes the weighted sum over nearby inputs. It also assumes that the networks‚Äô kernels can be reused for all nodes, hence the number of weights can be drastically reduced. To counteract only one feature being learnt per layer, multiple kernels are applied to the input which creates parallel channels in the output. Consecutive layers can also be stacked to allow the network to find more high-level features.\\n'), ('Why do we actually need convolutions? Can‚Äôt we use fully-connected layers for that? \\u200d‚≠êÔ∏è', 'A fully-connected layer needs one weight per inter-layer connection, which means the number of weights which needs to be computed quickly balloons as the number of layers and nodes per layer is increased. \\n'), ('What‚Äôs pooling in CNN? Why do we need it? \\u200d‚≠êÔ∏è', 'Pooling is a technique to downsample the feature map. It allows layers which receive relatively undistorted versions of the input to learn low level features such as lines, while layers deeper in the model can learn more abstract features such as texture.\\n'), ('How does max pooling work? Are there other pooling techniques? \\u200d‚≠êÔ∏è', 'Max pooling is a technique where the maximum value of a receptive field is passed on in the next feature map. The most commonly used receptive field is 2 x 2 with a stride of 2, which means the feature map is downsampled from N x N to N/2 x N/2. Receptive fields larger than 3 x 3 are rarely employed as too much information is lost. \\n\\nOther pooling techniques include:\\n\\n* Average pooling, the output is the average value of the receptive field.\\n* Min pooling, the output is the minimum value of the receptive field.\\n* Global pooling, where the receptive field is set to be equal to the input size, this means the output is equal to a scalar and can be used to reduce the dimensionality of the feature map. \\n'), ('Are CNNs resistant to rotations? What happens to the predictions of a CNN if an image is rotated? üöÄ', 'CNNs are not resistant to rotation by design. However, we can make our models resistant by augmenting our datasets with different rotations of the raw data. The predictions of a CNN will change if an image is rotated and we did not augment our dataset accordingly. A demonstration of this occurence can be seen in [this video](https://www.youtube.com/watch?v=VO1bQo4PXV4), where a CNN changes its predicted class between a duck and a rabbit based on the rotation of the image.\\n'), ('What are augmentations? Why do we need them? üë∂', 'Augmentations are an artifical way of expanding the existing datasets by performing some transformations, color shifts or many other things on the data. It helps in diversifying the data and even increasing the data when there is scarcity of data for a model to train on.  \\n'), ('What kind of augmentations do you know? üë∂', 'There are many kinds of augmentations which can be used according to the type of data you are working on some of which are geometric and numerical transformation, PCA, cropping, padding, shifting, noise injection etc.\\n'), ('How to choose which augmentations to use? \\u200d‚≠êÔ∏è', 'Augmentations really depend on the type of output classes and the features you want your model to learn. For eg. if you have mostly properly illuminated images in your dataset and want your model to predict poorly illuminated images too, you can apply channel shifting on your data and include the resultant images in your dataset for better results.\\n'), ('What kind of CNN architectures for classification do you know? üöÄ', 'Image Classification\\n* Inception v3\\n* Xception \\n* DenseNet\\n* AlexNet\\n* VGG16\\n* ResNet\\n* SqueezeNet\\n* EfficientNet\\n* MobileNet\\n\\nThe last three are designed so they use smaller number of parameters which is helpful for edge AI. \\n'), ('What is transfer learning? How does it work? \\u200d‚≠êÔ∏è', 'Given a source domain D_S and learning task T_S, a target domain D_T and learning task T_T, transfer learning aims to help improve the learning of the target predictive function f_T in D_T using the knowledge in D_S and T_S, where D_S ‚â† D_T,or T_S ‚â† T_T. In other words, transfer learning enables to reuse knowledge coming from other domains or learning tasks.\\n\\nIn the context of CNNs, we can use networks that were pre-trained on popular datasets such as ImageNet. We then can use the weights of the layers that learn to represent features and combine them with a new set of layers that learns to map the feature representations to the given classes. Two popular strategies are either to freeze the layers that learn the feature representations completely, or to give them a smaller learning rate.\\n'), ('What is object detection? Do you know any architectures for that? üöÄ', 'Object detection is finding Bounding Boxes around objects in an image. \\nArchitectures :\\nYOLO, Faster RCNN, Center Net\\n'), ('What is object segmentation? Do you know any architectures for that? üöÄ', 'Object Segmentation is predicting masks. It does not differentiate objects. \\nArchitectures :\\nMask RCNN, UNet\\n'), ('How can we use machine learning for text classification? \\u200d‚≠êÔ∏è', 'Machine learning classification algorithms predict a class based on a numerical feature representation. This means that in order to use machine learning for text classification, we need to extract numerical features from our text data first before we can apply machine learning algorithms. Common approaches to extract numerical features from text data are bag of words, N-grams or word embeddings.\\n'), ('What is bag of words? How we can use it for text classification? \\u200d‚≠êÔ∏è', 'Bag of Words is a representation of text that describes the occurrence of words within a document. The order or structure of the words is not considered. For text classification, we look at the histogram of the words within the text and consider each word count as a feature.\\n'), ('What are the advantages and disadvantages of bag of words? \\u200d‚≠êÔ∏è', 'Advantages:\\n1. Simple to understand and implement.\\n\\nDisadvantages:\\n1. The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\\n2. Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons\\n3. Discarding word order ignores the context, and in turn meaning of words in the document. Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (‚Äúthis is interesting‚Äù vs ‚Äúis this interesting‚Äù), synonyms (‚Äúold bike‚Äù vs ‚Äúused bike‚Äù).\\n'), ('What are N-grams? How can we use them? \\u200d‚≠êÔ∏è', 'The function to tokenize into consecutive sequences of words is called n-grams. It can be used to find out N most co-occurring words (how often word X is followed by word Y) in a given sentence.\\n'), ('How large should be N for our bag of words when using N-grams? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('What is TF-IDF? How is it useful for text classification? \\u200d‚≠êÔ∏è', 'Term Frequency (TF) is a scoring of the frequency of the word in the current document. Inverse Document Frequency(IDF) is a scoring of how rare the word is across documents. It is used in scenario where highly recurring words may not contain as much informational content as the domain specific words. For example, words like ‚Äúthe‚Äù that are frequent across all documents therefore need to be less weighted. The TF-IDF score highlights words that are distinct (contain useful information) in a given document.  \\n'), ('Which model would you use for text classification with bag of words features? \\u200d‚≠êÔ∏è', '1. Bag Of Words model\\n2. Word2Vec Embeddings\\n3. fastText Embeddings\\n4. Convolutional Neural Networks (CNN)\\n5. Long Short-Term Memory (LSTM)\\n6. Bidirectional Encoder Representations from Transformers (BERT)\\n'), ('Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words? \\u200d‚≠êÔ∏è', 'Usually logistic regression is better because bag of words creates a matrix with large number of columns. For a huge number of columns logistic regression is usually faster than gradient boosting trees.\\n'), ('What are word embeddings? Why are they useful? Do you know Word2Vec? \\u200d‚≠êÔ∏è', 'Word Embeddings are vector representations for words. Each word is mapped to one vector, this vector tries to capture some characteristics of the word, allowing similar words to have similar vector representations.  \\nWord Embeddings helps in capturing the inter-word semantics and represents it in real-valued vectors.  \\n'), ('Do you know any other ways to get word embeddings? üöÄ', '- GloVe\\n- BERT'), ('If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it? \\u200d‚≠êÔ∏è', 'Approaches ranked from simple to more complex:\\n\\n1. Take an average over all words\\n2. Take a weighted average over all words. Weighting can be done by inverse document frequency (idf part of tf-idf).\\n3. Use ML model like LSTM or Transformer.\\n'), ('Would you prefer gradient boosting trees model or logistic regression when doing text classification with embeddings? \\u200d‚≠êÔ∏è', 'Gradient boosting trees (GBTs) are generally a better choice than logistic regression for text classification with embeddings. This is because GBTs are able to learn more complex relationships between the features in the data, including the features extracted from the embeddings.\\n\\nLogistic regression is a linear model, which means that it can only learn linear relationships between the features. This can be a limitation for text classification, where the relationships between the features are often complex and non-linear.\\n\\nGBTs, on the other hand, are able to learn non-linear relationships between the features by combining multiple decision trees. This allows GBTs to learn more complex patterns in the data, which can lead to better performance on text classification tasks.\\n\\nIn addition, GBTs are more robust to outliers and noise in the data than logistic regression. This can be important for text classification tasks, where the data can be noisy and imbalanced.\\n\\nOverall, GBTs are a better choice than logistic regression for text classification with embeddings, especially when the data is noisy or imbalanced. However, it is important to consider the computational cost and interpretability of GBTs before using them.\\n'), ('How can you use neural nets for text classification? üöÄ', 'Here is a general overview of how to use neural nets for text classification:\\n\\nPreprocess the text: This includes cleaning the text by removing stop words, punctuation, and other irrelevant symbols. It may also involve converting the text to lowercase and stemming or lemmatizing the words.\\nRepresent the text as a vector: This can be done using a variety of methods, such as one-hot encoding or word embeddings.\\nBuild the neural net: The neural net architecture will depend on the specific text classification task. However, a typical architecture will include an embedding layer, one or more hidden layers, and an output layer.\\nTrain the neural net: The neural net is trained by feeding it labeled examples of text data. The neural net will learn to adjust its parameters in order to minimize the loss function, which is typically the cross-entropy loss function.\\nEvaluate the neural net: Once the neural net is trained, it can be evaluated on a held-out test set to assess its performance.\\nHere are some specific examples of how neural nets can be used for text classification:\\nSentiment analysis, Spam detection, Topic classification, Language identification\\n\\nNeural nets have achieved state-of-the-art results on many text classification tasks. However, they can be computationally expensive to train and deploy. \\n'), ('How can we use CNN for text classification? üöÄ', 'Here are some specific examples of how CNNs can be used for text classification:\\n\\nSentiment analysis: CNNs can be used to classify text as positive, negative, or neutral sentiment. This is a common task in social media analysis and customer service.\\nSpam detection: CNNs can be used to classify emails as spam or not spam. This is a common task in email filtering systems.\\nTopic classification: CNNs can be used to classify text documents into different topics. This is a common task in news and social media analysis.\\nLanguage identification: CNNs can be used to identify the language of a text document. This is a common task in translation systems.\\n'), ('What is unsupervised learning? üë∂', 'Unsupervised learning aims to detect patterns in data where no labels are given.\\n'), ('What is clustering? When do we need it? üë∂', 'Clustering algorithms group objects such that similar feature points are put into the same groups (clusters) and dissimilar feature points are put into different clusters.\\n'), ('Do you know how K-means works? \\u200d‚≠êÔ∏è', '1. Partition points into k subsets.\\n2. Compute the seed points as the new centroids of the clusters of the current partitioning.\\n3. Assign each point to the cluster with the nearest seed point.\\n4. Go back to step 2 or stop when the assignment does not change.\\n'), ('How to select K for K-means? \\u200d‚≠êÔ∏è', '* Domain knowledge, i.e. an expert knows the value of k\\n* Elbow method: compute the clusters for different values of k, for each k, calculate the total within-cluster sum of square, plot the sum according to the number of clusters and use the band as the number of clusters.\\n* Average silhouette method: compute the clusters for different values of k, for each k, calculate the average silhouette of observations, plot the silhouette according to the number of clusters and select the maximum as the number of clusters.\\n'), ('What are the other clustering algorithms do you know? \\u200d‚≠êÔ∏è', '* k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise.\\n* Agglomerative Hierarchical Clustering (AHC): hierarchical clusters combining the nearest clusters starting with each point as its own cluster.\\n* DIvisive ANAlysis Clustering (DIANA): hierarchical clustering starting with one cluster containing all points and splitting the clusters until each point describes its own cluster.\\n* Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Cluster defined as maximum set of density-connected points.\\n'), ('Do you know how DBScan works? \\u200d‚≠êÔ∏è', '* Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood)\\n* Cluster defined as maximum set of density-connected points.\\n* Points p_j and p_i are density-connected w.r.t. epsilon and minPts if there is a point o such that both, i and j are density-reachable from o w.r.t. epsilon and minPts.\\n* p_j is density-reachable from p_i w.r.t. epsilon, minPts if there is a chain of points p_i -> p_i+1 -> p_i+x = p_j such that p_i+x is directly density-reachable from p_i+x-1.\\n* p_j is a directly density-reachable point of the neighborhood of p_i if dist(p_i,p_j) <= epsilon.\\n'), ('When would you choose K-means and when DBScan? \\u200d‚≠êÔ∏è', '* DBScan is more robust to noise.\\n* DBScan is better when the amount of clusters is difficult to guess.\\n* K-means has a lower complexity, i.e. it will be much faster, especially with a larger amount of points.\\n'), ('What is the curse of dimensionality? Why do we care about it? \\u200d‚≠êÔ∏è', 'Data in only one dimension is relatively tightly packed. Adding a dimension stretches the points across that dimension, pushing them further apart. Additional dimensions spread the data even further making high dimensional data extremely sparse. We care about it, because it is difficult to use machine learning in sparse spaces.\\n'), ('Do you know any dimensionality reduction techniques? \\u200d‚≠êÔ∏è', '* Singular Value Decomposition (SVD)\\n* Principal Component Analysis (PCA)\\n* Linear Discriminant Analysis (LDA)\\n* T-distributed Stochastic Neighbor Embedding (t-SNE)\\n* Autoencoders\\n* Fourier and Wavelet Transforms\\n'), ('What‚Äôs singular value decomposition? How is it typically used for machine learning? \\u200d‚≠êÔ∏è', '* Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Œ£ (diagonal matrix) and R^T (right singular values).\\n* For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive.\\n* Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction.\\n'), ('What is the ranking problem? Which models can you use to solve them? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('What are good unsupervised baselines for text information retrieval? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('How would you evaluate your ranking algorithms? Which offline metrics would you use? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('What is precision and recall at k? \\u200d‚≠êÔ∏è', 'Precision at k and recall at k are evaluation metrics for ranking algorithms. Precision at k shows the share of relevant items in the first *k* results of the ranking algorithm. And Recall at k indicates the share of relevant items returned in top *k* results out of all correct answers for a given query.\\n\\nExample:\\nFor a search query \"Car\" there are 3 relevant products in your shop. Your search algorithm returns 2 of those relevant products in the first 5 search results.\\nPrecision at 5 = # num of relevant products in search result / k = 2/5 = 40%\\nRecall at 5 = # num of relevant products in search result / # num of all relevant products = 2/3 = 66.6%\\n'), ('What is mean average precision at k? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('How can we use machine learning for search? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('How can we get training data for our ranking algorithms? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('Can we formulate the search problem as a classification problem? How? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('How can we use clicks data as the training data for ranking algorithms? üöÄ', 'Answer here\\n'), ('Do you know how to use gradient boosting trees for ranking? üöÄ', 'Answer here\\n'), ('How do you do an online evaluation of a new ranking algorithm? \\u200d‚≠êÔ∏è', 'Answer here\\n'), ('What is a recommender system? üë∂', 'Recommender systems are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user.\\n'), ('What are good baselines when building a recommender system? \\u200d‚≠êÔ∏è', '* A good recommer system should give relevant and personalized information.\\n* It should not recommend items the user knows well or finds easily.\\n* It should make diverse suggestions.\\n* A user should explore new items.\\n'), ('What is collaborative filtering? \\u200d‚≠êÔ∏è', \"* Collaborative filtering is the most prominent approach to generate recommendations.\\n* It uses the wisdom of the crowd, i.e. it gives recommendations based on the experience of others.\\n* A recommendation is calculated as the average of other experiences.\\n* Say we want to give a score that indicates how much user u will like an item i. Then we can calculate it with the experience of N other users U as r_ui = 1/N * sum(v in U) r_vi.\\n* In order to rate similar experiences with a higher weight, we can introduce a similarity between users that we use as a multiplier for each rating.\\n* Also, as users have an individual profile, one user may have an average rating much larger than another user, so we use normalization techniques (e.g. centering or Z-score normalization) to remove the users' biases.\\n* Collaborative filtering does only need a rating matrix as input and improves over time. However, it does not work well on sparse data, does not work for cold starts (see below) and usually tends to overfit.\\n\"), ('How we can incorporate implicit feedback (clicks, etc) into our recommender systems? \\u200d‚≠êÔ∏è', \"In comparison to explicit feedback, implicit feedback datasets lack negative examples. For example, explicit feedback can be a positive or a negative rating, but implicit feedback may be the number of purchases or clicks. One popular approach to solve this problem is named weighted alternating least squares (wALS) [Hu, Y., Koren, Y., & Volinsky, C. (2008, December). Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on (pp. 263-272). IEEE.]. Instead of modeling the rating matrix directly, the numbers (e.g. amount of clicks) describe the strength in observations of user actions. The model tries to find latent factors that can be used to predict the expected preference of a user for an item.\\n\"), ('What is the cold start problem? \\u200d‚≠êÔ∏è', 'Collaborative filterung incorporates crowd knowledge to give recommendations for certain items. Say we want to recommend how much a user will like an item, we then will calculate the score using the recommendations of other users for this certain item. We can distinguish between two different ways of a cold start problem now. First, if there is a new item that has not been rated yet, we cannot give any recommendation. Also, when there is a new user, we cannot calculate a similarity to any other user.\\n'), ('Possible approaches to solving the cold start problem? \\u200d‚≠êÔ∏èüöÄ', '* Content-based filtering incorporates features about items to calculate a similarity between them. In this way, we can recommend items that have a high similarity to items that a user liked already. In this way, we are not dependent on the ratings of other users for a given item anymore and solve the cold start problem for new items.\\n* Demographic filtering incorporates user profiles to calculate a similarity between them and solves the cold start problem for new users.\\n'), ('What is a time series? üë∂', 'A time series is a set of observations ordered in time usually collected at regular intervals.\\n'), ('How is time series different from the usual regression problem? üë∂', 'The principle behind causal forecasting is that the value that has to be predicted is dependant on the input features (causal factors). In time series forecasting, the to be predicted value is expected to follow a certain pattern over time.\\n'), ('Which models do you know for solving time series problems? \\u200d‚≠êÔ∏è', '* Simple Exponential Smoothing: approximate the time series with an exponential function\\n* Trend-Corrected Exponential Smoothing (Holt‚Äòs Method): exponential smoothing that also models the trend\\n* Trend- and Seasonality-Corrected Exponential Smoothing (Holt-Winter‚Äòs Method): exponential smoothing that also models trend and seasonality\\n* Time Series Decomposition: decomposed a time series into the four components trend, seasonal variation, cycling variation and irregular component\\n* Autoregressive models: similar to multiple linear regression, except that the dependent variable y_t depends on its own previous values rather than other independent variables.\\n* Deep learning approaches (RNN, LSTM, etc.)\\n'), ('If there‚Äôs a trend in our series, how we can remove it? And why would we want to do it? \\u200d‚≠êÔ∏è', \"We can explicitly model the trend (and/or seasonality) with approaches such as Holt's Method or Holt-Winter's Method. We want to explicitly model the trend to reach the stationarity property for the data. Many time series approaches require stationarity. Without stationarity,the interpretation of the results of these analyses is problematic [Manuca, Radu & Savit, Robert. (1996). Stationarity and nonstationarity in time series analysis. Physica D: Nonlinear Phenomena. 99. 134-161. 10.1016/S0167-2789(96)00139-X. ].\\n\"), ('You have a series with only one variable ‚Äúy‚Äù measured at time t. How do predict ‚Äúy‚Äù at time t+1? Which approaches would you use? \\u200d‚≠êÔ∏è', 'We want to look at the correlation between different observations of y. This measure of correlation is called autocorrelation. Autoregressive models are multiple regression models where the time-lag series of the original time series are treated like multiple independent variables.\\n'), ('You have a series with a variable ‚Äúy‚Äù and a set of features. How do you predict ‚Äúy‚Äù at t+1? Which approaches would you use? \\u200d‚≠êÔ∏è', 'Given the assumption that the set of features gives a meaningful causation to y, a causal forecasting approach such as linear regression or multiple nonlinear regression might be useful. In case there is a lot of data and the explainability of the results is not a high priority, we can also consider deep learning approaches.\\n'), ('What are the problems with using trees for solving time series problems? \\u200d‚≠êÔ∏è', 'Random Forest models are not able to extrapolate time series data and understand increasing/decreasing trends. It will provide us with average data points if the validation data has values greater than the training data points.\\n')]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c21ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_PATTERN = r'(?m)^##\\s(.*)\\n([\\s\\S]*?)(?=^##|\\Z)'\n",
    "QA_PATTERN = r'\\*\\*(.*?)\\*\\*\\n\\n([\\s\\S]*?)(?=\\n<br/>\\n|\\Z)'\n",
    "\n",
    "def infer_difficulty(question: str) -> str:\n",
    "    if \"üë∂\" in question:\n",
    "        return \"easy\"\n",
    "    if \"‚≠ê\" in question:\n",
    "        return \"hard\"\n",
    "    return \"medium\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for section_title, section_body in re.findall(SECTION_PATTERN, content):\n",
    "    topic = section_title.strip().lower().replace(\" \", \"_\")\n",
    "\n",
    "    for question, answer in re.findall(QA_PATTERN, section_body):\n",
    "        results.append({\n",
    "            \"text\": question.strip(),\n",
    "            \"reference_answer\": answer.strip(),\n",
    "            \"topic\": topic,\n",
    "            \"difficulty\": infer_difficulty(question)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2db5a1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'What is supervised machine learning? üë∂',\n",
       "  'reference_answer': 'Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the\\xa0input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.\\n\\nk-Nearest Neighbors(KNN):Looking at the k closest labeled data points',\n",
       "  'topic': 'supervised_machine\\xa0learning',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is regression? Which models can you use to solve a regression problem? üë∂',\n",
       "  'reference_answer': 'Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor).\\nHere are some common regression models\\n\\n- *Linear Regression* establishes a linear relationship between target and predictor (s). It predicts a numeric value and has a shape of a straight line.\\n- *Polynomial Regression* has a regression equation with the power of independent variable more than 1. It is a curve that fits into the data points.\\n- *Ridge Regression* helps when predictors are highly correlated (multicollinearity problem). It penalizes the squares of regression coefficients but doesn‚Äôt allow the coefficients to reach zeros (uses L2 regularization).\\n- *Lasso Regression* penalizes the absolute values of regression coefficients and allows some of the coefficients to reach absolute zero (thereby allowing feature selection).',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is linear regression? When do we use it? üë∂',\n",
       "  'reference_answer': 'Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).\\n\\nWith a simple equation:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```\\n\\nB is regression coefficients, x values are the independent (explanatory) variables  and y is dependent variable.\\n\\nThe case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\\n\\nSimple linear regression:\\n\\n```\\ny = B0 + B1*x1\\n```\\n\\nMultiple linear regression:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What are the main assumptions of linear regression? ‚≠ê',\n",
       "  'reference_answer': 'There are several assumptions of linear regression. If any of them is violated, model predictions and interpretation may be worthless or misleading.\\n\\n1. **Linear relationship** between features and target variable.\\n2. **Additivity** means that the effect of changes in one of the features on the target variable does not depend on values of other features. For example, a model for predicting revenue of a company have of two features - the number of items _a_ sold and the number of items _b_ sold. When company sells more items _a_ the revenue increases and this is independent of the number of items _b_ sold. But, if customers who buy _a_ stop buying _b_, the additivity assumption is violated.\\n3. Features are not correlated (no **collinearity**) since it can be difficult to separate out the individual effects of collinear features on the target variable.\\n4. Errors are independently and identically normally distributed (y<sub>i</sub> = B0 + B1*x1<sub>i</sub> + ... + error<sub>i</sub>):\\n   1. No correlation between errors (consecutive errors in the case of time series data).\\n   2. Constant variance of errors - **homoscedasticity**. For example, in case of time series, seasonal patterns can increase errors in seasons with higher activity.\\n   3. Errors are normally distributed, otherwise some features will have more influence on the target variable than to others. If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What‚Äôs the normal distribution? Why do we care about it? üë∂',\n",
       "  'reference_answer': 'The normal distribution is a continuous probability distribution whose probability density function takes the following formula:\\n\\n![formula](https://mathworld.wolfram.com/images/equations/NormalDistribution/NumberedEquation1.gif)\\n\\nwhere Œº is the mean and œÉ is the standard deviation of the distribution.\\n\\nThe normal distribution derives its importance from the **Central Limit Theorem**, which states that if we draw a large enough number of samples, their mean will follow a normal distribution regardless of the initial distribution of the sample, i.e **the distribution of the mean of the samples is normal**. It is important that each sample is independent from the other.\\n\\nThis is powerful because it helps us study processes whose population distribution is unknown to us.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How do we check if a variable follows the normal distribution? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '1. Plot a histogram out of the sampled data. If you can fit the bell-shaped \"normal\" curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected.\\n2. Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution.\\n3. Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously.\\n4. Check for Quantile-Quantile plot. It is a scatterplot created by plotting two sets of quantiles against one another. Normal Q-Q plot place the data points in a roughly straight line.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"Data is not normal. Specially, real-world datasets or uncleaned datasets always have certain skewness. Same goes for the price prediction. Price of houses or any other thing under consideration depends on a number of factors. So, there's a great chance of presence of some skewed values i.e outliers if we talk in data science terms. \\n\\nYes, you may need to do pre-processing. Most probably, you will need to remove the outliers to make your distribution near-to-normal.\",\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What methods for solving linear regression do you know? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"To solve linear regression, you need to find the coefficients $`\\\\beta`$ which minimize the sum of squared errors.\\n\\nMatrix Algebra method: Let's say you have `X`, a matrix of features, and `y`, a vector with the values you want to predict. After going through the matrix algebra and minimization problem, you get this solution: $`\\\\beta = (X^{T}X)^{-1}X^{T}y`$. \\n\\nBut solving this requires you to find an inverse, which can be time-consuming, if not impossible. Luckily, there are methods like Singular Value Decomposition (SVD) or QR Decomposition that can reliably calculate this part $`(X^{T}X)^{-1}X^{T}`$ (called the pseudo-inverse) without actually needing to find an inverse. The popular python ML library `sklearn` uses SVD to solve least squares.\\n\\nAlternative method: Gradient Descent. See explanation below.\",\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is gradient descent? How does it work? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Gradient descent is an algorithm that uses calculus concept of gradient to try and reach local or global minima. It works by taking the negative of the gradient in a point of a given function, and updating that point repeatedly using the calculated negative gradient, until the algorithm reaches a local or global minimum, which will cause future iterations of the algorithm to return values that are equal or too close to the current point. It is widely used in machine learning applications.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is the normal equation? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Normal equations are equations obtained by setting equal to zero the partial derivatives of the sum of squared errors (least squares); normal equations allow one to estimate the parameters of a multiple linear regression.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is SGD \\u200a‚Äî\\u200a stochastic gradient descent? What‚Äôs the difference with the usual gradient descent? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function.\\n\\nThe difference lies in how the gradient of the loss function is estimated. In the usual GD, you have to run through ALL the samples in your training set in order to estimate the gradient and do a single update for a parameter in a particular iteration. In SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to estimate the gradient and do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Which metrics for evaluating regression models do you know? üë∂',\n",
       "  'reference_answer': '1. Mean Squared Error(MSE)\\n2. Root Mean Squared Error(RMSE)\\n3. Mean Absolute Error(MAE)\\n4. R¬≤ or Coefficient of Determination\\n5. Adjusted R¬≤',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What are MSE and RMSE? üë∂',\n",
       "  'reference_answer': 'MSE stands for <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror while RMSE stands for <strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror. They are metrics with which we can evaluate models.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is the bias-variance trade-off? üë∂',\n",
       "  'reference_answer': '**Bias** is the error introduced by approximating the true underlying function, which can be quite complex, by a simpler model. **Variance** is a model sensitivity to changes in the training dataset.\\n\\n**Bias-variance trade-off** is a relationship between the expected test error and the variance and the bias - both contribute to the level of the test error and ideally should be as small as possible:\\n\\n```\\nExpectedTestError = Variance + Bias¬≤ + IrreducibleError\\n```\\n\\nBut as a model complexity increases, the bias decreases and the variance increases which leads to *overfitting*. And vice versa, model simplification helps to decrease the variance but it increases the bias which leads to *underfitting*.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is overfitting? üë∂',\n",
       "  'reference_answer': \"When your model perform very well on your training set but can't generalize the test set, because it adjusted a lot to the training set.\",\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How to validate your models? üë∂',\n",
       "  'reference_answer': 'One of the most common approaches is splitting data into train, validation and test parts.\\nModels are trained on train data, hyperparameters (for example early stopping) are selected based on the validation data, the final measurement is done on test dataset.\\nAnother approach is cross-validation: split dataset into K folds and each time train models on training folds and measure the performance on the validation folds.\\nAlso you could combine these approaches: make a test/holdout dataset and do cross-validation on the rest of the data. The final quality is measured on test dataset.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Why do we need to split our data into three parts: train, validation, and test? üë∂',\n",
       "  'reference_answer': 'The training set is used to fit the model, i.e. to train the model with the data. The validation set is then used to provide an unbiased evaluation of a model while fine-tuning hyperparameters. This improves the generalization of the model. Finally, a test data set which the model has never \"seen\" before should be used for the final evaluation of the model. This allows for an unbiased evaluation of the model. The evaluation should never be performed on the same data that is used for training. Otherwise the model performance would not be representative.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Can you explain how cross-validation works? üë∂',\n",
       "  'reference_answer': 'Cross-validation is the process to separate your total training set into two subsets: training and validation set, and evaluate your model to choose the hyperparameters. But you do this process iteratively, selecting different training and validation set, in order to reduce the bias that you would have by selecting only one validation set.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is K-fold cross-validation? üë∂',\n",
       "  'reference_answer': 'K fold cross validation is a method of cross validation where we select a hyperparameter k. The dataset is now divided into k parts. Now, we take the 1st part as validation set and remaining k-1 as training set. Then we take the 2nd part as validation set and remaining k-1 parts as training set. Like this, each part is used as validation set once and the remaining k-1 parts are taken together and used as training set.\\nIt should not be used in a time series data.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How do we choose K in K-fold cross-validation? What‚Äôs your favorite K? üë∂',\n",
       "  'reference_answer': 'There are two things to consider while deciding K: the number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained. <br/>\\nI tend to use 4 for small datasets and 5 for large ones as K.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is classification? Which models would you use to solve a classification problem? üë∂',\n",
       "  'reference_answer': 'Classification problems are problems in which our prediction space is discrete, i.e. there is a finite number of values the output variable can be. Some models which can be used to solve classification problems are: logistic regression, decision tree, random forests, multi-layer perceptron, one-vs-all, amongst others.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is logistic regression? When do we need to use it? üë∂',\n",
       "  'reference_answer': 'Logistic regression is a Machine Learning algorithm that is used for binary classification. You should use logistic regression when your Y variable takes only two values, e.g. True and False, \"spam\" and \"not spam\", \"churn\" and \"not churn\" and so on. The variable is said to be a \"binary\" or \"dichotomous\".',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Is logistic regression a linear model? Why? üë∂',\n",
       "  'reference_answer': 'Yes, Logistic Regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is sigmoid? What does it do? üë∂',\n",
       "  'reference_answer': 'A sigmoid function is a type of activation function, and more specifically defined as a squashing function. Squashing functions limit the output to a range between 0 and 1, making these functions useful in the prediction of probabilities.\\n\\nSigmod(x) = 1/(1+e^{-x})',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How do we evaluate classification models? üë∂',\n",
       "  'reference_answer': 'Depending on the classification problem, we can use the following evaluation metrics:\\n\\n1. Accuracy\\n2. Precision\\n3. Recall\\n4. F1 Score\\n5. Logistic loss (also known as Cross-entropy loss)\\n6. Jaccard similarity coefficient score',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is accuracy? üë∂',\n",
       "  'reference_answer': 'Accuracy is a metric for evaluating classification models. It is calculated by dividing the number of correct predictions by the number of total predictions.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Is accuracy always a good metric? üë∂',\n",
       "  'reference_answer': 'Accuracy is not a good performance metric when there is imbalance in the dataset. For example, in binary classification with 95% of A class and 5% of B class, a constant prediction of A class would have an accuracy of 95%. In case of imbalance dataset, we need to choose Precision, recall, or F1 Score depending on the problem we are trying to solve.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is the confusion table? What are the cells in this table? üë∂',\n",
       "  'reference_answer': 'Confusion table (or confusion matrix) shows how many True positives (TP), True Negative (TN), False Positive (FP) and False Negative (FN) model has made.\\n\\n||                |     Actual   |        Actual |\\n|:---:|   :---:        |     :---:    |:---:          |\\n||                | Positive (1) | Negative (0)  |\\n|Predicted|   Positive (1) | TP           | FP            |\\n|Predicted|   Negative (0) | FN           | TN            |\\n\\n* True Positives (TP): When the actual class of the observation is 1 (True) and the prediction is 1 (True)\\n* True Negative (TN): When the actual class of the observation is 0 (False) and the prediction is 0 (False)\\n* False Positive (FP): When the actual class of the observation is 0 (False) and the prediction is 1 (True)\\n* False Negative (FN): When the actual class of the observation is 1 (True) and the prediction is 0 (False)\\n\\nMost of the performance metrics for classification models are based on the values of the confusion matrix.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What are precision, recall, and F1-score? üë∂',\n",
       "  'reference_answer': '* Precision and recall are classification evaluation metrics:\\n* P = TP / (TP + FP) and R = TP / (TP + FN).\\n* Where TP is true positives, FP is false positives and FN is false negatives\\n* In both cases the score of 1 is the best: we get no false positives or false negatives and only true positives.\\n* F1 is a combination of both precision and recall in one score (harmonic mean):\\n* F1 = 2 * PR / (P + R).\\n* Max F score is 1 and min is 0, with 1 being the best.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Precision-recall trade-off \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Tradeoff means increasing one parameter would lead to decreasing of other. Precision-recall tradeoff occur due to increasing one of the parameter(precision or recall) while keeping the model same. \\n\\nIn an ideal scenario where there is a perfectly separable data, both precision and recall can get maximum value of 1.0. But in most of the practical situations, there is noise in the dataset and the dataset is not perfectly separable. There might be some points of positive class closer to the negative class and vice versa. In such cases, shifting the decision boundary can either increase the precision or recall but not both. Increasing one parameter leads to decreasing of the other.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is the ROC curve? When to use it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'ROC stands for *Receiver Operating Characteristics*. The diagrammatic representation that shows the contrast between true positive rate vs false positive rate. It is used when we need to predict the probability of the binary outcome.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is AUC (AU ROC)? When to use it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"AUC stands for *Area Under the ROC Curve*. ROC is a probability curve and AUC represents degree or measure of separability. It's used when we need to value how much model is capable of distinguishing between classes.  The value is between 0 and 1, the higher the better.\",\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How to interpret the AU ROC score? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'AUC score is the value of *Area Under the ROC Curve*. \\n\\nIf we assume ROC curve consists of dots, $`(x_1, y_1), (x_2, y_2), \\\\cdots, (x_m,y_m)`$, then\\n\\n$`AUC = \\\\frac{1}{2} \\\\sum_{i=1}^{m-1}(x_{i+1}-x_i)\\\\cdot (y_i+y_{i+1})`$\\n\\nAn excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. When AUC score is 0.5, it means model has no class separation capacity whatsoever.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is the PR (precision-recall) curve? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'A *precision*-*recall curve* (or PR Curve) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. Precision-recall curves (PR curves) are recommended for highly skewed domains where ROC curves may provide an excessively optimistic view of the performance.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is the area under the PR curve? Is it a useful metric? \\u200d‚≠êÔ∏èI',\n",
       "  'reference_answer': 'The Precision-Recall AUC is just like the ROC AUC, in that it summarizes the curve with a range of threshold values as a single score.\\n\\nA high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'In which cases AU PR is better than AU ROC? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'What is different however is that AU ROC looks at a true positive rate TPR and false positive rate FPR while AU PR looks at positive predictive value PPV and true positive rate TPR.\\n\\nTypically, if true negatives are not meaningful to the problem or you care more about the positive class, AU PR is typically going to be more useful; otherwise, If you care equally about the positive and negative class or your dataset is quite balanced, then going with AU ROC is a good idea.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What do we do with categorical variables? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Categorical variables must be encoded before they can be used as features to train a machine learning model. There are various encoding techniques, including:\\n- One-hot encoding\\n- Label encoding\\n- Ordinal encoding\\n- Target encoding',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Why do we need one-hot encoding? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'If we simply encode categorical variables with a Label encoder, they become ordinal which can lead to undesirable consequences. In this case, linear models will treat category with id 4 as twice better than a category with id 2. One-hot encoding allows us to represent a categorical variable in a numerical vector space which ensures that vectors of each category have equal distances between each other. This approach is not suited for all situations, because by using it with categorical variables of high cardinality (e.g. customer id) we will encounter problems that come into play because of the curse of dimensionality.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is \"curse of dimensionality\"? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'The curse of dimensionality is an issue that arises when working with high-dimensional data. It is often said that \"the curse of dimensionality\" is one of the main problems with machine learning. The curse of dimensionality refers to the fact that, as the number of dimensions (features) in a data set increases, the number of data points required to accurately learn the relationships between those features increases exponentially.\\n\\nA simple example where we have a data set with two features, x1 and x2. If we want to learn the relationship between these two features, we need to have enough data points so that we can accurately estimate the parameters of that relationship. However, if we add a third feature, x3, then the number of data points required to accurately learn the relationships between all three features increases exponentially. This is because there are now more parameters to estimate, and the number of data points needed to accurately estimate those parameters increases exponentially with the number of parameters.\\n\\nSimply put, the curse of dimensionality basically means that the error increases with the increase in the number of features.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What happens to our linear regression model if we have three columns in our data: x, y, z \\u200a‚Äî\\u200a and z is a sum of x and y? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'We would not be able to perform the regression. Because z is linearly dependent on x and y so when performing the regression $`{X}^{T}{X}`$ would be a singular (not invertible) matrix.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What happens to our linear regression model if the column z in the data is a sum of columns x and y and some random noise? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'It creates a situation known as multicollinearity. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, leading to issues in the estimation of the regression coefficients. The issue arises because the variables X and Y will be highly correlated, making it difficult for the model to distinguish the individual effects of X and Y on the dependent variable Z. To address it, feature selection, PCA or regularization techniques (L2) may be used.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is regularization? Why do we need it? üë∂',\n",
       "  'reference_answer': 'Regularization is used to reduce overfitting in machine learning models. It helps the models to generalize well and make them robust to outliers and noise in the data.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Which regularization techniques do you know? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'There are mainly two types of regularization,\\n1. L1 Regularization (Lasso regularization) - Adds the sum of absolute values of the coefficients to the cost function. $`\\\\lambda\\\\sum_{i=1}^{n} \\\\left | w_i \\\\right |`$\\n2. L2 Regularization (Ridge regularization) - Adds the sum of squares of coefficients to the cost function. $`\\\\lambda\\\\sum_{i=1}^{n} {w_{i}}^{2}`$\\n\\n* Where $`\\\\lambda`$ determines the amount of regularization.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What kind of regularization techniques are applicable to linear models? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'AIC/BIC, Ridge regression, Lasso, Elastic Net, Basis pursuit denoising, Rudin‚ÄìOsher‚ÄìFatemi model (TV), Potts model, RLAD,\\nDantzig Selector,SLOPE',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How does L2 regularization look like in a linear model? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'L2 regularization adds a penalty term to our cost function which is equal to the sum of squares of models coefficients multiplied by a lambda hyperparameter. This technique makes sure that the coefficients are close to zero and is widely used in cases when we have a lot of features that might correlate with each other.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How do we select the right regularization parameters? üë∂',\n",
       "  'reference_answer': 'Regularization parameters can be chosen using a grid search, for example https://scikit-learn.org/stable/modules/linear_model.html has one formula for the implementing for regularization, alpha in the formula mentioned can be found by doing a RandomSearch or a GridSearch on a set of values and selecting the alpha which gives the least cross validation or validation error.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What‚Äôs the effect of L2 regularization on the weights of a linear model? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'L2 regularization penalizes larger weights more severely (due to the squared penalty term), which encourages weight values to decay toward zero.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How L1 regularization looks like in a linear model? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'L1 regularization adds a penalty term to our cost function which is equal to the sum of modules of models coefficients multiplied by a lambda hyperparameter. For example, cost function with L1 regularization will look like: $`\\\\sum_{i=0}^{N} (y_i - \\\\sum_{j=0}^{M} x_{ij} * w_j)+\\\\lambda\\\\sum_{j=0}^{M} \\\\left | w_j \\\\right |`$',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What‚Äôs the difference between L2 and L1 regularization? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '- Penalty terms: L1 regularization uses the sum of the absolute values of the weights, while L2 regularization uses the sum of the weights squared.\\n- Feature selection: L1 performs feature selection by reducing the coefficients of some predictors to 0, while L2 does not.\\n- Computational efficiency: L2 has an analytical solution, while L1 does not.\\n- Multicollinearity: L2 addresses multicollinearity by constraining the coefficient norm.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Can we have both L1 and L2 regularization components in a linear model? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Yes, elastic net regularization combines L1 and L2 regularization.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What‚Äôs the interpretation of the bias term in linear models? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Bias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How do we interpret weights in linear models? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Without normalizing weights or variables, if you increase the corresponding predictor by one unit, the coefficient represents on average how much the output changes. By the way, this interpretation still works for logistic regression - if you increase the corresponding predictor by one unit, the weight represents the change in the log of the odds.\\n\\nIf the variables are normalized, we can interpret weights in linear models like the importance of this variable in the predicted result.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'If a weight for one variable is higher than for another \\u200a‚Äî\\u200a can we say that this variable is more important? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"Yes - if your predictor variables are normalized.\\n\\nWithout normalization, the weight represents the change in the output per unit change in the predictor. If you have a predictor with a huge range and scale that is used to predict an output with a very small range - for example, using each nation's GDP to predict maternal mortality rates - your coefficient should be very small. That does not necessarily mean that this predictor variable is not important compared to the others.\",\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'When do we need to perform feature normalization for linear models? When it‚Äôs okay not to do it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"Feature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This can't be done effectively if every feature is scaled differently. \\n\\nLinear regression without regularization techniques can be used without feature normalization. Also, regularization can help to make the analytical solution more stable, ‚Äî it adds the regularization matrix to the feature matrix before inverting it.\",\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is feature selection? Why do we need it? üë∂',\n",
       "  'reference_answer': 'Feature Selection is a method used to select the relevant features for the model to train on. We need feature selection to remove the irrelevant features which leads the model to under-perform.',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Is feature selection important for linear models? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Yes, It is. It can make model performance better through selecting the most importance features and remove irrelevant features in order to make a prediction and it can also avoid overfitting, underfitting and bias-variance tradeoff.',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Which feature selection techniques do you know? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Here are some of the feature selections:\\n- Principal Component Analysis\\n- Neighborhood Component Analysis\\n- ReliefF Algorithm',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Can we use L1 regularization for feature selection? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Yes, because the nature of L1 regularization will lead to sparse coefficients of features. Feature selection can be done by keeping only features with non-zero coefficients.',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Can we use L2 regularization for feature selection? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'No, Because L2 regularization does not make the weights zero but only makes them very very small. L2 regularization can be used to solve multicollinearity since it stabilizes the model.',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are the decision trees? üë∂',\n",
       "  'reference_answer': 'This is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. \\n\\nIn this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible.\\n\\nA decision tree is a flowchart-like tree structure, where each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a value for the target variable.\\n\\nVarious techniques : like Gini, Information Gain, Chi-square, entropy.',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How do we train decision trees? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '1. Start at the root node.\\n2. For each variable X, find the set S_1 that minimizes the sum of the node impurities in the two child nodes and choose the split {X*,S*} that gives the minimum over all X and S.\\n3. If a stopping criterion is reached, exit. Otherwise, apply step 2 to each child node in turn.',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are the main parameters of the decision tree model? üë∂',\n",
       "  'reference_answer': '* maximum tree depth\\n* minimum samples per leaf node\\n* impurity criterion',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How do we handle categorical variables in decision trees? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Some decision tree algorithms can handle categorical variables out of the box, others cannot. However, we can transform categorical variables, e.g. with a binary or a one-hot encoder.',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are the benefits of a single decision tree compared to more complex models? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* easy to implement\\n* fast training\\n* fast inference\\n* good explainability',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How can we know which features are more important for the decision tree model? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Often, we want to find a split such that it minimizes the sum of the node impurities. The impurity criterion is a parameter of decision trees. Popular methods to measure the impurity are the Gini impurity and the entropy describing the information gain.',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is random forest? üë∂',\n",
       "  'reference_answer': 'Random Forest is a machine learning method for regression and classification which is composed of many decision trees. Random Forest belongs to a larger class of ML algorithms called ensemble methods (in other words, it involves the combination of several models to solve a single prediction problem).',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Why do we need randomization in random forest? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Random forest in an extension of the **bagging** algorithm which takes *random data samples from the training dataset* (with replacement), trains several models and averages predictions. In addition to that, each time a split in a tree is considered, random forest takes a *random sample of m features from full set of n features* (without replacement) and uses this subset of features as candidates for the split (for example, `m = sqrt(n)`).\\n\\nTraining decision trees on random data samples from the training dataset *reduces variance*. Sampling features for each split in a decision tree *decorrelates trees*.',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are the main parameters of the random forest model? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '- `max_depth`: Longest Path between root node and the leaf\\n- `min_sample_split`: The minimum number of observations needed to split a given node\\n- `max_leaf_nodes`: Conditions the splitting of the tree and hence, limits the growth of the trees\\n- `min_samples_leaf`: minimum number of samples in the leaf node\\n- `n_estimators`: Number of trees\\n- `max_sample`: Fraction of original dataset given to any individual tree in the given model\\n- `max_features`: Limits the maximum number of features provided to trees in random forest model',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How do we select the depth of the trees in random forest? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'The greater the depth, the greater amount of information is extracted from the tree, however, there is a limit to this, and the algorithm even if defensive against overfitting may learn complex features of noise present in data and as a result, may overfit on noise. Hence, there is no hard thumb rule in deciding the depth, but literature suggests a few tips on tuning the depth of the tree to prevent overfitting:\\n\\n- limit the maximum depth of a tree\\n- limit the number of test nodes\\n- limit the minimum number of objects at a node required to split\\n- do not split a node when, at least, one of the resulting subsample sizes is below a given threshold\\n- stop developing a node if it does not sufficiently improve the fit.',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How do we know how many trees we need in random forest? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'The number of trees in random forest is worked by n_estimators, and a random forest reduces overfitting by increasing the number of trees. There is no fixed thumb rule to decide the number of trees in a random forest, it is rather fine tuned with the data, typically starting off by taking the square of the number of features (n) present in the data followed by tuning until we get the optimal results.',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Is it easy to parallelize training of a random forest model? How can we do it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"Yes, R provides a simple way to parallelize training of random forests on large scale data.\\nIt makes use of a parameter called multicombine which can be set to TRUE for parallelizing random forest computations.\\n\\n```R\\nrf <- foreach(ntree=rep(25000, 6), .combine=randomForest::combine,\\n              .multicombine=TRUE, .packages='randomForest') %dopar% {\\n    randomForest(x, y, ntree=ntree)\\n}\\n```\",\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are the potential problems with many large trees? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"- Overfitting: A large number of large trees can lead to overfitting, where the model becomes too complex and is able to memorize the training data but doesn't generalize well to new, unseen data.\\n\\n- Slow prediction time: As the number of trees in the forest increases, the prediction time for new data points can become quite slow. This can be a problem when you need to make predictions in real-time or on a large dataset.\\n\\n- Memory consumption: Random Forest models with many large trees can consume a lot of memory, which can be a problem when working with large datasets or on a limited hardware.\\n\\n- Lack of interpretability: Random Forest models with many large trees can be difficult to interpret, making it harder to understand how the model is making predictions or what features are most important.\\n\\n- Difficulty in tuning : With an increasing number of large trees the tuning process becomes more complex and computationally expensive.\\n\\nIt's important to keep in mind that the number of trees in a Random Forest should be chosen based on the specific problem and dataset, rather than using a large number of trees by default. In practice, the number of trees in a random forest is chosen based on the trade-off between the computational cost and the performance.\",\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What if instead of finding the best split, we randomly select a few splits and just select the best from them. Will it work? üöÄ',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'What happens when we have correlated features in our data? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'In random forest, since random forest samples some features to build each tree, the information contained in correlated features is twice as much likely to be picked than any other information contained in other features. \\n\\nIn general, when you are adding correlated features, it means that they linearly contains the same information and thus it will reduce the robustness of your model. Each time you train your model, your model might pick one feature or the other to \"do the same job\" i.e. explain some variance, reduce entropy, etc.',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is gradient boosting trees? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What‚Äôs the difference between random forest and gradient boosting? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '1. Random Forests builds each tree independently while Gradient Boosting builds one tree at a time.\\n   2. Random Forests combine results at the end of the process (by averaging or \"majority rules\") while Gradient Boosting combines     results along the way.',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Is it possible to parallelize training of a gradient boosting model? How to do it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"Yes, different frameworks provide different options to make training faster, using GPUs to speed up the process by making it highly parallelizable.For example, for XGBoost <i>tree_method = 'gpu_hist'</i> option makes training faster by use of GPUs.\",\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Feature importance in gradient boosting trees \\u200a‚Äî\\u200a what are possible options? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'With CatBoost you can use implemented method get_feature_importance for getting SHAP values. https://arxiv.org/abs/1905.04610v1\\n\\nIt allows to understand how excluding features helps to provide better results. Higher value is better.\\n\\nAlso you can add random noise column to your data (with normal distribution), calculate feature importance and exclude all features below noise importance.',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Are there any differences between continuous and discrete variables when it comes to feature importance of gradient boosting models? üöÄ',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'What are the main parameters in the gradient boosting model? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'There are many parameters, but below are a few key defaults.\\n* learning_rate=0.1 (shrinkage).\\n* n_estimators=100 (number of trees).\\n* max_depth=3.\\n* min_samples_split=2.\\n* min_samples_leaf=1.\\n* subsample=1.0.',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How do you approach tuning parameters in XGBoost or LightGBM? üöÄ',\n",
       "  'reference_answer': 'Depending upon the dataset, parameter tuning can be done manually or using hyperparameter optimization frameworks such as optuna and hyperopt. In manual parameter tuning, we need to be aware of max-depth, min_samples_leaf and min_samples_split so that our model does not overfit the data but try to predict generalized characteristics of data (basically keeping variance and bias low for our model).',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'How do you select the number of trees in the gradient boosting model? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Most implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds or thousands. Using scikit-learn we can perform a grid search of the n_estimators model parameter',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Which hyper-parameter tuning strategies (in general) do you know? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'There are several strategies for hyper-tuning but I would argue that the three most popular nowadays are the following:\\n* <b>Grid Search</b> is an exhaustive approach such that for each hyper-parameter, the user needs to <i>manually</i> give a list of values for the algorithm to try. After these values are selected, grid search then evaluates the algorithm using each and every combination of hyper-parameters and returns the combination that gives the optimal result (i.e. lowest MAE). Because grid search evaluates the given algorithm using all combinations, it\\'s easy to see that this can be quite computationally expensive and can lead to sub-optimal results specifically since the user needs to specify specific values for these hyper-parameters, which is prone for error and requires domain knowledge.\\n\\n* <b>Random Search</b> is similar to grid search but differs in the sense that rather than specifying which values to try for each hyper-parameter, an upper and lower bound of values for each hyper-parameter is given instead. With uniform probability, random values within these bounds are then chosen and similarly, the best combination is returned to the user. Although this seems less intuitive, no domain knowledge is necessary and theoretically much more of the parameter space can be explored.\\n\\n* In a completely different framework, <b>Bayesian Optimization</b> is thought of as a more statistical way of optimization and is commonly used when using neural networks, specifically since one evaluation of a neural network can be computationally costly. In numerous research papers, this method heavily outperforms Grid Search and Random Search and is currently used on the Google Cloud Platform as well as AWS. Because an in-depth explanation requires a heavy background in bayesian statistics and gaussian processes (and maybe even some game theory), a \"simple\" explanation is that a much simpler/faster <i>acquisition function</i> intelligently chooses (using a <i>surrogate function</i> such as probability of improvement or GP-UCB) which hyper-parameter values to try on the computationally expensive, original algorithm. Using the result of the initial combination of values on the expensive/original function, the acquisition function takes the result of the expensive/original algorithm into account and uses it as its prior knowledge to again come up with another set of hyper-parameters to choose during the next iteration. This process continues either for a specified number of iterations or for a specified amount of time and similarly the combination of hyper-parameters that performs the best on the expensive/original algorithm is chosen.',\n",
       "  'topic': 'parameter_tuning',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What‚Äôs the difference between grid search parameter tuning strategy and random search? When to use one or another? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'For specifics, refer to the above answer.',\n",
       "  'topic': 'parameter_tuning',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What kind of problems neural nets can solve? üë∂',\n",
       "  'reference_answer': 'Neural nets are good at solving non-linear problems. Some good examples are problems that are relatively easy for humans (because of experience, intuition, understanding, etc), but difficult for traditional regression models: speech recognition, handwriting recognition, image identification, etc.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How does a usual fully-connected feed-forward neural network work? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'In a usual fully-connected feed-forward network, each neuron receives input from every element of the previous layer and thus the receptive field of a neuron is the entire previous layer. They are usually used to represent feature vectors for input data in classification problems but can be expensive to train because of the number of computations involved.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Why do we need activation functions? üë∂',\n",
       "  'reference_answer': 'The main idea of using neural networks is to learn complex nonlinear functions. If we are not using an activation function in between different layers of a neural network, we are just stacking up multiple linear layers one on top of another and this leads to learning a linear function. The Nonlinearity comes only with the activation function, this is the reason we need activation functions.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What are the problems with sigmoid as an activation function? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'The derivative of the sigmoid function for large positive or negative numbers is almost zero. From this comes the problem of vanishing gradient ‚Äî during the backpropagation our net will not learn (or will learn drastically slow). One possible way to solve this problem is to use ReLU activation function.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is ReLU? How is it better than sigmoid or tanh? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'ReLU is an abbreviation for Rectified Linear Unit. It is an activation function which has the value 0 for all negative values and the value f(x) = x for all positive values. The ReLU has a simple activation function which makes it fast to compute and while the sigmoid and tanh activation functions saturate at higher values, the ReLU has a potentially infinite activation, which addresses the problem of vanishing gradients.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How we can initialize the weights of a neural network? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Proper initialization of weight matrix in neural network is very necessary.\\nSimply we can say there are two ways for initializations.\\n   1. Initializing weights with zeroes.\\n      Setting weights to zero makes your network no better than a linear model. It is important to note that setting biases to 0 will not create any troubles as non zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron are still different.  \\n   2. Initializing weights randomly.\\n      Assigning random values to weights is better than just 0 assignment. \\n* a) If weights are initialized with very high values the term np.dot(W,X)+b becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where the slope of gradient changes slowly and learning takes a lot of time.\\n* b) If weights are initialized with low values it gets mapped to 0, where the case is the same as above. This problem is often referred to as the vanishing gradient.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What if we set all the weights of a neural network to 0? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'If all the weights of a neural network are set to zero, the output of each connection is same (W*x = 0). This means the gradients which are backpropagated to each connection in a layer is same. This means all the connections/weights learn the same thing, and the model never converges.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What regularization techniques for neural nets do you know? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* L1 Regularization - Defined as the sum of absolute values of the individual parameters. The L1 penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded. \\n* L2 Regularization - Defined as the sum of square of individual parameters. Often supported by regularization hyperparameter alpha. It results in weight decay. \\n* Data Augmentation - This requires some fake data to be created as a part of training set. \\n* Drop Out : This is most effective regularization technique for neural nets. Few random nodes in each layer is deactivated in forward pass. This allows the algorithm to train on different set of nodes in each iterations.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is dropout? Why is it useful? How does it work? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Dropout is a technique that at each training step turns off each neuron with a certain probability of *p*. This way at each iteration we train only *1-p* of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter *p*.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is backpropagation? How does it work? Why do we need it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'The Backpropagation algorithm looks for the minimum value of the error function in weight space using a technique called the delta rule or gradient descent. \\nThe weights that minimize the error function is then considered to be a solution to the learning problem. \\n\\nWe need backpropogation because,\\n* Calculate the error ‚Äì How far is your model output from the actual output.\\n* Minimum Error ‚Äì Check whether the error is minimized or not.\\n* Update the parameters ‚Äì If the error is huge then, update the parameters (weights and biases). After that again check the error.  \\nRepeat the process until the error becomes minimum.\\n* Model is ready to make a prediction ‚Äì Once the error becomes minimum, you can feed some inputs to your model and it will produce the output.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Which optimization techniques for training neural nets do you know? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* Gradient Descent\\n* Stochastic Gradient Descent\\n* Mini-Batch Gradient Descent(best among gradient descents)\\n* Nesterov Accelerated Gradient\\n* Momentum\\n* Adagrad \\n* AdaDelta\\n* Adam(best one. less time, more efficient)',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How do we use SGD (stochastic gradient descent) for training a neural net? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'SGD approximates the expectation with few randomly selected samples (instead of the full data). In comparison to batch gradient descent, we can efficiently approximate the expectation in large data sets using SGD. For neural networks this reduces the training time a lot even considering that it will converge later as the random sampling adds noise to the gradient descent.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What‚Äôs the learning rate? üë∂',\n",
       "  'reference_answer': 'The learning rate is an important hyperparameter that controls how quickly the model is adapted to the problem during the training. It can be seen as the \"step width\" during the parameter updates, i.e. how far the weights are moved into the direction of the minimum of our optimization problem.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What happens when the learning rate is too large? Too small? üë∂',\n",
       "  'reference_answer': 'A large learning rate can accelerate the training. However, it is possible that we \"shoot\" too far and miss the minimum of the function that we want to optimize, which will not result in the best solution. On the other hand, training with a small learning rate takes more time but it is possible to find a more precise minimum. The downside can be that the solution is stuck in a local minimum, and the weights won\\'t update even if it is not the best possible global solution.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How to set the learning rate? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"There is no straightforward way of finding an optimum learning rate for a model. It involves a lot of hit and trial. Usually starting with a small values such as 0.01 is a good starting point for setting a learning rate and further tweaking it so that it doesn't overshoot or converge too slowly.\",\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is Adam? What‚Äôs the main difference between Adam and SGD? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"Adam (Adaptive Moment Estimation) is a optimization technique for training neural networks. on an average, it is the best optimizer .It works with momentums of first and second order. The intuition behind the Adam is that we don‚Äôt want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.\\n\\nAdam tends to converge faster, while SGD often converges to more optimal solutions.\\nSGD's high variance disadvantages gets rectified by Adam (as advantage for Adam).\",\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'When would you use Adam and when SGD? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Adam tends to converge faster, while SGD often converges to more optimal solutions.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Do we want to have a constant learning rate or we better change it throughout training? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"Generally, it is recommended to start learning rate with relatively high value and then gradually decrease learning rate so the model does not overshoot the minima and at the same time we don't want to start with very low learning rate as the model will take too long to converge. There are many available techniques to do decay the learning rate. For example, in PyTorch you can use\\na function called **StepLR** which decays the learning rate of each parameter by value **gamma**-which we have to pass through argument- after n number of epoch which you can also set through function argument named **epoch_size**.\",\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How do we decide when to stop training a neural net? üë∂',\n",
       "  'reference_answer': 'Simply stop training when the validation error is the minimum.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is model checkpointing? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Saving the weights learned by a model mid training for long running processes is known as model checkpointing so that you can resume your training from a certain checkpoint.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Can you tell us how you approach the model training process? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How we can use neural nets for computer vision? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"Neural nets used in the area of computer vision are generally Convolutional Neural Networks(CNN's). You can learn about convolutions below. It appears that convolutions are quite powerful when it comes to working with images and videos due to their ability to extract and learn complex features. Thus CNN's are a go-to method for any problem in computer vision.\",\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What‚Äôs a convolutional layer? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'The idea of the convolutional layer is the assumption that the information needed for making a decision often is spatially close and thus, it only takes the weighted sum over nearby inputs. It also assumes that the networks‚Äô kernels can be reused for all nodes, hence the number of weights can be drastically reduced. To counteract only one feature being learnt per layer, multiple kernels are applied to the input which creates parallel channels in the output. Consecutive layers can also be stacked to allow the network to find more high-level features.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Why do we actually need convolutions? Can‚Äôt we use fully-connected layers for that? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'A fully-connected layer needs one weight per inter-layer connection, which means the number of weights which needs to be computed quickly balloons as the number of layers and nodes per layer is increased.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What‚Äôs pooling in CNN? Why do we need it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Pooling is a technique to downsample the feature map. It allows layers which receive relatively undistorted versions of the input to learn low level features such as lines, while layers deeper in the model can learn more abstract features such as texture.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How does max pooling work? Are there other pooling techniques? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Max pooling is a technique where the maximum value of a receptive field is passed on in the next feature map. The most commonly used receptive field is 2 x 2 with a stride of 2, which means the feature map is downsampled from N x N to N/2 x N/2. Receptive fields larger than 3 x 3 are rarely employed as too much information is lost. \\n\\nOther pooling techniques include:\\n\\n* Average pooling, the output is the average value of the receptive field.\\n* Min pooling, the output is the minimum value of the receptive field.\\n* Global pooling, where the receptive field is set to be equal to the input size, this means the output is equal to a scalar and can be used to reduce the dimensionality of the feature map.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Are CNNs resistant to rotations? What happens to the predictions of a CNN if an image is rotated? üöÄ',\n",
       "  'reference_answer': 'CNNs are not resistant to rotation by design. However, we can make our models resistant by augmenting our datasets with different rotations of the raw data. The predictions of a CNN will change if an image is rotated and we did not augment our dataset accordingly. A demonstration of this occurence can be seen in [this video](https://www.youtube.com/watch?v=VO1bQo4PXV4), where a CNN changes its predicted class between a duck and a rabbit based on the rotation of the image.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'What are augmentations? Why do we need them? üë∂',\n",
       "  'reference_answer': 'Augmentations are an artifical way of expanding the existing datasets by performing some transformations, color shifts or many other things on the data. It helps in diversifying the data and even increasing the data when there is scarcity of data for a model to train on.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What kind of augmentations do you know? üë∂',\n",
       "  'reference_answer': 'There are many kinds of augmentations which can be used according to the type of data you are working on some of which are geometric and numerical transformation, PCA, cropping, padding, shifting, noise injection etc.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How to choose which augmentations to use? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Augmentations really depend on the type of output classes and the features you want your model to learn. For eg. if you have mostly properly illuminated images in your dataset and want your model to predict poorly illuminated images too, you can apply channel shifting on your data and include the resultant images in your dataset for better results.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What kind of CNN architectures for classification do you know? üöÄ',\n",
       "  'reference_answer': 'Image Classification\\n* Inception v3\\n* Xception \\n* DenseNet\\n* AlexNet\\n* VGG16\\n* ResNet\\n* SqueezeNet\\n* EfficientNet\\n* MobileNet\\n\\nThe last three are designed so they use smaller number of parameters which is helpful for edge AI.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'What is transfer learning? How does it work? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Given a source domain D_S and learning task T_S, a target domain D_T and learning task T_T, transfer learning aims to help improve the learning of the target predictive function f_T in D_T using the knowledge in D_S and T_S, where D_S ‚â† D_T,or T_S ‚â† T_T. In other words, transfer learning enables to reuse knowledge coming from other domains or learning tasks.\\n\\nIn the context of CNNs, we can use networks that were pre-trained on popular datasets such as ImageNet. We then can use the weights of the layers that learn to represent features and combine them with a new set of layers that learns to map the feature representations to the given classes. Two popular strategies are either to freeze the layers that learn the feature representations completely, or to give them a smaller learning rate.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is object detection? Do you know any architectures for that? üöÄ',\n",
       "  'reference_answer': 'Object detection is finding Bounding Boxes around objects in an image. \\nArchitectures :\\nYOLO, Faster RCNN, Center Net',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'What is object segmentation? Do you know any architectures for that? üöÄ',\n",
       "  'reference_answer': 'Object Segmentation is predicting masks. It does not differentiate objects. \\nArchitectures :\\nMask RCNN, UNet',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'How can we use machine learning for text classification? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Machine learning classification algorithms predict a class based on a numerical feature representation. This means that in order to use machine learning for text classification, we need to extract numerical features from our text data first before we can apply machine learning algorithms. Common approaches to extract numerical features from text data are bag of words, N-grams or word embeddings.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is bag of words? How we can use it for text classification? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Bag of Words is a representation of text that describes the occurrence of words within a document. The order or structure of the words is not considered. For text classification, we look at the histogram of the words within the text and consider each word count as a feature.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are the advantages and disadvantages of bag of words? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Advantages:\\n1. Simple to understand and implement.\\n\\nDisadvantages:\\n1. The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\\n2. Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons\\n3. Discarding word order ignores the context, and in turn meaning of words in the document. Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (‚Äúthis is interesting‚Äù vs ‚Äúis this interesting‚Äù), synonyms (‚Äúold bike‚Äù vs ‚Äúused bike‚Äù).',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are N-grams? How can we use them? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'The function to tokenize into consecutive sequences of words is called n-grams. It can be used to find out N most co-occurring words (how often word X is followed by word Y) in a given sentence.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How large should be N for our bag of words when using N-grams? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is TF-IDF? How is it useful for text classification? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Term Frequency (TF) is a scoring of the frequency of the word in the current document. Inverse Document Frequency(IDF) is a scoring of how rare the word is across documents. It is used in scenario where highly recurring words may not contain as much informational content as the domain specific words. For example, words like ‚Äúthe‚Äù that are frequent across all documents therefore need to be less weighted. The TF-IDF score highlights words that are distinct (contain useful information) in a given document.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Which model would you use for text classification with bag of words features? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '1. Bag Of Words model\\n2. Word2Vec Embeddings\\n3. fastText Embeddings\\n4. Convolutional Neural Networks (CNN)\\n5. Long Short-Term Memory (LSTM)\\n6. Bidirectional Encoder Representations from Transformers (BERT)',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Usually logistic regression is better because bag of words creates a matrix with large number of columns. For a huge number of columns logistic regression is usually faster than gradient boosting trees.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are word embeddings? Why are they useful? Do you know Word2Vec? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Word Embeddings are vector representations for words. Each word is mapped to one vector, this vector tries to capture some characteristics of the word, allowing similar words to have similar vector representations.  \\nWord Embeddings helps in capturing the inter-word semantics and represents it in real-valued vectors.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Do you know any other ways to get word embeddings? üöÄ',\n",
       "  'reference_answer': '- GloVe\\n- BERT',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Approaches ranked from simple to more complex:\\n\\n1. Take an average over all words\\n2. Take a weighted average over all words. Weighting can be done by inverse document frequency (idf part of tf-idf).\\n3. Use ML model like LSTM or Transformer.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Would you prefer gradient boosting trees model or logistic regression when doing text classification with embeddings? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Gradient boosting trees (GBTs) are generally a better choice than logistic regression for text classification with embeddings. This is because GBTs are able to learn more complex relationships between the features in the data, including the features extracted from the embeddings.\\n\\nLogistic regression is a linear model, which means that it can only learn linear relationships between the features. This can be a limitation for text classification, where the relationships between the features are often complex and non-linear.\\n\\nGBTs, on the other hand, are able to learn non-linear relationships between the features by combining multiple decision trees. This allows GBTs to learn more complex patterns in the data, which can lead to better performance on text classification tasks.\\n\\nIn addition, GBTs are more robust to outliers and noise in the data than logistic regression. This can be important for text classification tasks, where the data can be noisy and imbalanced.\\n\\nOverall, GBTs are a better choice than logistic regression for text classification with embeddings, especially when the data is noisy or imbalanced. However, it is important to consider the computational cost and interpretability of GBTs before using them.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How can you use neural nets for text classification? üöÄ',\n",
       "  'reference_answer': 'Here is a general overview of how to use neural nets for text classification:\\n\\nPreprocess the text: This includes cleaning the text by removing stop words, punctuation, and other irrelevant symbols. It may also involve converting the text to lowercase and stemming or lemmatizing the words.\\nRepresent the text as a vector: This can be done using a variety of methods, such as one-hot encoding or word embeddings.\\nBuild the neural net: The neural net architecture will depend on the specific text classification task. However, a typical architecture will include an embedding layer, one or more hidden layers, and an output layer.\\nTrain the neural net: The neural net is trained by feeding it labeled examples of text data. The neural net will learn to adjust its parameters in order to minimize the loss function, which is typically the cross-entropy loss function.\\nEvaluate the neural net: Once the neural net is trained, it can be evaluated on a held-out test set to assess its performance.\\nHere are some specific examples of how neural nets can be used for text classification:\\nSentiment analysis, Spam detection, Topic classification, Language identification\\n\\nNeural nets have achieved state-of-the-art results on many text classification tasks. However, they can be computationally expensive to train and deploy.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'How can we use CNN for text classification? üöÄ',\n",
       "  'reference_answer': 'Here are some specific examples of how CNNs can be used for text classification:\\n\\nSentiment analysis: CNNs can be used to classify text as positive, negative, or neutral sentiment. This is a common task in social media analysis and customer service.\\nSpam detection: CNNs can be used to classify emails as spam or not spam. This is a common task in email filtering systems.\\nTopic classification: CNNs can be used to classify text documents into different topics. This is a common task in news and social media analysis.\\nLanguage identification: CNNs can be used to identify the language of a text document. This is a common task in translation systems.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'What is unsupervised learning? üë∂',\n",
       "  'reference_answer': 'Unsupervised learning aims to detect patterns in data where no labels are given.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What is clustering? When do we need it? üë∂',\n",
       "  'reference_answer': 'Clustering algorithms group objects such that similar feature points are put into the same groups (clusters) and dissimilar feature points are put into different clusters.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Do you know how K-means works? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '1. Partition points into k subsets.\\n2. Compute the seed points as the new centroids of the clusters of the current partitioning.\\n3. Assign each point to the cluster with the nearest seed point.\\n4. Go back to step 2 or stop when the assignment does not change.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How to select K for K-means? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* Domain knowledge, i.e. an expert knows the value of k\\n* Elbow method: compute the clusters for different values of k, for each k, calculate the total within-cluster sum of square, plot the sum according to the number of clusters and use the band as the number of clusters.\\n* Average silhouette method: compute the clusters for different values of k, for each k, calculate the average silhouette of observations, plot the silhouette according to the number of clusters and select the maximum as the number of clusters.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are the other clustering algorithms do you know? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise.\\n* Agglomerative Hierarchical Clustering (AHC): hierarchical clusters combining the nearest clusters starting with each point as its own cluster.\\n* DIvisive ANAlysis Clustering (DIANA): hierarchical clustering starting with one cluster containing all points and splitting the clusters until each point describes its own cluster.\\n* Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Cluster defined as maximum set of density-connected points.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Do you know how DBScan works? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood)\\n* Cluster defined as maximum set of density-connected points.\\n* Points p_j and p_i are density-connected w.r.t. epsilon and minPts if there is a point o such that both, i and j are density-reachable from o w.r.t. epsilon and minPts.\\n* p_j is density-reachable from p_i w.r.t. epsilon, minPts if there is a chain of points p_i -> p_i+1 -> p_i+x = p_j such that p_i+x is directly density-reachable from p_i+x-1.\\n* p_j is a directly density-reachable point of the neighborhood of p_i if dist(p_i,p_j) <= epsilon.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'When would you choose K-means and when DBScan? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* DBScan is more robust to noise.\\n* DBScan is better when the amount of clusters is difficult to guess.\\n* K-means has a lower complexity, i.e. it will be much faster, especially with a larger amount of points.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is the curse of dimensionality? Why do we care about it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Data in only one dimension is relatively tightly packed. Adding a dimension stretches the points across that dimension, pushing them further apart. Additional dimensions spread the data even further making high dimensional data extremely sparse. We care about it, because it is difficult to use machine learning in sparse spaces.',\n",
       "  'topic': 'dimensionality_reduction',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Do you know any dimensionality reduction techniques? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* Singular Value Decomposition (SVD)\\n* Principal Component Analysis (PCA)\\n* Linear Discriminant Analysis (LDA)\\n* T-distributed Stochastic Neighbor Embedding (t-SNE)\\n* Autoencoders\\n* Fourier and Wavelet Transforms',\n",
       "  'topic': 'dimensionality_reduction',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What‚Äôs singular value decomposition? How is it typically used for machine learning? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Œ£ (diagonal matrix) and R^T (right singular values).\\n* For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive.\\n* Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction.',\n",
       "  'topic': 'dimensionality_reduction',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is the ranking problem? Which models can you use to solve them? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are good unsupervised baselines for text information retrieval? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How would you evaluate your ranking algorithms? Which offline metrics would you use? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is precision and recall at k? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Precision at k and recall at k are evaluation metrics for ranking algorithms. Precision at k shows the share of relevant items in the first *k* results of the ranking algorithm. And Recall at k indicates the share of relevant items returned in top *k* results out of all correct answers for a given query.\\n\\nExample:\\nFor a search query \"Car\" there are 3 relevant products in your shop. Your search algorithm returns 2 of those relevant products in the first 5 search results.\\nPrecision at 5 = # num of relevant products in search result / k = 2/5 = 40%\\nRecall at 5 = # num of relevant products in search result / # num of all relevant products = 2/3 = 66.6%',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is mean average precision at k? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How can we use machine learning for search? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How can we get training data for our ranking algorithms? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Can we formulate the search problem as a classification problem? How? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How can we use clicks data as the training data for ranking algorithms? üöÄ',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'Do you know how to use gradient boosting trees for ranking? üöÄ',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'medium'},\n",
       " {'text': 'How do you do an online evaluation of a new ranking algorithm? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is a recommender system? üë∂',\n",
       "  'reference_answer': 'Recommender systems are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user.',\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'What are good baselines when building a recommender system? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* A good recommer system should give relevant and personalized information.\\n* It should not recommend items the user knows well or finds easily.\\n* It should make diverse suggestions.\\n* A user should explore new items.',\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is collaborative filtering? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"* Collaborative filtering is the most prominent approach to generate recommendations.\\n* It uses the wisdom of the crowd, i.e. it gives recommendations based on the experience of others.\\n* A recommendation is calculated as the average of other experiences.\\n* Say we want to give a score that indicates how much user u will like an item i. Then we can calculate it with the experience of N other users U as r_ui = 1/N * sum(v in U) r_vi.\\n* In order to rate similar experiences with a higher weight, we can introduce a similarity between users that we use as a multiplier for each rating.\\n* Also, as users have an individual profile, one user may have an average rating much larger than another user, so we use normalization techniques (e.g. centering or Z-score normalization) to remove the users' biases.\\n* Collaborative filtering does only need a rating matrix as input and improves over time. However, it does not work well on sparse data, does not work for cold starts (see below) and usually tends to overfit.\",\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'How we can incorporate implicit feedback (clicks, etc) into our recommender systems? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"In comparison to explicit feedback, implicit feedback datasets lack negative examples. For example, explicit feedback can be a positive or a negative rating, but implicit feedback may be the number of purchases or clicks. One popular approach to solve this problem is named weighted alternating least squares (wALS) [Hu, Y., Koren, Y., & Volinsky, C. (2008, December). Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on (pp. 263-272). IEEE.]. Instead of modeling the rating matrix directly, the numbers (e.g. amount of clicks) describe the strength in observations of user actions. The model tries to find latent factors that can be used to predict the expected preference of a user for an item.\",\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is the cold start problem? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Collaborative filterung incorporates crowd knowledge to give recommendations for certain items. Say we want to recommend how much a user will like an item, we then will calculate the score using the recommendations of other users for this certain item. We can distinguish between two different ways of a cold start problem now. First, if there is a new item that has not been rated yet, we cannot give any recommendation. Also, when there is a new user, we cannot calculate a similarity to any other user.',\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'Possible approaches to solving the cold start problem? \\u200d‚≠êÔ∏èüöÄ',\n",
       "  'reference_answer': '* Content-based filtering incorporates features about items to calculate a similarity between them. In this way, we can recommend items that have a high similarity to items that a user liked already. In this way, we are not dependent on the ratings of other users for a given item anymore and solve the cold start problem for new items.\\n* Demographic filtering incorporates user profiles to calculate a similarity between them and solves the cold start problem for new users.',\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What is a time series? üë∂',\n",
       "  'reference_answer': 'A time series is a set of observations ordered in time usually collected at regular intervals.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'How is time series different from the usual regression problem? üë∂',\n",
       "  'reference_answer': 'The principle behind causal forecasting is that the value that has to be predicted is dependant on the input features (causal factors). In time series forecasting, the to be predicted value is expected to follow a certain pattern over time.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'easy'},\n",
       " {'text': 'Which models do you know for solving time series problems? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': '* Simple Exponential Smoothing: approximate the time series with an exponential function\\n* Trend-Corrected Exponential Smoothing (Holt‚Äòs Method): exponential smoothing that also models the trend\\n* Trend- and Seasonality-Corrected Exponential Smoothing (Holt-Winter‚Äòs Method): exponential smoothing that also models trend and seasonality\\n* Time Series Decomposition: decomposed a time series into the four components trend, seasonal variation, cycling variation and irregular component\\n* Autoregressive models: similar to multiple linear regression, except that the dependent variable y_t depends on its own previous values rather than other independent variables.\\n* Deep learning approaches (RNN, LSTM, etc.)',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'If there‚Äôs a trend in our series, how we can remove it? And why would we want to do it? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': \"We can explicitly model the trend (and/or seasonality) with approaches such as Holt's Method or Holt-Winter's Method. We want to explicitly model the trend to reach the stationarity property for the data. Many time series approaches require stationarity. Without stationarity,the interpretation of the results of these analyses is problematic [Manuca, Radu & Savit, Robert. (1996). Stationarity and nonstationarity in time series analysis. Physica D: Nonlinear Phenomena. 99. 134-161. 10.1016/S0167-2789(96)00139-X. ].\",\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'You have a series with only one variable ‚Äúy‚Äù measured at time t. How do predict ‚Äúy‚Äù at time t+1? Which approaches would you use? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'We want to look at the correlation between different observations of y. This measure of correlation is called autocorrelation. Autoregressive models are multiple regression models where the time-lag series of the original time series are treated like multiple independent variables.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'You have a series with a variable ‚Äúy‚Äù and a set of features. How do you predict ‚Äúy‚Äù at t+1? Which approaches would you use? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Given the assumption that the set of features gives a meaningful causation to y, a causal forecasting approach such as linear regression or multiple nonlinear regression might be useful. In case there is a lot of data and the explainability of the results is not a high priority, we can also consider deep learning approaches.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard'},\n",
       " {'text': 'What are the problems with using trees for solving time series problems? \\u200d‚≠êÔ∏è',\n",
       "  'reference_answer': 'Random Forest models are not able to extrapolate time series data and understand increasing/decreasing trends. It will provide us with average data points if the validation data has values greater than the training data points.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a87b7789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e243de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_question(question: str):\n",
    "    difficulty = \"medium\"\n",
    "    if \"üë∂\" in question:\n",
    "        difficulty = \"easy\"\n",
    "    elif \"‚≠ê\" in question:\n",
    "        difficulty = \"hard\"\n",
    "\n",
    "    clean_question = re.sub(r\"[üë∂‚≠ê]\", \"\", question).strip()\n",
    "    return clean_question, difficulty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "094ad68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_question_type(question: str) -> str:\n",
    "    q = question.lower()\n",
    "\n",
    "    if any(k in q for k in [\"implement\", \"code\", \"write\", \"python\"]):\n",
    "        return \"coding\"\n",
    "    if any(k in q for k in [\"derive\", \"prove\", \"formula\", \"equation\"]):\n",
    "        return \"math\"\n",
    "    if any(k in q for k in [\"how would you\", \"what would you do\", \"design\"]):\n",
    "        return \"scenario\"\n",
    "\n",
    "    return \"conceptual\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "421a68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {\n",
    "    \"what\", \"is\", \"are\", \"the\", \"of\", \"to\", \"and\", \"when\", \"do\", \"we\",\n",
    "    \"you\", \"which\", \"can\", \"use\", \"main\"\n",
    "}\n",
    "\n",
    "def generate_tags(question: str, topic: str):\n",
    "    words = re.findall(r\"[a-zA-Z_]+\", question.lower())\n",
    "    keywords = [\n",
    "        w for w in words\n",
    "        if w not in STOPWORDS and len(w) > 3\n",
    "    ]\n",
    "\n",
    "    return list(set([topic] + keywords[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7c70d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_concepts(answer: str):\n",
    "    bold_terms = re.findall(r\"\\*\\*(.*?)\\*\\*\", answer)\n",
    "    italic_terms = re.findall(r\"\\*(.*?)\\*\", answer)\n",
    "\n",
    "    candidates = set(bold_terms + italic_terms)\n",
    "\n",
    "    # Light cleanup\n",
    "    cleaned = [\n",
    "        c.strip().lower().replace(\" \", \"_\")\n",
    "        for c in candidates\n",
    "        if len(c) > 2\n",
    "    ]\n",
    "\n",
    "    return list(set(cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b63945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SECTION_PATTERN = r'(?m)^##\\s(.*)\\n([\\s\\S]*?)(?=^##|\\Z)'\n",
    "QA_PATTERN = r'\\*\\*(.*?)\\*\\*\\n\\n([\\s\\S]*?)(?=\\n<br/>\\n|\\Z)'\n",
    "\n",
    "results = []\n",
    "\n",
    "for section_title, section_body in re.findall(SECTION_PATTERN, content):\n",
    "    topic = section_title.strip().lower().replace(\" \", \"_\")\n",
    "\n",
    "    for raw_question, answer in re.findall(QA_PATTERN, section_body):\n",
    "        question, difficulty = normalize_question(raw_question)\n",
    "\n",
    "        question_type = infer_question_type(question)\n",
    "        tags = generate_tags(question, topic)\n",
    "        key_concepts = extract_key_concepts(answer)\n",
    "\n",
    "        results.append({\n",
    "            \"text\": question,\n",
    "            \"reference_answer\": answer.strip(),\n",
    "            \"topic\": topic,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"question_type\": question_type,\n",
    "            \"tags\": tags,\n",
    "            \"key_concepts\": key_concepts\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "688560af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'What is supervised machine learning?',\n",
       "  'reference_answer': 'Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the\\xa0input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.\\n\\nk-Nearest Neighbors(KNN):Looking at the k closest labeled data points',\n",
       "  'topic': 'supervised_machine\\xa0learning',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['supervised_machine\\xa0learning',\n",
       "   'supervised',\n",
       "   'machine',\n",
       "   'learning'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is regression? Which models can you use to solve a regression problem?',\n",
       "  'reference_answer': 'Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor).\\nHere are some common regression models\\n\\n- *Linear Regression* establishes a linear relationship between target and predictor (s). It predicts a numeric value and has a shape of a straight line.\\n- *Polynomial Regression* has a regression equation with the power of independent variable more than 1. It is a curve that fits into the data points.\\n- *Ridge Regression* helps when predictors are highly correlated (multicollinearity problem). It penalizes the squares of regression coefficients but doesn‚Äôt allow the coefficients to reach zeros (uses L2 regularization).\\n- *Lasso Regression* penalizes the absolute values of regression coefficients and allows some of the coefficients to reach absolute zero (thereby allowing feature selection).',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['models', 'solve', 'problem', 'linear_regression', 'regression'],\n",
       "  'key_concepts': ['linear_regression',\n",
       "   'lasso_regression',\n",
       "   'ridge_regression',\n",
       "   'polynomial_regression']},\n",
       " {'text': 'What is linear regression? When do we use it?',\n",
       "  'reference_answer': 'Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).\\n\\nWith a simple equation:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```\\n\\nB is regression coefficients, x values are the independent (explanatory) variables  and y is dependent variable.\\n\\nThe case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\\n\\nSimple linear regression:\\n\\n```\\ny = B0 + B1*x1\\n```\\n\\nMultiple linear regression:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['linear_regression', 'linear', 'regression'],\n",
       "  'key_concepts': ['x1_+_..._+_bn']},\n",
       " {'text': 'What are the main assumptions of linear regression?',\n",
       "  'reference_answer': 'There are several assumptions of linear regression. If any of them is violated, model predictions and interpretation may be worthless or misleading.\\n\\n1. **Linear relationship** between features and target variable.\\n2. **Additivity** means that the effect of changes in one of the features on the target variable does not depend on values of other features. For example, a model for predicting revenue of a company have of two features - the number of items _a_ sold and the number of items _b_ sold. When company sells more items _a_ the revenue increases and this is independent of the number of items _b_ sold. But, if customers who buy _a_ stop buying _b_, the additivity assumption is violated.\\n3. Features are not correlated (no **collinearity**) since it can be difficult to separate out the individual effects of collinear features on the target variable.\\n4. Errors are independently and identically normally distributed (y<sub>i</sub> = B0 + B1*x1<sub>i</sub> + ... + error<sub>i</sub>):\\n   1. No correlation between errors (consecutive errors in the case of time series data).\\n   2. Constant variance of errors - **homoscedasticity**. For example, in case of time series, seasonal patterns can increase errors in seasons with higher activity.\\n   3. Errors are normally distributed, otherwise some features will have more influence on the target variable than to others. If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['linear_regression', 'assumptions', 'linear', 'regression'],\n",
       "  'key_concepts': ['homoscedasticity',\n",
       "   'collinearity',\n",
       "   'additivity',\n",
       "   'linear_relationship']},\n",
       " {'text': 'What‚Äôs the normal distribution? Why do we care about it?',\n",
       "  'reference_answer': 'The normal distribution is a continuous probability distribution whose probability density function takes the following formula:\\n\\n![formula](https://mathworld.wolfram.com/images/equations/NormalDistribution/NumberedEquation1.gif)\\n\\nwhere Œº is the mean and œÉ is the standard deviation of the distribution.\\n\\nThe normal distribution derives its importance from the **Central Limit Theorem**, which states that if we draw a large enough number of samples, their mean will follow a normal distribution regardless of the initial distribution of the sample, i.e **the distribution of the mean of the samples is normal**. It is important that each sample is independent from the other.\\n\\nThis is powerful because it helps us study processes whose population distribution is unknown to us.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['distribution', 'about', 'linear_regression', 'care', 'normal'],\n",
       "  'key_concepts': ['the_distribution_of_the_mean_of_the_samples_is_normal',\n",
       "   'central_limit_theorem']},\n",
       " {'text': 'How do we check if a variable follows the normal distribution? \\u200dÔ∏è',\n",
       "  'reference_answer': '1. Plot a histogram out of the sampled data. If you can fit the bell-shaped \"normal\" curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected.\\n2. Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution.\\n3. Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously.\\n4. Check for Quantile-Quantile plot. It is a scatterplot created by plotting two sets of quantiles against one another. Normal Q-Q plot place the data points in a roughly straight line.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['follows',\n",
       "   'distribution',\n",
       "   'variable',\n",
       "   'linear_regression',\n",
       "   'check',\n",
       "   'normal'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices? \\u200dÔ∏è',\n",
       "  'reference_answer': \"Data is not normal. Specially, real-world datasets or uncleaned datasets always have certain skewness. Same goes for the price prediction. Price of houses or any other thing under consideration depends on a number of factors. So, there's a great chance of presence of some skewed values i.e outliers if we talk in data science terms. \\n\\nYes, you may need to do pre-processing. Most probably, you will need to remove the outliers to make your distribution near-to-normal.\",\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['want',\n",
       "   'model',\n",
       "   'linear_regression',\n",
       "   'prices',\n",
       "   'predicting',\n",
       "   'build'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What methods for solving linear regression do you know? \\u200dÔ∏è',\n",
       "  'reference_answer': \"To solve linear regression, you need to find the coefficients $`\\\\beta`$ which minimize the sum of squared errors.\\n\\nMatrix Algebra method: Let's say you have `X`, a matrix of features, and `y`, a vector with the values you want to predict. After going through the matrix algebra and minimization problem, you get this solution: $`\\\\beta = (X^{T}X)^{-1}X^{T}y`$. \\n\\nBut solving this requires you to find an inverse, which can be time-consuming, if not impossible. Luckily, there are methods like Singular Value Decomposition (SVD) or QR Decomposition that can reliably calculate this part $`(X^{T}X)^{-1}X^{T}`$ (called the pseudo-inverse) without actually needing to find an inverse. The popular python ML library `sklearn` uses SVD to solve least squares.\\n\\nAlternative method: Gradient Descent. See explanation below.\",\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['methods',\n",
       "   'linear_regression',\n",
       "   'solving',\n",
       "   'know',\n",
       "   'linear',\n",
       "   'regression'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is gradient descent? How does it work? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Gradient descent is an algorithm that uses calculus concept of gradient to try and reach local or global minima. It works by taking the negative of the gradient in a point of a given function, and updating that point repeatedly using the calculated negative gradient, until the algorithm reaches a local or global minimum, which will cause future iterations of the algorithm to return values that are equal or too close to the current point. It is widely used in machine learning applications.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['does', 'descent', 'linear_regression', 'gradient', 'work'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is the normal equation? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Normal equations are equations obtained by setting equal to zero the partial derivatives of the sum of squared errors (least squares); normal equations allow one to estimate the parameters of a multiple linear regression.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'math',\n",
       "  'tags': ['linear_regression', 'equation', 'normal'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is SGD \\u200a‚Äî\\u200a stochastic gradient descent? What‚Äôs the difference with the usual gradient descent? \\u200dÔ∏è',\n",
       "  'reference_answer': 'In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function.\\n\\nThe difference lies in how the gradient of the loss function is estimated. In the usual GD, you have to run through ALL the samples in your training set in order to estimate the gradient and do a single update for a parameter in a particular iteration. In SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to estimate the gradient and do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['difference',\n",
       "   'descent',\n",
       "   'linear_regression',\n",
       "   'gradient',\n",
       "   'with',\n",
       "   'stochastic'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Which metrics for evaluating regression models do you know?',\n",
       "  'reference_answer': '1. Mean Squared Error(MSE)\\n2. Root Mean Squared Error(RMSE)\\n3. Mean Absolute Error(MAE)\\n4. R¬≤ or Coefficient of Determination\\n5. Adjusted R¬≤',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['models',\n",
       "   'linear_regression',\n",
       "   'metrics',\n",
       "   'know',\n",
       "   'evaluating',\n",
       "   'regression'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are MSE and RMSE?',\n",
       "  'reference_answer': 'MSE stands for <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror while RMSE stands for <strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror. They are metrics with which we can evaluate models.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['linear_regression', 'rmse'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is the bias-variance trade-off?',\n",
       "  'reference_answer': '**Bias** is the error introduced by approximating the true underlying function, which can be quite complex, by a simpler model. **Variance** is a model sensitivity to changes in the training dataset.\\n\\n**Bias-variance trade-off** is a relationship between the expected test error and the variance and the bias - both contribute to the level of the test error and ideally should be as small as possible:\\n\\n```\\nExpectedTestError = Variance + Bias¬≤ + IrreducibleError\\n```\\n\\nBut as a model complexity increases, the bias decreases and the variance increases which leads to *overfitting*. And vice versa, model simplification helps to decrease the variance but it increases the bias which leads to *underfitting*.',\n",
       "  'topic': 'linear_regression',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['variance', 'linear_regression', 'trade', 'bias'],\n",
       "  'key_concepts': ['bias-variance_trade-off',\n",
       "   'variance',\n",
       "   'bias',\n",
       "   'underfitting',\n",
       "   'overfitting']},\n",
       " {'text': 'What is overfitting?',\n",
       "  'reference_answer': \"When your model perform very well on your training set but can't generalize the test set, because it adjusted a lot to the training set.\",\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['validation', 'overfitting'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How to validate your models?',\n",
       "  'reference_answer': 'One of the most common approaches is splitting data into train, validation and test parts.\\nModels are trained on train data, hyperparameters (for example early stopping) are selected based on the validation data, the final measurement is done on test dataset.\\nAnother approach is cross-validation: split dataset into K folds and each time train models on training folds and measure the performance on the validation folds.\\nAlso you could combine these approaches: make a test/holdout dataset and do cross-validation on the rest of the data. The final quality is measured on test dataset.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['your', 'validation', 'models', 'validate'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Why do we need to split our data into three parts: train, validation, and test?',\n",
       "  'reference_answer': 'The training set is used to fit the model, i.e. to train the model with the data. The validation set is then used to provide an unbiased evaluation of a model while fine-tuning hyperparameters. This improves the generalization of the model. Finally, a test data set which the model has never \"seen\" before should be used for the final evaluation of the model. This allows for an unbiased evaluation of the model. The evaluation should never be performed on the same data that is used for training. Otherwise the model performance would not be representative.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['into', 'data', 'need', 'split', 'validation', 'three'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Can you explain how cross-validation works?',\n",
       "  'reference_answer': 'Cross-validation is the process to separate your total training set into two subsets: training and validation set, and evaluate your model to choose the hyperparameters. But you do this process iteratively, selecting different training and validation set, in order to reduce the bias that you would have by selecting only one validation set.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['validation', 'works', 'cross', 'explain'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is K-fold cross-validation?',\n",
       "  'reference_answer': 'K fold cross validation is a method of cross validation where we select a hyperparameter k. The dataset is now divided into k parts. Now, we take the 1st part as validation set and remaining k-1 as training set. Then we take the 2nd part as validation set and remaining k-1 parts as training set. Like this, each part is used as validation set once and the remaining k-1 parts are taken together and used as training set.\\nIt should not be used in a time series data.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['fold', 'validation', 'cross'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do we choose K in K-fold cross-validation? What‚Äôs your favorite K?',\n",
       "  'reference_answer': 'There are two things to consider while deciding K: the number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained. <br/>\\nI tend to use 4 for small datasets and 5 for large ones as K.',\n",
       "  'topic': 'validation',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['your', 'choose', 'cross', 'fold', 'validation'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is classification? Which models would you use to solve a classification problem?',\n",
       "  'reference_answer': 'Classification problems are problems in which our prediction space is discrete, i.e. there is a finite number of values the output variable can be. Some models which can be used to solve classification problems are: logistic regression, decision tree, random forests, multi-layer perceptron, one-vs-all, amongst others.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['solve', 'models', 'would', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is logistic regression? When do we need to use it?',\n",
       "  'reference_answer': 'Logistic regression is a Machine Learning algorithm that is used for binary classification. You should use logistic regression when your Y variable takes only two values, e.g. True and False, \"spam\" and \"not spam\", \"churn\" and \"not churn\" and so on. The variable is said to be a \"binary\" or \"dichotomous\".',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['need', 'logistic', 'regression', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Is logistic regression a linear model? Why?',\n",
       "  'reference_answer': 'Yes, Logistic Regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['logistic', 'model', 'classification', 'linear', 'regression'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is sigmoid? What does it do?',\n",
       "  'reference_answer': 'A sigmoid function is a type of activation function, and more specifically defined as a squashing function. Squashing functions limit the output to a range between 0 and 1, making these functions useful in the prediction of probabilities.\\n\\nSigmod(x) = 1/(1+e^{-x})',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['does', 'sigmoid', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do we evaluate classification models?',\n",
       "  'reference_answer': 'Depending on the classification problem, we can use the following evaluation metrics:\\n\\n1. Accuracy\\n2. Precision\\n3. Recall\\n4. F1 Score\\n5. Logistic loss (also known as Cross-entropy loss)\\n6. Jaccard similarity coefficient score',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['models', 'evaluate', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is accuracy?',\n",
       "  'reference_answer': 'Accuracy is a metric for evaluating classification models. It is calculated by dividing the number of correct predictions by the number of total predictions.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['accuracy', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Is accuracy always a good metric?',\n",
       "  'reference_answer': 'Accuracy is not a good performance metric when there is imbalance in the dataset. For example, in binary classification with 95% of A class and 5% of B class, a constant prediction of A class would have an accuracy of 95%. In case of imbalance dataset, we need to choose Precision, recall, or F1 Score depending on the problem we are trying to solve.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['accuracy', 'metric', 'classification', 'always', 'good'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is the confusion table? What are the cells in this table?',\n",
       "  'reference_answer': 'Confusion table (or confusion matrix) shows how many True positives (TP), True Negative (TN), False Positive (FP) and False Negative (FN) model has made.\\n\\n||                |     Actual   |        Actual |\\n|:---:|   :---:        |     :---:    |:---:          |\\n||                | Positive (1) | Negative (0)  |\\n|Predicted|   Positive (1) | TP           | FP            |\\n|Predicted|   Negative (0) | FN           | TN            |\\n\\n* True Positives (TP): When the actual class of the observation is 1 (True) and the prediction is 1 (True)\\n* True Negative (TN): When the actual class of the observation is 0 (False) and the prediction is 0 (False)\\n* False Positive (FP): When the actual class of the observation is 0 (False) and the prediction is 1 (True)\\n* False Negative (FN): When the actual class of the observation is 1 (True) and the prediction is 0 (False)\\n\\nMost of the performance metrics for classification models are based on the values of the confusion matrix.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['classification', 'cells', 'table', 'confusion', 'this'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are precision, recall, and F1-score?',\n",
       "  'reference_answer': '* Precision and recall are classification evaluation metrics:\\n* P = TP / (TP + FP) and R = TP / (TP + FN).\\n* Where TP is true positives, FP is false positives and FN is false negatives\\n* In both cases the score of 1 is the best: we get no false positives or false negatives and only true positives.\\n* F1 is a combination of both precision and recall in one score (harmonic mean):\\n* F1 = 2 * PR / (P + R).\\n* Max F score is 1 and min is 0, with 1 being the best.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['precision', 'score', 'recall', 'classification'],\n",
       "  'key_concepts': ['f1_=_2']},\n",
       " {'text': 'Precision-recall trade-off \\u200dÔ∏è',\n",
       "  'reference_answer': 'Tradeoff means increasing one parameter would lead to decreasing of other. Precision-recall tradeoff occur due to increasing one of the parameter(precision or recall) while keeping the model same. \\n\\nIn an ideal scenario where there is a perfectly separable data, both precision and recall can get maximum value of 1.0. But in most of the practical situations, there is noise in the dataset and the dataset is not perfectly separable. There might be some points of positive class closer to the negative class and vice versa. In such cases, shifting the decision boundary can either increase the precision or recall but not both. Increasing one parameter leads to decreasing of the other.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['precision', 'trade', 'recall', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is the ROC curve? When to use it? \\u200dÔ∏è',\n",
       "  'reference_answer': 'ROC stands for *Receiver Operating Characteristics*. The diagrammatic representation that shows the contrast between true positive rate vs false positive rate. It is used when we need to predict the probability of the binary outcome.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['curve', 'classification'],\n",
       "  'key_concepts': ['receiver_operating_characteristics']},\n",
       " {'text': 'What is AUC (AU ROC)? When to use it? \\u200dÔ∏è',\n",
       "  'reference_answer': \"AUC stands for *Area Under the ROC Curve*. ROC is a probability curve and AUC represents degree or measure of separability. It's used when we need to value how much model is capable of distinguishing between classes.  The value is between 0 and 1, the higher the better.\",\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['classification'],\n",
       "  'key_concepts': ['area_under_the_roc_curve']},\n",
       " {'text': 'How to interpret the AU ROC score? \\u200dÔ∏è',\n",
       "  'reference_answer': 'AUC score is the value of *Area Under the ROC Curve*. \\n\\nIf we assume ROC curve consists of dots, $`(x_1, y_1), (x_2, y_2), \\\\cdots, (x_m,y_m)`$, then\\n\\n$`AUC = \\\\frac{1}{2} \\\\sum_{i=1}^{m-1}(x_{i+1}-x_i)\\\\cdot (y_i+y_{i+1})`$\\n\\nAn excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. When AUC score is 0.5, it means model has no class separation capacity whatsoever.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['score', 'interpret', 'classification'],\n",
       "  'key_concepts': ['area_under_the_roc_curve']},\n",
       " {'text': 'What is the PR (precision-recall) curve? \\u200dÔ∏è',\n",
       "  'reference_answer': 'A *precision*-*recall curve* (or PR Curve) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. Precision-recall curves (PR curves) are recommended for highly skewed domains where ROC curves may provide an excessively optimistic view of the performance.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['precision', 'curve', 'recall', 'classification'],\n",
       "  'key_concepts': ['precision', 'recall_curve']},\n",
       " {'text': 'What is the area under the PR curve? Is it a useful metric? \\u200dÔ∏èI',\n",
       "  'reference_answer': 'The Precision-Recall AUC is just like the ROC AUC, in that it summarizes the curve with a range of threshold values as a single score.\\n\\nA high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['metric', 'useful', 'classification', 'curve', 'area', 'under'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'In which cases AU PR is better than AU ROC? \\u200dÔ∏è',\n",
       "  'reference_answer': 'What is different however is that AU ROC looks at a true positive rate TPR and false positive rate FPR while AU PR looks at positive predictive value PPV and true positive rate TPR.\\n\\nTypically, if true negatives are not meaningful to the problem or you care more about the positive class, AU PR is typically going to be more useful; otherwise, If you care equally about the positive and negative class or your dataset is quite balanced, then going with AU ROC is a good idea.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['better', 'than', 'cases', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What do we do with categorical variables? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Categorical variables must be encoded before they can be used as features to train a machine learning model. There are various encoding techniques, including:\\n- One-hot encoding\\n- Label encoding\\n- Ordinal encoding\\n- Target encoding',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['variables', 'with', 'categorical', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Why do we need one-hot encoding? \\u200dÔ∏è',\n",
       "  'reference_answer': 'If we simply encode categorical variables with a Label encoder, they become ordinal which can lead to undesirable consequences. In this case, linear models will treat category with id 4 as twice better than a category with id 2. One-hot encoding allows us to represent a categorical variable in a numerical vector space which ensures that vectors of each category have equal distances between each other. This approach is not suited for all situations, because by using it with categorical variables of high cardinality (e.g. customer id) we will encounter problems that come into play because of the curse of dimensionality.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['encoding', 'need', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is \"curse of dimensionality\"? \\u200dÔ∏è',\n",
       "  'reference_answer': 'The curse of dimensionality is an issue that arises when working with high-dimensional data. It is often said that \"the curse of dimensionality\" is one of the main problems with machine learning. The curse of dimensionality refers to the fact that, as the number of dimensions (features) in a data set increases, the number of data points required to accurately learn the relationships between those features increases exponentially.\\n\\nA simple example where we have a data set with two features, x1 and x2. If we want to learn the relationship between these two features, we need to have enough data points so that we can accurately estimate the parameters of that relationship. However, if we add a third feature, x3, then the number of data points required to accurately learn the relationships between all three features increases exponentially. This is because there are now more parameters to estimate, and the number of data points needed to accurately estimate those parameters increases exponentially with the number of parameters.\\n\\nSimply put, the curse of dimensionality basically means that the error increases with the increase in the number of features.',\n",
       "  'topic': 'classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['curse', 'dimensionality', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What happens to our linear regression model if we have three columns in our data: x, y, z \\u200a‚Äî\\u200a and z is a sum of x and y? \\u200dÔ∏è',\n",
       "  'reference_answer': 'We would not be able to perform the regression. Because z is linearly dependent on x and y so when performing the regression $`{X}^{T}{X}`$ would be a singular (not invertible) matrix.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['happens',\n",
       "   'regularization',\n",
       "   'model',\n",
       "   'have',\n",
       "   'linear',\n",
       "   'regression'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What happens to our linear regression model if the column z in the data is a sum of columns x and y and some random noise? \\u200dÔ∏è',\n",
       "  'reference_answer': 'It creates a situation known as multicollinearity. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, leading to issues in the estimation of the regression coefficients. The issue arises because the variables X and Y will be highly correlated, making it difficult for the model to distinguish the individual effects of X and Y on the dependent variable Z. To address it, feature selection, PCA or regularization techniques (L2) may be used.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['happens',\n",
       "   'regularization',\n",
       "   'column',\n",
       "   'model',\n",
       "   'linear',\n",
       "   'regression'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is regularization? Why do we need it?',\n",
       "  'reference_answer': 'Regularization is used to reduce overfitting in machine learning models. It helps the models to generalize well and make them robust to outliers and noise in the data.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['regularization', 'need'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Which regularization techniques do you know? \\u200dÔ∏è',\n",
       "  'reference_answer': 'There are mainly two types of regularization,\\n1. L1 Regularization (Lasso regularization) - Adds the sum of absolute values of the coefficients to the cost function. $`\\\\lambda\\\\sum_{i=1}^{n} \\\\left | w_i \\\\right |`$\\n2. L2 Regularization (Ridge regularization) - Adds the sum of squares of coefficients to the cost function. $`\\\\lambda\\\\sum_{i=1}^{n} {w_{i}}^{2}`$\\n\\n* Where $`\\\\lambda`$ determines the amount of regularization.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['know', 'regularization', 'techniques'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What kind of regularization techniques are applicable to linear models? \\u200dÔ∏è',\n",
       "  'reference_answer': 'AIC/BIC, Ridge regression, Lasso, Elastic Net, Basis pursuit denoising, Rudin‚ÄìOsher‚ÄìFatemi model (TV), Potts model, RLAD,\\nDantzig Selector,SLOPE',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['techniques', 'kind', 'regularization', 'applicable', 'linear'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How does L2 regularization look like in a linear model? \\u200dÔ∏è',\n",
       "  'reference_answer': 'L2 regularization adds a penalty term to our cost function which is equal to the sum of squares of models coefficients multiplied by a lambda hyperparameter. This technique makes sure that the coefficients are close to zero and is widely used in cases when we have a lot of features that might correlate with each other.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['does', 'like', 'look', 'regularization', 'linear'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do we select the right regularization parameters?',\n",
       "  'reference_answer': 'Regularization parameters can be chosen using a grid search, for example https://scikit-learn.org/stable/modules/linear_model.html has one formula for the implementing for regularization, alpha in the formula mentioned can be found by doing a RandomSearch or a GridSearch on a set of values and selecting the alpha which gives the least cross validation or validation error.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['regularization', 'parameters', 'right', 'select'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What‚Äôs the effect of L2 regularization on the weights of a linear model? \\u200dÔ∏è',\n",
       "  'reference_answer': 'L2 regularization penalizes larger weights more severely (due to the squared penalty term), which encourages weight values to decay toward zero.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['effect', 'weights', 'regularization', 'model', 'linear'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How L1 regularization looks like in a linear model? \\u200dÔ∏è',\n",
       "  'reference_answer': 'L1 regularization adds a penalty term to our cost function which is equal to the sum of modules of models coefficients multiplied by a lambda hyperparameter. For example, cost function with L1 regularization will look like: $`\\\\sum_{i=0}^{N} (y_i - \\\\sum_{j=0}^{M} x_{ij} * w_j)+\\\\lambda\\\\sum_{j=0}^{M} \\\\left | w_j \\\\right |`$',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['like', 'regularization', 'model', 'linear', 'looks'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What‚Äôs the difference between L2 and L1 regularization? \\u200dÔ∏è',\n",
       "  'reference_answer': '- Penalty terms: L1 regularization uses the sum of the absolute values of the weights, while L2 regularization uses the sum of the weights squared.\\n- Feature selection: L1 performs feature selection by reducing the coefficients of some predictors to 0, while L2 does not.\\n- Computational efficiency: L2 has an analytical solution, while L1 does not.\\n- Multicollinearity: L2 addresses multicollinearity by constraining the coefficient norm.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['between', 'regularization', 'difference'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Can we have both L1 and L2 regularization components in a linear model? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Yes, elastic net regularization combines L1 and L2 regularization.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['components', 'both', 'regularization', 'have', 'linear'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What‚Äôs the interpretation of the bias term in linear models? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Bias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['term',\n",
       "   'regularization',\n",
       "   'models',\n",
       "   'bias',\n",
       "   'linear',\n",
       "   'interpretation'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do we interpret weights in linear models? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Without normalizing weights or variables, if you increase the corresponding predictor by one unit, the coefficient represents on average how much the output changes. By the way, this interpretation still works for logistic regression - if you increase the corresponding predictor by one unit, the weight represents the change in the log of the odds.\\n\\nIf the variables are normalized, we can interpret weights in linear models like the importance of this variable in the predicted result.',\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['weights', 'regularization', 'models', 'interpret', 'linear'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'If a weight for one variable is higher than for another \\u200a‚Äî\\u200a can we say that this variable is more important? \\u200dÔ∏è',\n",
       "  'reference_answer': \"Yes - if your predictor variables are normalized.\\n\\nWithout normalization, the weight represents the change in the output per unit change in the predictor. If you have a predictor with a huge range and scale that is used to predict an output with a very small range - for example, using each nation's GDP to predict maternal mortality rates - your coefficient should be very small. That does not necessarily mean that this predictor variable is not important compared to the others.\",\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['weight',\n",
       "   'than',\n",
       "   'regularization',\n",
       "   'another',\n",
       "   'higher',\n",
       "   'variable'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'When do we need to perform feature normalization for linear models? When it‚Äôs okay not to do it? \\u200dÔ∏è',\n",
       "  'reference_answer': \"Feature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This can't be done effectively if every feature is scaled differently. \\n\\nLinear regression without regularization techniques can be used without feature normalization. Also, regularization can help to make the analytical solution more stable, ‚Äî it adds the regularization matrix to the feature matrix before inverting it.\",\n",
       "  'topic': 'regularization',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['regularization',\n",
       "   'perform',\n",
       "   'need',\n",
       "   'normalization',\n",
       "   'feature',\n",
       "   'linear'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is feature selection? Why do we need it?',\n",
       "  'reference_answer': 'Feature Selection is a method used to select the relevant features for the model to train on. We need feature selection to remove the irrelevant features which leads the model to under-perform.',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['feature_selection', 'need', 'selection', 'feature'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Is feature selection important for linear models? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Yes, It is. It can make model performance better through selecting the most importance features and remove irrelevant features in order to make a prediction and it can also avoid overfitting, underfitting and bias-variance tradeoff.',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['important',\n",
       "   'models',\n",
       "   'feature_selection',\n",
       "   'selection',\n",
       "   'feature',\n",
       "   'linear'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Which feature selection techniques do you know? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Here are some of the feature selections:\\n- Principal Component Analysis\\n- Neighborhood Component Analysis\\n- ReliefF Algorithm',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['techniques', 'feature_selection', 'selection', 'feature', 'know'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Can we use L1 regularization for feature selection? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Yes, because the nature of L1 regularization will lead to sparse coefficients of features. Feature selection can be done by keeping only features with non-zero coefficients.',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['feature_selection', 'regularization', 'selection', 'feature'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Can we use L2 regularization for feature selection? \\u200dÔ∏è',\n",
       "  'reference_answer': 'No, Because L2 regularization does not make the weights zero but only makes them very very small. L2 regularization can be used to solve multicollinearity since it stabilizes the model.',\n",
       "  'topic': 'feature_selection',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['feature_selection', 'regularization', 'selection', 'feature'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are the decision trees?',\n",
       "  'reference_answer': 'This is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. \\n\\nIn this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible.\\n\\nA decision tree is a flowchart-like tree structure, where each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a value for the target variable.\\n\\nVarious techniques : like Gini, Information Gain, Chi-square, entropy.',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['decision', 'decision_trees', 'trees'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do we train decision trees? \\u200dÔ∏è',\n",
       "  'reference_answer': '1. Start at the root node.\\n2. For each variable X, find the set S_1 that minimizes the sum of the node impurities in the two child nodes and choose the split {X*,S*} that gives the minimum over all X and S.\\n3. If a stopping criterion is reached, exit. Otherwise, apply step 2 to each child node in turn.',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['decision', 'decision_trees', 'train', 'trees'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are the main parameters of the decision tree model?',\n",
       "  'reference_answer': '* maximum tree depth\\n* minimum samples per leaf node\\n* impurity criterion',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['decision_trees', 'decision', 'parameters', 'model', 'tree'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do we handle categorical variables in decision trees? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Some decision tree algorithms can handle categorical variables out of the box, others cannot. However, we can transform categorical variables, e.g. with a binary or a one-hot encoder.',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['decision_trees',\n",
       "   'variables',\n",
       "   'decision',\n",
       "   'trees',\n",
       "   'categorical',\n",
       "   'handle'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are the benefits of a single decision tree compared to more complex models? \\u200dÔ∏è',\n",
       "  'reference_answer': '* easy to implement\\n* fast training\\n* fast inference\\n* good explainability',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['decision_trees',\n",
       "   'decision',\n",
       "   'benefits',\n",
       "   'compared',\n",
       "   'single',\n",
       "   'tree'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How can we know which features are more important for the decision tree model? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Often, we want to find a split such that it minimizes the sum of the node impurities. The impurity criterion is a parameter of decision trees. Popular methods to measure the impurity are the Gini impurity and the entropy describing the information gain.',\n",
       "  'topic': 'decision_trees',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['important',\n",
       "   'decision_trees',\n",
       "   'decision',\n",
       "   'features',\n",
       "   'more',\n",
       "   'know'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is random forest?',\n",
       "  'reference_answer': 'Random Forest is a machine learning method for regression and classification which is composed of many decision trees. Random Forest belongs to a larger class of ML algorithms called ensemble methods (in other words, it involves the combination of several models to solve a single prediction problem).',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['forest', 'random', 'random_forest'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Why do we need randomization in random forest? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Random forest in an extension of the **bagging** algorithm which takes *random data samples from the training dataset* (with replacement), trains several models and averages predictions. In addition to that, each time a split in a tree is considered, random forest takes a *random sample of m features from full set of n features* (without replacement) and uses this subset of features as candidates for the split (for example, `m = sqrt(n)`).\\n\\nTraining decision trees on random data samples from the training dataset *reduces variance*. Sampling features for each split in a decision tree *decorrelates trees*.',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['random_forest', 'need', 'forest', 'random', 'randomization'],\n",
       "  'key_concepts': ['decorrelates_trees',\n",
       "   'random_data_samples_from_the_training_dataset',\n",
       "   'random_sample_of_m_features_from_full_set_of_n_features',\n",
       "   'reduces_variance',\n",
       "   'bagging']},\n",
       " {'text': 'What are the main parameters of the random forest model? \\u200dÔ∏è',\n",
       "  'reference_answer': '- `max_depth`: Longest Path between root node and the leaf\\n- `min_sample_split`: The minimum number of observations needed to split a given node\\n- `max_leaf_nodes`: Conditions the splitting of the tree and hence, limits the growth of the trees\\n- `min_samples_leaf`: minimum number of samples in the leaf node\\n- `n_estimators`: Number of trees\\n- `max_sample`: Fraction of original dataset given to any individual tree in the given model\\n- `max_features`: Limits the maximum number of features provided to trees in random forest model',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['parameters', 'random_forest', 'model', 'forest', 'random'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do we select the depth of the trees in random forest? \\u200dÔ∏è',\n",
       "  'reference_answer': 'The greater the depth, the greater amount of information is extracted from the tree, however, there is a limit to this, and the algorithm even if defensive against overfitting may learn complex features of noise present in data and as a result, may overfit on noise. Hence, there is no hard thumb rule in deciding the depth, but literature suggests a few tips on tuning the depth of the tree to prevent overfitting:\\n\\n- limit the maximum depth of a tree\\n- limit the number of test nodes\\n- limit the minimum number of objects at a node required to split\\n- do not split a node when, at least, one of the resulting subsample sizes is below a given threshold\\n- stop developing a node if it does not sufficiently improve the fit.',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['depth', 'select', 'trees', 'random_forest', 'forest', 'random'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do we know how many trees we need in random forest? \\u200dÔ∏è',\n",
       "  'reference_answer': 'The number of trees in random forest is worked by n_estimators, and a random forest reduces overfitting by increasing the number of trees. There is no fixed thumb rule to decide the number of trees in a random forest, it is rather fine tuned with the data, typically starting off by taking the square of the number of features (n) present in the data followed by tuning until we get the optimal results.',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['trees', 'random_forest', 'many', 'need', 'random', 'know'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Is it easy to parallelize training of a random forest model? How can we do it? \\u200dÔ∏è',\n",
       "  'reference_answer': \"Yes, R provides a simple way to parallelize training of random forests on large scale data.\\nIt makes use of a parameter called multicombine which can be set to TRUE for parallelizing random forest computations.\\n\\n```R\\nrf <- foreach(ntree=rep(25000, 6), .combine=randomForest::combine,\\n              .multicombine=TRUE, .packages='randomForest') %dopar% {\\n    randomForest(x, y, ntree=ntree)\\n}\\n```\",\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['parallelize',\n",
       "   'training',\n",
       "   'random_forest',\n",
       "   'easy',\n",
       "   'forest',\n",
       "   'random'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are the potential problems with many large trees? \\u200dÔ∏è',\n",
       "  'reference_answer': \"- Overfitting: A large number of large trees can lead to overfitting, where the model becomes too complex and is able to memorize the training data but doesn't generalize well to new, unseen data.\\n\\n- Slow prediction time: As the number of trees in the forest increases, the prediction time for new data points can become quite slow. This can be a problem when you need to make predictions in real-time or on a large dataset.\\n\\n- Memory consumption: Random Forest models with many large trees can consume a lot of memory, which can be a problem when working with large datasets or on a limited hardware.\\n\\n- Lack of interpretability: Random Forest models with many large trees can be difficult to interpret, making it harder to understand how the model is making predictions or what features are most important.\\n\\n- Difficulty in tuning : With an increasing number of large trees the tuning process becomes more complex and computationally expensive.\\n\\nIt's important to keep in mind that the number of trees in a Random Forest should be chosen based on the specific problem and dataset, rather than using a large number of trees by default. In practice, the number of trees in a random forest is chosen based on the trade-off between the computational cost and the performance.\",\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['large', 'potential', 'random_forest', 'with', 'many', 'problems'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What if instead of finding the best split, we randomly select a few splits and just select the best from them. Will it work? üöÄ',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['randomly', 'random_forest', 'best', 'split', 'instead', 'finding'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What happens when we have correlated features in our data? \\u200dÔ∏è',\n",
       "  'reference_answer': 'In random forest, since random forest samples some features to build each tree, the information contained in correlated features is twice as much likely to be picked than any other information contained in other features. \\n\\nIn general, when you are adding correlated features, it means that they linearly contains the same information and thus it will reduce the robustness of your model. Each time you train your model, your model might pick one feature or the other to \"do the same job\" i.e. explain some variance, reduce entropy, etc.',\n",
       "  'topic': 'random_forest',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['happens',\n",
       "   'features',\n",
       "   'random_forest',\n",
       "   'correlated',\n",
       "   'data',\n",
       "   'have'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is gradient boosting trees? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['gradient', 'boosting', 'gradient_boosting', 'trees'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What‚Äôs the difference between random forest and gradient boosting? \\u200dÔ∏è',\n",
       "  'reference_answer': '1. Random Forests builds each tree independently while Gradient Boosting builds one tree at a time.\\n   2. Random Forests combine results at the end of the process (by averaging or \"majority rules\") while Gradient Boosting combines     results along the way.',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['difference',\n",
       "   'gradient_boosting',\n",
       "   'between',\n",
       "   'forest',\n",
       "   'gradient',\n",
       "   'random'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Is it possible to parallelize training of a gradient boosting model? How to do it? \\u200dÔ∏è',\n",
       "  'reference_answer': \"Yes, different frameworks provide different options to make training faster, using GPUs to speed up the process by making it highly parallelizable.For example, for XGBoost <i>tree_method = 'gpu_hist'</i> option makes training faster by use of GPUs.\",\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['gradient_boosting',\n",
       "   'parallelize',\n",
       "   'training',\n",
       "   'boosting',\n",
       "   'possible',\n",
       "   'gradient'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Feature importance in gradient boosting trees \\u200a‚Äî\\u200a what are possible options? \\u200dÔ∏è',\n",
       "  'reference_answer': 'With CatBoost you can use implemented method get_feature_importance for getting SHAP values. https://arxiv.org/abs/1905.04610v1\\n\\nIt allows to understand how excluding features helps to provide better results. Higher value is better.\\n\\nAlso you can add random noise column to your data (with normal distribution), calculate feature importance and exclude all features below noise importance.',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['importance',\n",
       "   'gradient_boosting',\n",
       "   'trees',\n",
       "   'boosting',\n",
       "   'gradient',\n",
       "   'feature'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Are there any differences between continuous and discrete variables when it comes to feature importance of gradient boosting models? üöÄ',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['gradient_boosting',\n",
       "   'discrete',\n",
       "   'continuous',\n",
       "   'between',\n",
       "   'there',\n",
       "   'differences'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are the main parameters in the gradient boosting model? \\u200dÔ∏è',\n",
       "  'reference_answer': 'There are many parameters, but below are a few key defaults.\\n* learning_rate=0.1 (shrinkage).\\n* n_estimators=100 (number of trees).\\n* max_depth=3.\\n* min_samples_split=2.\\n* min_samples_leaf=1.\\n* subsample=1.0.',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['gradient_boosting', 'boosting', 'parameters', 'model', 'gradient'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do you approach tuning parameters in XGBoost or LightGBM? üöÄ',\n",
       "  'reference_answer': 'Depending upon the dataset, parameter tuning can be done manually or using hyperparameter optimization frameworks such as optuna and hyperopt. In manual parameter tuning, we need to be aware of max-depth, min_samples_leaf and min_samples_split so that our model does not overfit the data but try to predict generalized characteristics of data (basically keeping variance and bias low for our model).',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['approach',\n",
       "   'gradient_boosting',\n",
       "   'tuning',\n",
       "   'parameters',\n",
       "   'lightgbm',\n",
       "   'xgboost'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do you select the number of trees in the gradient boosting model? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Most implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds or thousands. Using scikit-learn we can perform a grid search of the n_estimators model parameter',\n",
       "  'topic': 'gradient_boosting',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['gradient_boosting',\n",
       "   'select',\n",
       "   'trees',\n",
       "   'boosting',\n",
       "   'gradient',\n",
       "   'number'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Which hyper-parameter tuning strategies (in general) do you know? \\u200dÔ∏è',\n",
       "  'reference_answer': 'There are several strategies for hyper-tuning but I would argue that the three most popular nowadays are the following:\\n* <b>Grid Search</b> is an exhaustive approach such that for each hyper-parameter, the user needs to <i>manually</i> give a list of values for the algorithm to try. After these values are selected, grid search then evaluates the algorithm using each and every combination of hyper-parameters and returns the combination that gives the optimal result (i.e. lowest MAE). Because grid search evaluates the given algorithm using all combinations, it\\'s easy to see that this can be quite computationally expensive and can lead to sub-optimal results specifically since the user needs to specify specific values for these hyper-parameters, which is prone for error and requires domain knowledge.\\n\\n* <b>Random Search</b> is similar to grid search but differs in the sense that rather than specifying which values to try for each hyper-parameter, an upper and lower bound of values for each hyper-parameter is given instead. With uniform probability, random values within these bounds are then chosen and similarly, the best combination is returned to the user. Although this seems less intuitive, no domain knowledge is necessary and theoretically much more of the parameter space can be explored.\\n\\n* In a completely different framework, <b>Bayesian Optimization</b> is thought of as a more statistical way of optimization and is commonly used when using neural networks, specifically since one evaluation of a neural network can be computationally costly. In numerous research papers, this method heavily outperforms Grid Search and Random Search and is currently used on the Google Cloud Platform as well as AWS. Because an in-depth explanation requires a heavy background in bayesian statistics and gaussian processes (and maybe even some game theory), a \"simple\" explanation is that a much simpler/faster <i>acquisition function</i> intelligently chooses (using a <i>surrogate function</i> such as probability of improvement or GP-UCB) which hyper-parameter values to try on the computationally expensive, original algorithm. Using the result of the initial combination of values on the expensive/original function, the acquisition function takes the result of the expensive/original algorithm into account and uses it as its prior knowledge to again come up with another set of hyper-parameters to choose during the next iteration. This process continues either for a specified number of iterations or for a specified amount of time and similarly the combination of hyper-parameters that performs the best on the expensive/original algorithm is chosen.',\n",
       "  'topic': 'parameter_tuning',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['tuning',\n",
       "   'strategies',\n",
       "   'general',\n",
       "   'parameter_tuning',\n",
       "   'parameter',\n",
       "   'hyper'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What‚Äôs the difference between grid search parameter tuning strategy and random search? When to use one or another? \\u200dÔ∏è',\n",
       "  'reference_answer': 'For specifics, refer to the above answer.',\n",
       "  'topic': 'parameter_tuning',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['difference',\n",
       "   'search',\n",
       "   'grid',\n",
       "   'between',\n",
       "   'parameter_tuning',\n",
       "   'parameter'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What kind of problems neural nets can solve?',\n",
       "  'reference_answer': 'Neural nets are good at solving non-linear problems. Some good examples are problems that are relatively easy for humans (because of experience, intuition, understanding, etc), but difficult for traditional regression models: speech recognition, handwriting recognition, image identification, etc.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['kind', 'nets', 'solve', 'neural_networks', 'neural', 'problems'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How does a usual fully-connected feed-forward neural network work? \\u200dÔ∏è',\n",
       "  'reference_answer': 'In a usual fully-connected feed-forward network, each neuron receives input from every element of the previous layer and thus the receptive field of a neuron is the entire previous layer. They are usually used to represent feature vectors for input data in classification problems but can be expensive to train because of the number of computations involved.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['does', 'feed', 'fully', 'neural_networks', 'usual', 'connected'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Why do we need activation functions?',\n",
       "  'reference_answer': 'The main idea of using neural networks is to learn complex nonlinear functions. If we are not using an activation function in between different layers of a neural network, we are just stacking up multiple linear layers one on top of another and this leads to learning a linear function. The Nonlinearity comes only with the activation function, this is the reason we need activation functions.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['neural_networks', 'need', 'activation', 'functions'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are the problems with sigmoid as an activation function? \\u200dÔ∏è',\n",
       "  'reference_answer': 'The derivative of the sigmoid function for large positive or negative numbers is almost zero. From this comes the problem of vanishing gradient ‚Äî during the backpropagation our net will not learn (or will learn drastically slow). One possible way to solve this problem is to use ReLU activation function.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['activation',\n",
       "   'function',\n",
       "   'neural_networks',\n",
       "   'sigmoid',\n",
       "   'with',\n",
       "   'problems'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is ReLU? How is it better than sigmoid or tanh? \\u200dÔ∏è',\n",
       "  'reference_answer': 'ReLU is an abbreviation for Rectified Linear Unit. It is an activation function which has the value 0 for all negative values and the value f(x) = x for all positive values. The ReLU has a simple activation function which makes it fast to compute and while the sigmoid and tanh activation functions saturate at higher values, the ReLU has a potentially infinite activation, which addresses the problem of vanishing gradients.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['relu', 'than', 'tanh', 'neural_networks', 'better', 'sigmoid'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How we can initialize the weights of a neural network? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Proper initialization of weight matrix in neural network is very necessary.\\nSimply we can say there are two ways for initializations.\\n   1. Initializing weights with zeroes.\\n      Setting weights to zero makes your network no better than a linear model. It is important to note that setting biases to 0 will not create any troubles as non zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron are still different.  \\n   2. Initializing weights randomly.\\n      Assigning random values to weights is better than just 0 assignment. \\n* a) If weights are initialized with very high values the term np.dot(W,X)+b becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where the slope of gradient changes slowly and learning takes a lot of time.\\n* b) If weights are initialized with low values it gets mapped to 0, where the case is the same as above. This problem is often referred to as the vanishing gradient.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['initialize', 'weights', 'neural_networks', 'neural', 'network'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What if we set all the weights of a neural network to 0? \\u200dÔ∏è',\n",
       "  'reference_answer': 'If all the weights of a neural network are set to zero, the output of each connection is same (W*x = 0). This means the gradients which are backpropagated to each connection in a layer is same. This means all the connections/weights learn the same thing, and the model never converges.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['neural_networks', 'neural', 'network', 'weights'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What regularization techniques for neural nets do you know? \\u200dÔ∏è',\n",
       "  'reference_answer': '* L1 Regularization - Defined as the sum of absolute values of the individual parameters. The L1 penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded. \\n* L2 Regularization - Defined as the sum of square of individual parameters. Often supported by regularization hyperparameter alpha. It results in weight decay. \\n* Data Augmentation - This requires some fake data to be created as a part of training set. \\n* Drop Out : This is most effective regularization technique for neural nets. Few random nodes in each layer is deactivated in forward pass. This allows the algorithm to train on different set of nodes in each iterations.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['techniques',\n",
       "   'nets',\n",
       "   'regularization',\n",
       "   'neural_networks',\n",
       "   'neural',\n",
       "   'know'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is dropout? Why is it useful? How does it work? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Dropout is a technique that at each training step turns off each neuron with a certain probability of *p*. This way at each iteration we train only *1-p* of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter *p*.',\n",
       "  'topic': 'neural_networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['does', 'dropout', 'useful', 'neural_networks', 'work'],\n",
       "  'key_concepts': ['1-p']},\n",
       " {'text': 'What is backpropagation? How does it work? Why do we need it? \\u200dÔ∏è',\n",
       "  'reference_answer': 'The Backpropagation algorithm looks for the minimum value of the error function in weight space using a technique called the delta rule or gradient descent. \\nThe weights that minimize the error function is then considered to be a solution to the learning problem. \\n\\nWe need backpropogation because,\\n* Calculate the error ‚Äì How far is your model output from the actual output.\\n* Minimum Error ‚Äì Check whether the error is minimized or not.\\n* Update the parameters ‚Äì If the error is huge then, update the parameters (weights and biases). After that again check the error.  \\nRepeat the process until the error becomes minimum.\\n* Model is ready to make a prediction ‚Äì Once the error becomes minimum, you can feed some inputs to your model and it will produce the output.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['does',\n",
       "   'need',\n",
       "   'optimization_in_neural\\xa0networks',\n",
       "   'backpropagation',\n",
       "   'work'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Which optimization techniques for training neural nets do you know? \\u200dÔ∏è',\n",
       "  'reference_answer': '* Gradient Descent\\n* Stochastic Gradient Descent\\n* Mini-Batch Gradient Descent(best among gradient descents)\\n* Nesterov Accelerated Gradient\\n* Momentum\\n* Adagrad \\n* AdaDelta\\n* Adam(best one. less time, more efficient)',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['techniques',\n",
       "   'nets',\n",
       "   'training',\n",
       "   'neural',\n",
       "   'optimization',\n",
       "   'optimization_in_neural\\xa0networks'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do we use SGD (stochastic gradient descent) for training a neural net? \\u200dÔ∏è',\n",
       "  'reference_answer': 'SGD approximates the expectation with few randomly selected samples (instead of the full data). In comparison to batch gradient descent, we can efficiently approximate the expectation in large data sets using SGD. For neural networks this reduces the training time a lot even considering that it will converge later as the random sampling adds noise to the gradient descent.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['descent',\n",
       "   'stochastic',\n",
       "   'training',\n",
       "   'neural',\n",
       "   'gradient',\n",
       "   'optimization_in_neural\\xa0networks'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What‚Äôs the learning rate?',\n",
       "  'reference_answer': 'The learning rate is an important hyperparameter that controls how quickly the model is adapted to the problem during the training. It can be seen as the \"step width\" during the parameter updates, i.e. how far the weights are moved into the direction of the minimum of our optimization problem.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['optimization_in_neural\\xa0networks', 'rate', 'learning'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What happens when the learning rate is too large? Too small?',\n",
       "  'reference_answer': 'A large learning rate can accelerate the training. However, it is possible that we \"shoot\" too far and miss the minimum of the function that we want to optimize, which will not result in the best solution. On the other hand, training with a small learning rate takes more time but it is possible to find a more precise minimum. The downside can be that the solution is stuck in a local minimum, and the weights won\\'t update even if it is not the best possible global solution.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['large',\n",
       "   'happens',\n",
       "   'small',\n",
       "   'learning',\n",
       "   'optimization_in_neural\\xa0networks',\n",
       "   'rate'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How to set the learning rate? \\u200dÔ∏è',\n",
       "  'reference_answer': \"There is no straightforward way of finding an optimum learning rate for a model. It involves a lot of hit and trial. Usually starting with a small values such as 0.01 is a good starting point for setting a learning rate and further tweaking it so that it doesn't overshoot or converge too slowly.\",\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['optimization_in_neural\\xa0networks', 'rate', 'learning'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is Adam? What‚Äôs the main difference between Adam and SGD? \\u200dÔ∏è',\n",
       "  'reference_answer': \"Adam (Adaptive Moment Estimation) is a optimization technique for training neural networks. on an average, it is the best optimizer .It works with momentums of first and second order. The intuition behind the Adam is that we don‚Äôt want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.\\n\\nAdam tends to converge faster, while SGD often converges to more optimal solutions.\\nSGD's high variance disadvantages gets rectified by Adam (as advantage for Adam).\",\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['optimization_in_neural\\xa0networks',\n",
       "   'difference',\n",
       "   'adam',\n",
       "   'between'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'When would you use Adam and when SGD? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Adam tends to converge faster, while SGD often converges to more optimal solutions.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['optimization_in_neural\\xa0networks', 'adam', 'would'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Do we want to have a constant learning rate or we better change it throughout training? \\u200dÔ∏è',\n",
       "  'reference_answer': \"Generally, it is recommended to start learning rate with relatively high value and then gradually decrease learning rate so the model does not overshoot the minima and at the same time we don't want to start with very low learning rate as the model will take too long to converge. There are many available techniques to do decay the learning rate. For example, in PyTorch you can use\\na function called **StepLR** which decays the learning rate of each parameter by value **gamma**-which we have to pass through argument- after n number of epoch which you can also set through function argument named **epoch_size**.\",\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['want',\n",
       "   'have',\n",
       "   'constant',\n",
       "   'learning',\n",
       "   'optimization_in_neural\\xa0networks',\n",
       "   'rate'],\n",
       "  'key_concepts': ['gamma', 'steplr', 'epoch_size']},\n",
       " {'text': 'How do we decide when to stop training a neural net?',\n",
       "  'reference_answer': 'Simply stop training when the validation error is the minimum.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['training',\n",
       "   'neural',\n",
       "   'optimization_in_neural\\xa0networks',\n",
       "   'stop',\n",
       "   'decide'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is model checkpointing? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Saving the weights learned by a model mid training for long running processes is known as model checkpointing so that you can resume your training from a certain checkpoint.',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['optimization_in_neural\\xa0networks', 'checkpointing', 'model'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Can you tell us how you approach the model training process? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'optimization_in_neural\\xa0networks',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['approach',\n",
       "   'training',\n",
       "   'model',\n",
       "   'tell',\n",
       "   'process',\n",
       "   'optimization_in_neural\\xa0networks'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How we can use neural nets for computer vision? \\u200dÔ∏è',\n",
       "  'reference_answer': \"Neural nets used in the area of computer vision are generally Convolutional Neural Networks(CNN's). You can learn about convolutions below. It appears that convolutions are quite powerful when it comes to working with images and videos due to their ability to extract and learn complex features. Thus CNN's are a go-to method for any problem in computer vision.\",\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['nets',\n",
       "   'neural',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'vision',\n",
       "   'computer'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What‚Äôs a convolutional layer? \\u200dÔ∏è',\n",
       "  'reference_answer': 'The idea of the convolutional layer is the assumption that the information needed for making a decision often is spatially close and thus, it only takes the weighted sum over nearby inputs. It also assumes that the networks‚Äô kernels can be reused for all nodes, hence the number of weights can be drastically reduced. To counteract only one feature being learnt per layer, multiple kernels are applied to the input which creates parallel channels in the output. Consecutive layers can also be stacked to allow the network to find more high-level features.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['convolutional', 'neural_networks_for_computer\\xa0vision', 'layer'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Why do we actually need convolutions? Can‚Äôt we use fully-connected layers for that? \\u200dÔ∏è',\n",
       "  'reference_answer': 'A fully-connected layer needs one weight per inter-layer connection, which means the number of weights which needs to be computed quickly balloons as the number of layers and nodes per layer is increased.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['actually',\n",
       "   'fully',\n",
       "   'need',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'convolutions',\n",
       "   'connected'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What‚Äôs pooling in CNN? Why do we need it? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Pooling is a technique to downsample the feature map. It allows layers which receive relatively undistorted versions of the input to learn low level features such as lines, while layers deeper in the model can learn more abstract features such as texture.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['need', 'neural_networks_for_computer\\xa0vision', 'pooling'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How does max pooling work? Are there other pooling techniques? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Max pooling is a technique where the maximum value of a receptive field is passed on in the next feature map. The most commonly used receptive field is 2 x 2 with a stride of 2, which means the feature map is downsampled from N x N to N/2 x N/2. Receptive fields larger than 3 x 3 are rarely employed as too much information is lost. \\n\\nOther pooling techniques include:\\n\\n* Average pooling, the output is the average value of the receptive field.\\n* Min pooling, the output is the minimum value of the receptive field.\\n* Global pooling, where the receptive field is set to be equal to the input size, this means the output is equal to a scalar and can be used to reduce the dimensionality of the feature map.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['other',\n",
       "   'does',\n",
       "   'pooling',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'there',\n",
       "   'work'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Are CNNs resistant to rotations? What happens to the predictions of a CNN if an image is rotated? üöÄ',\n",
       "  'reference_answer': 'CNNs are not resistant to rotation by design. However, we can make our models resistant by augmenting our datasets with different rotations of the raw data. The predictions of a CNN will change if an image is rotated and we did not augment our dataset accordingly. A demonstration of this occurence can be seen in [this video](https://www.youtube.com/watch?v=VO1bQo4PXV4), where a CNN changes its predicted class between a duck and a rabbit based on the rotation of the image.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['happens',\n",
       "   'rotations',\n",
       "   'cnns',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'resistant',\n",
       "   'predictions'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are augmentations? Why do we need them?',\n",
       "  'reference_answer': 'Augmentations are an artifical way of expanding the existing datasets by performing some transformations, color shifts or many other things on the data. It helps in diversifying the data and even increasing the data when there is scarcity of data for a model to train on.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['need',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'them',\n",
       "   'augmentations'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What kind of augmentations do you know?',\n",
       "  'reference_answer': 'There are many kinds of augmentations which can be used according to the type of data you are working on some of which are geometric and numerical transformation, PCA, cropping, padding, shifting, noise injection etc.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['know',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'kind',\n",
       "   'augmentations'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How to choose which augmentations to use? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Augmentations really depend on the type of output classes and the features you want your model to learn. For eg. if you have mostly properly illuminated images in your dataset and want your model to predict poorly illuminated images too, you can apply channel shifting on your data and include the resultant images in your dataset for better results.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['neural_networks_for_computer\\xa0vision',\n",
       "   'choose',\n",
       "   'augmentations'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What kind of CNN architectures for classification do you know? üöÄ',\n",
       "  'reference_answer': 'Image Classification\\n* Inception v3\\n* Xception \\n* DenseNet\\n* AlexNet\\n* VGG16\\n* ResNet\\n* SqueezeNet\\n* EfficientNet\\n* MobileNet\\n\\nThe last three are designed so they use smaller number of parameters which is helpful for edge AI.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['kind',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'know',\n",
       "   'classification',\n",
       "   'architectures'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is transfer learning? How does it work? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Given a source domain D_S and learning task T_S, a target domain D_T and learning task T_T, transfer learning aims to help improve the learning of the target predictive function f_T in D_T using the knowledge in D_S and T_S, where D_S ‚â† D_T,or T_S ‚â† T_T. In other words, transfer learning enables to reuse knowledge coming from other domains or learning tasks.\\n\\nIn the context of CNNs, we can use networks that were pre-trained on popular datasets such as ImageNet. We then can use the weights of the layers that learn to represent features and combine them with a new set of layers that learns to map the feature representations to the given classes. Two popular strategies are either to freeze the layers that learn the feature representations completely, or to give them a smaller learning rate.',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['does',\n",
       "   'transfer',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'learning',\n",
       "   'work'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is object detection? Do you know any architectures for that? üöÄ',\n",
       "  'reference_answer': 'Object detection is finding Bounding Boxes around objects in an image. \\nArchitectures :\\nYOLO, Faster RCNN, Center Net',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['detection',\n",
       "   'architectures',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'that',\n",
       "   'know',\n",
       "   'object'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is object segmentation? Do you know any architectures for that? üöÄ',\n",
       "  'reference_answer': 'Object Segmentation is predicting masks. It does not differentiate objects. \\nArchitectures :\\nMask RCNN, UNet',\n",
       "  'topic': 'neural_networks_for_computer\\xa0vision',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['architectures',\n",
       "   'neural_networks_for_computer\\xa0vision',\n",
       "   'that',\n",
       "   'know',\n",
       "   'segmentation',\n",
       "   'object'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How can we use machine learning for text classification? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Machine learning classification algorithms predict a class based on a numerical feature representation. This means that in order to use machine learning for text classification, we need to extract numerical features from our text data first before we can apply machine learning algorithms. Common approaches to extract numerical features from text data are bag of words, N-grams or word embeddings.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['text',\n",
       "   'text_classification',\n",
       "   'machine',\n",
       "   'classification',\n",
       "   'learning'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is bag of words? How we can use it for text classification? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Bag of Words is a representation of text that describes the occurrence of words within a document. The order or structure of the words is not considered. For text classification, we look at the histogram of the words within the text and consider each word count as a feature.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['text', 'words', 'text_classification', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are the advantages and disadvantages of bag of words? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Advantages:\\n1. Simple to understand and implement.\\n\\nDisadvantages:\\n1. The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\\n2. Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons\\n3. Discarding word order ignores the context, and in turn meaning of words in the document. Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (‚Äúthis is interesting‚Äù vs ‚Äúis this interesting‚Äù), synonyms (‚Äúold bike‚Äù vs ‚Äúused bike‚Äù).',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['advantages', 'disadvantages', 'text_classification', 'words'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are N-grams? How can we use them? \\u200dÔ∏è',\n",
       "  'reference_answer': 'The function to tokenize into consecutive sequences of words is called n-grams. It can be used to find out N most co-occurring words (how often word X is followed by word Y) in a given sentence.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['grams', 'them', 'text_classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How large should be N for our bag of words when using N-grams? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['large',\n",
       "   'should',\n",
       "   'words',\n",
       "   'text_classification',\n",
       "   'using',\n",
       "   'grams'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is TF-IDF? How is it useful for text classification? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Term Frequency (TF) is a scoring of the frequency of the word in the current document. Inverse Document Frequency(IDF) is a scoring of how rare the word is across documents. It is used in scenario where highly recurring words may not contain as much informational content as the domain specific words. For example, words like ‚Äúthe‚Äù that are frequent across all documents therefore need to be less weighted. The TF-IDF score highlights words that are distinct (contain useful information) in a given document.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['useful', 'text', 'text_classification', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Which model would you use for text classification with bag of words features? \\u200dÔ∏è',\n",
       "  'reference_answer': '1. Bag Of Words model\\n2. Word2Vec Embeddings\\n3. fastText Embeddings\\n4. Convolutional Neural Networks (CNN)\\n5. Long Short-Term Memory (LSTM)\\n6. Bidirectional Encoder Representations from Transformers (BERT)',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['text',\n",
       "   'text_classification',\n",
       "   'model',\n",
       "   'with',\n",
       "   'would',\n",
       "   'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Usually logistic regression is better because bag of words creates a matrix with large number of columns. For a huge number of columns logistic regression is usually faster than gradient boosting trees.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['trees',\n",
       "   'boosting',\n",
       "   'text_classification',\n",
       "   'gradient',\n",
       "   'prefer',\n",
       "   'would'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are word embeddings? Why are they useful? Do you know Word2Vec? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Word Embeddings are vector representations for words. Each word is mapped to one vector, this vector tries to capture some characteristics of the word, allowing similar words to have similar vector representations.  \\nWord Embeddings helps in capturing the inter-word semantics and represents it in real-valued vectors.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['embeddings',\n",
       "   'useful',\n",
       "   'text_classification',\n",
       "   'word',\n",
       "   'they',\n",
       "   'know'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Do you know any other ways to get word embeddings? üöÄ',\n",
       "  'reference_answer': '- GloVe\\n- BERT',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['other',\n",
       "   'embeddings',\n",
       "   'text_classification',\n",
       "   'word',\n",
       "   'know',\n",
       "   'ways'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Approaches ranked from simple to more complex:\\n\\n1. Take an average over all words\\n2. Take a weighted average over all words. Weighting can be done by inverse document frequency (idf part of tf-idf).\\n3. Use ML model like LSTM or Transformer.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'scenario',\n",
       "  'tags': ['multiple',\n",
       "   'words',\n",
       "   'sentence',\n",
       "   'text_classification',\n",
       "   'have',\n",
       "   'with'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Would you prefer gradient boosting trees model or logistic regression when doing text classification with embeddings? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Gradient boosting trees (GBTs) are generally a better choice than logistic regression for text classification with embeddings. This is because GBTs are able to learn more complex relationships between the features in the data, including the features extracted from the embeddings.\\n\\nLogistic regression is a linear model, which means that it can only learn linear relationships between the features. This can be a limitation for text classification, where the relationships between the features are often complex and non-linear.\\n\\nGBTs, on the other hand, are able to learn non-linear relationships between the features by combining multiple decision trees. This allows GBTs to learn more complex patterns in the data, which can lead to better performance on text classification tasks.\\n\\nIn addition, GBTs are more robust to outliers and noise in the data than logistic regression. This can be important for text classification tasks, where the data can be noisy and imbalanced.\\n\\nOverall, GBTs are a better choice than logistic regression for text classification with embeddings, especially when the data is noisy or imbalanced. However, it is important to consider the computational cost and interpretability of GBTs before using them.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['trees',\n",
       "   'boosting',\n",
       "   'text_classification',\n",
       "   'gradient',\n",
       "   'prefer',\n",
       "   'would'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How can you use neural nets for text classification? üöÄ',\n",
       "  'reference_answer': 'Here is a general overview of how to use neural nets for text classification:\\n\\nPreprocess the text: This includes cleaning the text by removing stop words, punctuation, and other irrelevant symbols. It may also involve converting the text to lowercase and stemming or lemmatizing the words.\\nRepresent the text as a vector: This can be done using a variety of methods, such as one-hot encoding or word embeddings.\\nBuild the neural net: The neural net architecture will depend on the specific text classification task. However, a typical architecture will include an embedding layer, one or more hidden layers, and an output layer.\\nTrain the neural net: The neural net is trained by feeding it labeled examples of text data. The neural net will learn to adjust its parameters in order to minimize the loss function, which is typically the cross-entropy loss function.\\nEvaluate the neural net: Once the neural net is trained, it can be evaluated on a held-out test set to assess its performance.\\nHere are some specific examples of how neural nets can be used for text classification:\\nSentiment analysis, Spam detection, Topic classification, Language identification\\n\\nNeural nets have achieved state-of-the-art results on many text classification tasks. However, they can be computationally expensive to train and deploy.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['nets', 'text', 'text_classification', 'neural', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How can we use CNN for text classification? üöÄ',\n",
       "  'reference_answer': 'Here are some specific examples of how CNNs can be used for text classification:\\n\\nSentiment analysis: CNNs can be used to classify text as positive, negative, or neutral sentiment. This is a common task in social media analysis and customer service.\\nSpam detection: CNNs can be used to classify emails as spam or not spam. This is a common task in email filtering systems.\\nTopic classification: CNNs can be used to classify text documents into different topics. This is a common task in news and social media analysis.\\nLanguage identification: CNNs can be used to identify the language of a text document. This is a common task in translation systems.',\n",
       "  'topic': 'text_classification',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['text', 'text_classification', 'classification'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is unsupervised learning?',\n",
       "  'reference_answer': 'Unsupervised learning aims to detect patterns in data where no labels are given.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['learning', 'unsupervised', 'clustering'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is clustering? When do we need it?',\n",
       "  'reference_answer': 'Clustering algorithms group objects such that similar feature points are put into the same groups (clusters) and dissimilar feature points are put into different clusters.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['need', 'clustering'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Do you know how K-means works? \\u200dÔ∏è',\n",
       "  'reference_answer': '1. Partition points into k subsets.\\n2. Compute the seed points as the new centroids of the clusters of the current partitioning.\\n3. Assign each point to the cluster with the nearest seed point.\\n4. Go back to step 2 or stop when the assignment does not change.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['know', 'works', 'means', 'clustering'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How to select K for K-means? \\u200dÔ∏è',\n",
       "  'reference_answer': '* Domain knowledge, i.e. an expert knows the value of k\\n* Elbow method: compute the clusters for different values of k, for each k, calculate the total within-cluster sum of square, plot the sum according to the number of clusters and use the band as the number of clusters.\\n* Average silhouette method: compute the clusters for different values of k, for each k, calculate the average silhouette of observations, plot the silhouette according to the number of clusters and select the maximum as the number of clusters.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['means', 'select', 'clustering'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are the other clustering algorithms do you know? \\u200dÔ∏è',\n",
       "  'reference_answer': '* k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise.\\n* Agglomerative Hierarchical Clustering (AHC): hierarchical clusters combining the nearest clusters starting with each point as its own cluster.\\n* DIvisive ANAlysis Clustering (DIANA): hierarchical clustering starting with one cluster containing all points and splitting the clusters until each point describes its own cluster.\\n* Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Cluster defined as maximum set of density-connected points.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['know', 'other', 'algorithms', 'clustering'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Do you know how DBScan works? \\u200dÔ∏è',\n",
       "  'reference_answer': '* Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood)\\n* Cluster defined as maximum set of density-connected points.\\n* Points p_j and p_i are density-connected w.r.t. epsilon and minPts if there is a point o such that both, i and j are density-reachable from o w.r.t. epsilon and minPts.\\n* p_j is density-reachable from p_i w.r.t. epsilon, minPts if there is a chain of points p_i -> p_i+1 -> p_i+x = p_j such that p_i+x is directly density-reachable from p_i+x-1.\\n* p_j is a directly density-reachable point of the neighborhood of p_i if dist(p_i,p_j) <= epsilon.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['know', 'works', 'dbscan', 'clustering'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'When would you choose K-means and when DBScan? \\u200dÔ∏è',\n",
       "  'reference_answer': '* DBScan is more robust to noise.\\n* DBScan is better when the amount of clusters is difficult to guess.\\n* K-means has a lower complexity, i.e. it will be much faster, especially with a larger amount of points.',\n",
       "  'topic': 'clustering',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['means', 'clustering', 'choose', 'dbscan', 'would'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is the curse of dimensionality? Why do we care about it? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Data in only one dimension is relatively tightly packed. Adding a dimension stretches the points across that dimension, pushing them further apart. Additional dimensions spread the data even further making high dimensional data extremely sparse. We care about it, because it is difficult to use machine learning in sparse spaces.',\n",
       "  'topic': 'dimensionality_reduction',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['about',\n",
       "   'dimensionality',\n",
       "   'dimensionality_reduction',\n",
       "   'care',\n",
       "   'curse'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Do you know any dimensionality reduction techniques? \\u200dÔ∏è',\n",
       "  'reference_answer': '* Singular Value Decomposition (SVD)\\n* Principal Component Analysis (PCA)\\n* Linear Discriminant Analysis (LDA)\\n* T-distributed Stochastic Neighbor Embedding (t-SNE)\\n* Autoencoders\\n* Fourier and Wavelet Transforms',\n",
       "  'topic': 'dimensionality_reduction',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['techniques',\n",
       "   'dimensionality',\n",
       "   'dimensionality_reduction',\n",
       "   'reduction',\n",
       "   'know'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What‚Äôs singular value decomposition? How is it typically used for machine learning? \\u200dÔ∏è',\n",
       "  'reference_answer': '* Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Œ£ (diagonal matrix) and R^T (right singular values).\\n* For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive.\\n* Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction.',\n",
       "  'topic': 'dimensionality_reduction',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['value',\n",
       "   'singular',\n",
       "   'decomposition',\n",
       "   'typically',\n",
       "   'dimensionality_reduction',\n",
       "   'used'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is the ranking problem? Which models can you use to solve them? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['them',\n",
       "   'models',\n",
       "   'solve',\n",
       "   'problem',\n",
       "   'ranking',\n",
       "   'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are good unsupervised baselines for text information retrieval? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['baselines',\n",
       "   'text',\n",
       "   'unsupervised',\n",
       "   'information',\n",
       "   'good',\n",
       "   'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How would you evaluate your ranking algorithms? Which offline metrics would you use? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'scenario',\n",
       "  'tags': ['ranking_and\\xa0search',\n",
       "   'your',\n",
       "   'ranking',\n",
       "   'would',\n",
       "   'algorithms',\n",
       "   'evaluate'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is precision and recall at k? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Precision at k and recall at k are evaluation metrics for ranking algorithms. Precision at k shows the share of relevant items in the first *k* results of the ranking algorithm. And Recall at k indicates the share of relevant items returned in top *k* results out of all correct answers for a given query.\\n\\nExample:\\nFor a search query \"Car\" there are 3 relevant products in your shop. Your search algorithm returns 2 of those relevant products in the first 5 search results.\\nPrecision at 5 = # num of relevant products in search result / k = 2/5 = 40%\\nRecall at 5 = # num of relevant products in search result / # num of all relevant products = 2/3 = 66.6%',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['precision', 'recall', 'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is mean average precision at k? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['precision', 'average', 'mean', 'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How can we use machine learning for search? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['learning', 'search', 'machine', 'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How can we get training data for our ranking algorithms? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['training',\n",
       "   'data',\n",
       "   'ranking',\n",
       "   'algorithms',\n",
       "   'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Can we formulate the search problem as a classification problem? How? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'math',\n",
       "  'tags': ['search',\n",
       "   'problem',\n",
       "   'classification',\n",
       "   'formulate',\n",
       "   'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How can we use clicks data as the training data for ranking algorithms? üöÄ',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['training', 'data', 'ranking', 'clicks', 'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Do you know how to use gradient boosting trees for ranking? üöÄ',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'medium',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['trees',\n",
       "   'boosting',\n",
       "   'gradient',\n",
       "   'ranking',\n",
       "   'know',\n",
       "   'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How do you do an online evaluation of a new ranking algorithm? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Answer here',\n",
       "  'topic': 'ranking_and\\xa0search',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['algorithm',\n",
       "   'online',\n",
       "   'evaluation',\n",
       "   'ranking',\n",
       "   'ranking_and\\xa0search'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is a recommender system?',\n",
       "  'reference_answer': 'Recommender systems are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user.',\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['recommender', 'recommender_systems', 'system'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are good baselines when building a recommender system? \\u200dÔ∏è',\n",
       "  'reference_answer': '* A good recommer system should give relevant and personalized information.\\n* It should not recommend items the user knows well or finds easily.\\n* It should make diverse suggestions.\\n* A user should explore new items.',\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['baselines',\n",
       "   'building',\n",
       "   'system',\n",
       "   'recommender',\n",
       "   'recommender_systems',\n",
       "   'good'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is collaborative filtering? \\u200dÔ∏è',\n",
       "  'reference_answer': \"* Collaborative filtering is the most prominent approach to generate recommendations.\\n* It uses the wisdom of the crowd, i.e. it gives recommendations based on the experience of others.\\n* A recommendation is calculated as the average of other experiences.\\n* Say we want to give a score that indicates how much user u will like an item i. Then we can calculate it with the experience of N other users U as r_ui = 1/N * sum(v in U) r_vi.\\n* In order to rate similar experiences with a higher weight, we can introduce a similarity between users that we use as a multiplier for each rating.\\n* Also, as users have an individual profile, one user may have an average rating much larger than another user, so we use normalization techniques (e.g. centering or Z-score normalization) to remove the users' biases.\\n* Collaborative filtering does only need a rating matrix as input and improves over time. However, it does not work well on sparse data, does not work for cold starts (see below) and usually tends to overfit.\",\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['filtering', 'collaborative', 'recommender_systems'],\n",
       "  'key_concepts': ['say_we_want_to_give_a_score_that_indicates_how_much_user_u_will_like_an_item_i._then_we_can_calculate_it_with_the_experience_of_n_other_users_u_as_r_ui_=_1/n']},\n",
       " {'text': 'How we can incorporate implicit feedback (clicks, etc) into our recommender systems? \\u200dÔ∏è',\n",
       "  'reference_answer': \"In comparison to explicit feedback, implicit feedback datasets lack negative examples. For example, explicit feedback can be a positive or a negative rating, but implicit feedback may be the number of purchases or clicks. One popular approach to solve this problem is named weighted alternating least squares (wALS) [Hu, Y., Koren, Y., & Volinsky, C. (2008, December). Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on (pp. 263-272). IEEE.]. Instead of modeling the rating matrix directly, the numbers (e.g. amount of clicks) describe the strength in observations of user actions. The model tries to find latent factors that can be used to predict the expected preference of a user for an item.\",\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['implicit',\n",
       "   'incorporate',\n",
       "   'into',\n",
       "   'feedback',\n",
       "   'recommender_systems',\n",
       "   'clicks'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is the cold start problem? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Collaborative filterung incorporates crowd knowledge to give recommendations for certain items. Say we want to recommend how much a user will like an item, we then will calculate the score using the recommendations of other users for this certain item. We can distinguish between two different ways of a cold start problem now. First, if there is a new item that has not been rated yet, we cannot give any recommendation. Also, when there is a new user, we cannot calculate a similarity to any other user.',\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['start', 'problem', 'recommender_systems', 'cold'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Possible approaches to solving the cold start problem? \\u200dÔ∏èüöÄ',\n",
       "  'reference_answer': '* Content-based filtering incorporates features about items to calculate a similarity between them. In this way, we can recommend items that have a high similarity to items that a user liked already. In this way, we are not dependent on the ratings of other users for a given item anymore and solve the cold start problem for new items.\\n* Demographic filtering incorporates user profiles to calculate a similarity between them and solves the cold start problem for new users.',\n",
       "  'topic': 'recommender_systems',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['approaches',\n",
       "   'possible',\n",
       "   'start',\n",
       "   'solving',\n",
       "   'recommender_systems',\n",
       "   'cold'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What is a time series?',\n",
       "  'reference_answer': 'A time series is a set of observations ordered in time usually collected at regular intervals.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['time', 'series', 'time_series'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'How is time series different from the usual regression problem?',\n",
       "  'reference_answer': 'The principle behind causal forecasting is that the value that has to be predicted is dependant on the input features (causal factors). In time series forecasting, the to be predicted value is expected to follow a certain pattern over time.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'easy',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['from', 'different', 'usual', 'time', 'series', 'time_series'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'Which models do you know for solving time series problems? \\u200dÔ∏è',\n",
       "  'reference_answer': '* Simple Exponential Smoothing: approximate the time series with an exponential function\\n* Trend-Corrected Exponential Smoothing (Holt‚Äòs Method): exponential smoothing that also models the trend\\n* Trend- and Seasonality-Corrected Exponential Smoothing (Holt-Winter‚Äòs Method): exponential smoothing that also models trend and seasonality\\n* Time Series Decomposition: decomposed a time series into the four components trend, seasonal variation, cycling variation and irregular component\\n* Autoregressive models: similar to multiple linear regression, except that the dependent variable y_t depends on its own previous values rather than other independent variables.\\n* Deep learning approaches (RNN, LSTM, etc.)',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['models', 'solving', 'know', 'time', 'series', 'time_series'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'If there‚Äôs a trend in our series, how we can remove it? And why would we want to do it? \\u200dÔ∏è',\n",
       "  'reference_answer': \"We can explicitly model the trend (and/or seasonality) with approaches such as Holt's Method or Holt-Winter's Method. We want to explicitly model the trend to reach the stationarity property for the data. Many time series approaches require stationarity. Without stationarity,the interpretation of the results of these analyses is problematic [Manuca, Radu & Savit, Robert. (1996). Stationarity and nonstationarity in time series analysis. Physica D: Nonlinear Phenomena. 99. 134-161. 10.1016/S0167-2789(96)00139-X. ].\",\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['remove', 'trend', 'would', 'series', 'there', 'time_series'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'You have a series with only one variable ‚Äúy‚Äù measured at time t. How do predict ‚Äúy‚Äù at time t+1? Which approaches would you use? \\u200dÔ∏è',\n",
       "  'reference_answer': 'We want to look at the correlation between different observations of y. This measure of correlation is called autocorrelation. Autoregressive models are multiple regression models where the time-lag series of the original time series are treated like multiple independent variables.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['variable', 'only', 'have', 'with', 'series', 'time_series'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'You have a series with a variable ‚Äúy‚Äù and a set of features. How do you predict ‚Äúy‚Äù at t+1? Which approaches would you use? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Given the assumption that the set of features gives a meaningful causation to y, a causal forecasting approach such as linear regression or multiple nonlinear regression might be useful. In case there is a lot of data and the explainability of the results is not a high priority, we can also consider deep learning approaches.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['features', 'variable', 'have', 'with', 'series', 'time_series'],\n",
       "  'key_concepts': []},\n",
       " {'text': 'What are the problems with using trees for solving time series problems? \\u200dÔ∏è',\n",
       "  'reference_answer': 'Random Forest models are not able to extrapolate time series data and understand increasing/decreasing trends. It will provide us with average data points if the validation data has values greater than the training data points.',\n",
       "  'topic': 'time_series',\n",
       "  'difficulty': 'hard',\n",
       "  'question_type': 'conceptual',\n",
       "  'tags': ['trees', 'using', 'solving', 'with', 'problems', 'time_series'],\n",
       "  'key_concepts': []}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a2a06e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alokpadhi/ai-interview-system/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to /home/alokpadhi/.cache/kagglehub/datasets/sandy1811/data-science-interview-questions/1036.archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.03M/1.03M [00:01<00:00, 669kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /home/alokpadhi/.cache/kagglehub/datasets/sandy1811/data-science-interview-questions/versions/1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sandy1811/data-science-interview-questions\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6abd5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f400298",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = pd.read_csv(\"/home/alokpadhi/ai-interview-system/data/datasets/kaggle/chatbot_conversations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eaec8b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12996120, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "087cd6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conversation_id', 'turn', 'role', 'intent', 'message'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9c39e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27f91c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ai', 'android', 'books', 'business', 'career', 'cloud', 'coding',\n",
       "       'coding_errors', 'college', 'communication', 'datascience', 'dl',\n",
       "       'education', 'emotions', 'entertainment', 'farewell', 'finance',\n",
       "       'fitness', 'food', 'gaming', 'general', 'geography', 'greeting',\n",
       "       'habits', 'health', 'history', 'interview', 'ios', 'life',\n",
       "       'marketing', 'math', 'ml', 'motivation', 'motivation_daily',\n",
       "       'motivation_strong', 'music', 'networking', 'news', 'philosophy',\n",
       "       'productivity', 'projects', 'psychology', 'quotes', 'relationship',\n",
       "       'resume', 'salary', 'science', 'security', 'shopping', 'sleep',\n",
       "       'sports', 'study', 'technology', 'travel', 'weather'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(gpt[\"intent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2508527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gpt = gpt[gpt['intent'].isin([\"coding\", \"coding_errors\", \"datascience\", \"interview\", \"ml\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "467bc01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183174, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_gpt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52ecfebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gpt.to_csv(\"gpt_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f290fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Vector Embeddings, Semantic Search, Cosine Similarity, HNSW Index, Chunking Strategies (Fixed-size, Semantic), Re-ranking (Cross-Encoders), Dense vs. Sparse Retrieval (BM25 vs. Embeddings)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d979e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vector Embeddings',\n",
       " ' Semantic Search',\n",
       " ' Cosine Similarity',\n",
       " ' HNSW Index',\n",
       " ' Chunking Strategies (Fixed-size',\n",
       " ' Semantic)',\n",
       " ' Re-ranking (Cross-Encoders)',\n",
       " ' Dense vs. Sparse Retrieval (BM25 vs. Embeddings)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62daa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dffb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"/home/alokpadhi/ai-interview-system/data/datasets/raw/interview_questions/deeplearning_questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5741e687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is padding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sigmoid Vs Softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What is PoS Tagging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>What is tokenization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What is topic modeling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID              DESCRIPTION\n",
       "0   1          What is padding\n",
       "1   2       Sigmoid Vs Softmax\n",
       "2   3      What is PoS Tagging\n",
       "3   4     What is tokenization\n",
       "4   5   What is topic modeling"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15dab154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'DESCRIPTION'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf6e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503756f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/alokpadhi/ai-interview-system/data/datasets/processed/interview_questions/kaggle_datascience_interview_questions.json\", \"r\") as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cacfcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5972906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed95cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in d:\n",
    "    i[\"id\"] = re.sub(r\"github\", \"kaggle\", i[\"id\"])\n",
    "    r.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50a436e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' What is padding',\n",
       " 'difficulty': 'easy',\n",
       " 'topic': 'padding',\n",
       " 'question_type': 'conceptual',\n",
       " 'estimated_time_minutes': 3,\n",
       " 'tags': ['data-padding', 'padding', 'text-padding', 'attribute-padding'],\n",
       " 'reference_answer': 'Padding refers to the process of adding extra characters or spaces to a string, often to make it a certain length or to align with other strings.',\n",
       " 'key_concepts': ['space character', 'null character'],\n",
       " 'source': 'kaggle_data_science_interviews',\n",
       " 'id': 'kaggle_ds_interviews_0000'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e2c9c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65bd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/alokpadhi/ai-interview-system/data/datasets/processed/interview_questions/kaggle_datascience_interview_questions.json\", \"w\") as fp:\n",
    "    json.dump(r, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d9148e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detoxify import Detoxify\n",
    "\n",
    "# each model takes in either a string or a list of strings\n",
    "\n",
    "results = Detoxify('original', device='cuda').predict('example text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42943d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(float(results['toxicity']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a2ddd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.0006478309)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['toxicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8d57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a71d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/alokpadhi/ai-interview-system/data/datasets/processed/interview_questions/github_datascience_interview_questions.json\", \"r\") as fp:\n",
    "    github_data =  json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8219d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/alokpadhi/ai-interview-system/data/datasets/processed/interview_questions/kaggle_datascience_interview_questions.json\", \"r\") as fp:\n",
    "    kaggle_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56485442",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = github_data + kaggle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a11dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1277"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad03fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'What are the key differences between classification and regression? (Sample 1000)',\n",
       " 'difficulty': 'easy',\n",
       " 'topic': 'Machine Learning',\n",
       " 'question_type': 'conceptual',\n",
       " 'estimated_time_minutes': 3,\n",
       " 'tags': ['machine_learning',\n",
       "  'machine learning',\n",
       "  'between',\n",
       "  'supervised learning',\n",
       "  'regression',\n",
       "  'differences',\n",
       "  'sample',\n",
       "  'classification',\n",
       "  'unsupervised learning'],\n",
       " 'reference_answer': {'classification': 'Classification is used for predicting categorical outcomes, whereas regression is used for predicting continuous outcomes.',\n",
       "  'regression': 'Regression is a type of supervised learning algorithm that predicts continuous outcomes.'},\n",
       " 'key_concepts': ['target variable',\n",
       "  'prediction task',\n",
       "  'continuous outcome',\n",
       "  'categorical outcome'],\n",
       " 'source': 'kaggle_data_science_interviews',\n",
       " 'id': 'kaggle_ds_interviews_1110'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2548de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/alokpadhi/ai-interview-system/data/validation_output/questions_to_remove.json\", \"r\") as fp:\n",
    "    q_to_remove = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68663507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'github_ds_interviews_0004',\n",
       " 'text': 'What‚Äôs the normal distribution? Why do we care about it?',\n",
       " 'reasons': ['Irrelevant content']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_to_remove[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab0ab1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required = {\n",
    "#     \"Duplicate question\",\n",
    "#     \"Invalid difficulty 'NA'\"\n",
    "# }\n",
    "\n",
    "# excluded = {\n",
    "#     \"Irrelevant content\"\n",
    "# }\n",
    "\n",
    "# ids = [\n",
    "#     d[\"id\"]\n",
    "#     for d in q_to_remove\n",
    "#     if required.intersection(d.get(\"reasons\", []))\n",
    "#     and excluded.isdisjoint(d.get(\"reasons\", []))\n",
    "# ]\n",
    "\n",
    "ids = [\n",
    "    d[\"id\"]\n",
    "    for d in q_to_remove\n",
    "    if set(d.get(\"reasons\", [])) != {\"Irrelevant content\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fdd7beea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acbe9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_ids = [i for i in q_to_remove if \"Irrelevant content\" in i[\"reasons\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47cf2f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'github_ds_interviews_0004',\n",
       "  'text': 'What‚Äôs the normal distribution? Why do we care about it?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0005',\n",
       "  'text': 'How do we check if a variable follows the normal distribution? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0009',\n",
       "  'text': 'What is the normal equation? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0016',\n",
       "  'text': 'Why do we need to split our data into three parts: train, validation, and test?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0017',\n",
       "  'text': 'Can you explain how cross-validation works?',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'github_ds_interviews_0018',\n",
       "  'text': 'What is K-fold cross-validation?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0019',\n",
       "  'text': 'How do we choose K in K-fold cross-validation? What‚Äôs your favorite K?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0023',\n",
       "  'text': 'What is sigmoid? What does it do?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0027',\n",
       "  'text': 'What is the confusion table? What are the cells in this table?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0030',\n",
       "  'text': 'What is the ROC curve? When to use it? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0032',\n",
       "  'text': 'How to interpret the AU ROC score? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0034',\n",
       "  'text': 'What is the area under the PR curve? Is it a useful metric? \\u200dÔ∏èI',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0035',\n",
       "  'text': 'In which cases AU PR is better than AU ROC? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0036',\n",
       "  'text': 'What do we do with categorical variables? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0037',\n",
       "  'text': 'Why do we need one-hot encoding? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0038',\n",
       "  'text': 'What is \"curse of dimensionality\"? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'github_ds_interviews_0052',\n",
       "  'text': 'If a weight for one variable is higher than for another \\u200a‚Äî\\u200a can we say that this variable is more important? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0071',\n",
       "  'text': 'What are the potential problems with many large trees? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0072',\n",
       "  'text': 'What if instead of finding the best split, we randomly select a few splits and just select the best from them. Will it work? üöÄ',\n",
       "  'reasons': ['Irrelevant content', \"Invalid difficulty 'NA'\"]},\n",
       " {'id': 'github_ds_interviews_0073',\n",
       "  'text': 'What happens when we have correlated features in our data? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0082',\n",
       "  'text': 'Which hyper-parameter tuning strategies (in general) do you know? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0083',\n",
       "  'text': 'What‚Äôs the difference between grid search parameter tuning strategy and random search? When to use one or another? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0084',\n",
       "  'text': 'What kind of problems neural nets can solve?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0085',\n",
       "  'text': 'How does a usual fully-connected feed-forward neural network work? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0087',\n",
       "  'text': 'What are the problems with sigmoid as an activation function? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0088',\n",
       "  'text': 'What is ReLU? How is it better than sigmoid or tanh? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0089',\n",
       "  'text': 'How we can initialize the weights of a neural network? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0090',\n",
       "  'text': 'What if we set all the weights of a neural network to 0? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0092',\n",
       "  'text': 'What is dropout? Why is it useful? How does it work? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0093',\n",
       "  'text': 'What is backpropagation? How does it work? Why do we need it? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0096',\n",
       "  'text': 'What‚Äôs the learning rate?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0097',\n",
       "  'text': 'What happens when the learning rate is too large? Too small?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0098',\n",
       "  'text': 'How to set the learning rate? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0101',\n",
       "  'text': 'Do we want to have a constant learning rate or we better change it throughout training? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0102',\n",
       "  'text': 'How do we decide when to stop training a neural net?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0105',\n",
       "  'text': 'How we can use neural nets for computer vision? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0108',\n",
       "  'text': 'What‚Äôs pooling in CNN? Why do we need it? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0109',\n",
       "  'text': 'How does max pooling work? Are there other pooling techniques? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0110',\n",
       "  'text': 'Are CNNs resistant to rotations? What happens to the predictions of a CNN if an image is rotated? üöÄ',\n",
       "  'reasons': ['Irrelevant content', \"Invalid difficulty 'NA'\"]},\n",
       " {'id': 'github_ds_interviews_0111',\n",
       "  'text': 'What are augmentations? Why do we need them?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0112',\n",
       "  'text': 'What kind of augmentations do you know?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0113',\n",
       "  'text': 'How to choose which augmentations to use? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0115',\n",
       "  'text': 'What is transfer learning? How does it work? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'github_ds_interviews_0116',\n",
       "  'text': 'What is object detection? Do you know any architectures for that? üöÄ',\n",
       "  'reasons': ['Irrelevant content', \"Invalid difficulty 'NA'\"]},\n",
       " {'id': 'github_ds_interviews_0117',\n",
       "  'text': 'What is object segmentation? Do you know any architectures for that? üöÄ',\n",
       "  'reasons': ['Irrelevant content', \"Invalid difficulty 'NA'\"]},\n",
       " {'id': 'github_ds_interviews_0120',\n",
       "  'text': 'What are the advantages and disadvantages of bag of words? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0121',\n",
       "  'text': 'What are N-grams? How can we use them? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0122',\n",
       "  'text': 'How large should be N for our bag of words when using N-grams? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0126',\n",
       "  'text': 'What are word embeddings? Why are they useful? Do you know Word2Vec? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0127',\n",
       "  'text': 'Do you know any other ways to get word embeddings? üöÄ',\n",
       "  'reasons': ['Irrelevant content', \"Invalid difficulty 'NA'\"]},\n",
       " {'id': 'github_ds_interviews_0128',\n",
       "  'text': 'If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0139',\n",
       "  'text': 'What is the curse of dimensionality? Why do we care about it? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0140',\n",
       "  'text': 'Do you know any dimensionality reduction techniques? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0143',\n",
       "  'text': 'What are good unsupervised baselines for text information retrieval? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0153',\n",
       "  'text': 'What is a recommender system?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0154',\n",
       "  'text': 'What are good baselines when building a recommender system? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0155',\n",
       "  'text': 'What is collaborative filtering? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0156',\n",
       "  'text': 'How we can incorporate implicit feedback (clicks, etc) into our recommender systems? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0157',\n",
       "  'text': 'What is the cold start problem? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0158',\n",
       "  'text': 'Possible approaches to solving the cold start problem? \\u200dÔ∏èüöÄ',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0159',\n",
       "  'text': 'What is a time series?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0162',\n",
       "  'text': 'If there‚Äôs a trend in our series, how we can remove it? And why would we want to do it? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0163',\n",
       "  'text': 'You have a series with only one variable ‚Äúy‚Äù measured at time t. How do predict ‚Äúy‚Äù at time t+1? Which approaches would you use? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'github_ds_interviews_0164',\n",
       "  'text': 'You have a series with a variable ‚Äúy‚Äù and a set of features. How do you predict ‚Äúy‚Äù at t+1? Which approaches would you use? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'github_ds_interviews_0165',\n",
       "  'text': 'What are the problems with using trees for solving time series problems? \\u200dÔ∏è',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0000',\n",
       "  'text': ' What is padding',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0001',\n",
       "  'text': ' Sigmoid Vs Softmax',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0002',\n",
       "  'text': ' What is PoS Tagging',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0003',\n",
       "  'text': ' What is tokenization',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0005',\n",
       "  'text': ' What is back propagation',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0006',\n",
       "  'text': ' What is the idea behind GANs',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0007',\n",
       "  'text': ' What is the Computational Graph',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0008',\n",
       "  'text': ' What is sigmoid What does it do',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0009',\n",
       "  'text': ' What is Named-Entity Recognition',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0013',\n",
       "  'text': ' How is wordvec different from Glove',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0014',\n",
       "  'text': ' What Are the Different Layers on CNN',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0015',\n",
       "  'text': ' What makes CNNs translation invariant',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0016',\n",
       "  'text': ' How is fastText different from wordvec',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0017',\n",
       "  'text': ' Explain Generative Adversarial Network',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0018',\n",
       "  'text': ' What is backward and forward propagation',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0019',\n",
       "  'text': ' What are Syntactic and Semantic Analysis',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0020',\n",
       "  'text': ' What is a local optimumWhat is a local optimum',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0022',\n",
       "  'text': ' What is ReLU How is it better than sigmoid or tanh',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0023',\n",
       "  'text': ' What is transfer learning have you used it before',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0024',\n",
       "  'text': ' What is multi-task learning When should it be used',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0025',\n",
       "  'text': ' Difference between convex and non-convex cost function',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0026',\n",
       "  'text': ' Why do we remove stop words When do we not remove them',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0027',\n",
       "  'text': ' Explain the difference between an epoch a batch and an iteration',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0029',\n",
       "  'text': ' For online learning which one would you prefer SGD or Adagrad and why',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0030',\n",
       "  'text': ' What Is a Multi-layer Perceptron MLPWhat Is a Multi-layer Perceptron MLP',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0031',\n",
       "  'text': ' Is it always bad to have local optimaIs it always bad to have local optima',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0032',\n",
       "  'text': ' In node2vec, what does embedding represent topological similarity or nearness',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0033',\n",
       "  'text': ' What do you understand by Boltzmann Machine and Restricted Boltzmann Machines',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0034',\n",
       "  'text': ' How to compute an inverse matrix faster by playing around with some computational tricks',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0035',\n",
       "  'text': ' For infrequent/rare words which among CBOW and SkipGram should be used for wordvec training',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0036',\n",
       "  'text': ' What is pooling in CNN Why do we need it',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0038',\n",
       "  'text': ' How to Select a Batch Size Will selecting a batch size produce better or worse results?',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0039',\n",
       "  'text': ' What are N-grams How can we use them',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0040',\n",
       "  'text': ' How large should be N for our bag of words when using N-grams',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0046',\n",
       "  'text': ' difference between Vanishing gradient Vs Exploding gradient',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0047',\n",
       "  'text': ' How to handle dying node problems in case of ReLU activation function',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0048',\n",
       "  'text': ' What is the use of the leaky ReLU function',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0051',\n",
       "  'text': ' What is a dropout layer and how does it help a neural network',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0052',\n",
       "  'text': ' Explain why dropout in a neural network acts as a regularizer',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0054',\n",
       "  'text': ' How to handle exploding gradient problem',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0058',\n",
       "  'text': ' What happens to the predictions of a CNN if an image is rotated',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0060',\n",
       "  'text': ' Define Term Freuency & Inverse Document Freuency Tf-idf and how to use it for converting text to vector',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0063',\n",
       "  'text': ' What do you mean by Dropout and Batch Normalization, When and why use',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0064',\n",
       "  'text': ' What is the difference between online and batch learning',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0065',\n",
       "  'text': ' Is dropout used on the test set',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0066',\n",
       "  'text': ' What is an activation function and discuss the use of an activation function',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0069',\n",
       "  'text': ' Why is Rectified Linear Unit a good activation function',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0070',\n",
       "  'text': \" Why don't we use the Relu activation function in the output layer\",\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0071',\n",
       "  'text': ' What can go wrong if we use a linear activation instead of ReLU',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0072',\n",
       "  'text': ' Give examples in which a many-to-one RNN architecture is appropriate, Give examples in which a many-to-one RNN architecture is appropriate',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0073',\n",
       "  'text': ' What is RNN and How does an RNN work',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0074',\n",
       "  'text': ' Why Sigmoid or Tanh is not preferred to be used as the activation function in the hidden layer of the neural network',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0076',\n",
       "  'text': ' Why Tanh activation function preferred over sigmoid',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0077',\n",
       "  'text': ' What are word embeddings Why are they useful',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0078',\n",
       "  'text': ' what is WordVec',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0079',\n",
       "  'text': ' What are some advantages of using character embeddings instead of word embeddings',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0080',\n",
       "  'text': ' How do you get sentence meanings from word embeddings, considering the position of words in the sentence',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0082',\n",
       "  'text': ' What is bag of words How we can use it for text vectorization',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0083',\n",
       "  'text': ' What are the advantages and disadvantages of bag of words',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0087',\n",
       "  'text': ' When would you use GD over SDG and vice-versa',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0088',\n",
       "  'text': ' How would you choose the number of filters and the filter size at each CNN layer',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0092',\n",
       "  'text': ' Why do segmentation CNNs typically have an encoder-decoder style / structure',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0097',\n",
       "  'text': ' Why we generally use Softmax non-linearity function as the last operation in-network',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0098',\n",
       "  'text': ' How does BatchNormalization differ in training and inferencing',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0103',\n",
       "  'text': ' How would you initialize weights in a neural network',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0104',\n",
       "  'text': ' Why weights are initialized with small random numbers in a neural network What happens when weights are all or constant values',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0105',\n",
       "  'text': ' Suppose you have a NN with layers and ReLU activations What will happen if we initialize all the weights with the same value',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0106',\n",
       "  'text': ' What is backpropagation How does it work Why do we need it',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0107',\n",
       "  'text': ' Why large filter sizes in early layers can be a bad choice How to choose filter size',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0122',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 12)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0124',\n",
       "  'text': 'How does a neural network work? (Sample 14)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0130',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 20)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0136',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 26)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0144',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 34)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0154',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 44)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0158',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 48)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0162',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 52)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0166',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 56)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0167',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 57)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0169',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 59)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0172',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 62)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0177',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 67)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0181',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 71)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0185',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 75)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0192',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 82)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0195',\n",
       "  'text': 'How does a neural network work? (Sample 85)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0197',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 87)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0201',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 91)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0202',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 92)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0207',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 97)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0217',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 107)',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0219',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 109)',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0220',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 110)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0221',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 111)',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0223',\n",
       "  'text': 'How does a neural network work? (Sample 113)',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0246',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 136)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0247',\n",
       "  'text': 'How does a neural network work? (Sample 137)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0255',\n",
       "  'text': 'How does a neural network work? (Sample 145)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0259',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 149)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0263',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 153)',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0270',\n",
       "  'text': 'How does a neural network work? (Sample 160)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0277',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 167)',\n",
       "  'reasons': ['Irrelevant content']},\n",
       " {'id': 'kaggle_ds_interviews_0280',\n",
       "  'text': 'How does a neural network work? (Sample 170)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0282',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 172)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0284',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 174)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0286',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 176)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0308',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 198)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0309',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 199)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0316',\n",
       "  'text': 'How does a neural network work? (Sample 206)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0319',\n",
       "  'text': 'How does a neural network work? (Sample 209)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0320',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 210)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0321',\n",
       "  'text': 'How does a neural network work? (Sample 211)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0327',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 217)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0328',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 218)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0332',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 222)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0336',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 226)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0346',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 236)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0347',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 237)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0352',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 242)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0362',\n",
       "  'text': 'How does a neural network work? (Sample 252)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0364',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 254)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0365',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 255)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0369',\n",
       "  'text': 'How does a neural network work? (Sample 259)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0371',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 261)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0373',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 263)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0383',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 273)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0386',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 276)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0387',\n",
       "  'text': 'How does a neural network work? (Sample 277)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0391',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 281)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0400',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 290)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0403',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 293)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0404',\n",
       "  'text': 'How does a neural network work? (Sample 294)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0405',\n",
       "  'text': 'How does a neural network work? (Sample 295)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0406',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 296)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0413',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 303)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0422',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 312)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0424',\n",
       "  'text': 'How does a neural network work? (Sample 314)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0427',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 317)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0428',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 318)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0429',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 319)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0432',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 322)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0438',\n",
       "  'text': 'How does a neural network work? (Sample 328)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0441',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 331)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0445',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 335)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0448',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 338)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0449',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 339)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0457',\n",
       "  'text': 'How does a neural network work? (Sample 347)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0459',\n",
       "  'text': 'How does a neural network work? (Sample 349)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0463',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 353)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0474',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 364)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0479',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 369)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0481',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 371)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0483',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 373)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0484',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 374)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0486',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 376)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0493',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 383)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0496',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 386)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0499',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 389)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0502',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 392)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0505',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 395)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0506',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 396)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0507',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 397)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0508',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 398)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0525',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 415)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0526',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 416)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0528',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 418)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0531',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 421)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0532',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 422)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0533',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 423)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0534',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 424)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0536',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 426)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0539',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 429)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0540',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 430)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0541',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 431)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0547',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 437)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0550',\n",
       "  'text': 'How does a neural network work? (Sample 440)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0551',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 441)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0556',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 446)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0560',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 450)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0564',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 454)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0565',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 455)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0567',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 457)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0571',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 461)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0572',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 462)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0574',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 464)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0579',\n",
       "  'text': 'How does a neural network work? (Sample 469)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0589',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 479)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0593',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 483)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0599',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 489)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0600',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 490)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0602',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 492)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0603',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 493)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0604',\n",
       "  'text': 'How does a neural network work? (Sample 494)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0607',\n",
       "  'text': 'How does a neural network work? (Sample 497)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0609',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 499)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0610',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 500)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0611',\n",
       "  'text': 'How does a neural network work? (Sample 501)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0614',\n",
       "  'text': 'How does a neural network work? (Sample 504)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0618',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 508)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0622',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 512)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0628',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 518)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0629',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 519)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0631',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 521)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0633',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 523)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0634',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 524)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0637',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 527)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0639',\n",
       "  'text': 'How does a neural network work? (Sample 529)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0643',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 533)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0644',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 534)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0649',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 539)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0651',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 541)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0658',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 548)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0662',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 552)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0667',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 557)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0671',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 561)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0674',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 564)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0679',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 569)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0684',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 574)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0692',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 582)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0700',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 590)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0708',\n",
       "  'text': 'How does a neural network work? (Sample 598)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0713',\n",
       "  'text': 'How does a neural network work? (Sample 603)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0717',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 607)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0718',\n",
       "  'text': 'How does a neural network work? (Sample 608)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0721',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 611)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0725',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 615)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0726',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 616)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0735',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 625)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0748',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 638)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0763',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 653)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0764',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 654)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0766',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 656)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0768',\n",
       "  'text': 'How does a neural network work? (Sample 658)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0769',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 659)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0773',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 663)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0776',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 666)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0780',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 670)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0782',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 672)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0783',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 673)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0784',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 674)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0787',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 677)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0790',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 680)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0794',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 684)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0795',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 685)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0797',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 687)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0798',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 688)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0806',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 696)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0810',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 700)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0812',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 702)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0817',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 707)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0819',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 709)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0821',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 711)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0822',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 712)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0827',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 717)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0830',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 720)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0835',\n",
       "  'text': 'How does a neural network work? (Sample 725)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0837',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 727)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0838',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 728)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0840',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 730)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0842',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 732)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0846',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 736)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0848',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 738)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0850',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 740)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0853',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 743)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0862',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 752)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0863',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 753)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0875',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 765)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0877',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 767)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0878',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 768)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0882',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 772)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0883',\n",
       "  'text': 'How does a neural network work? (Sample 773)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0885',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 775)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0887',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 777)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0888',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 778)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0895',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 785)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0896',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 786)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0900',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 790)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0905',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 795)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0915',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 805)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0925',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 815)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0927',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 817)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0928',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 818)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0931',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 821)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0934',\n",
       "  'text': 'How does a neural network work? (Sample 824)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0935',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 825)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0936',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 826)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0938',\n",
       "  'text': 'How does a neural network work? (Sample 828)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0941',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 831)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0943',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 833)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0945',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 835)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0949',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 839)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0955',\n",
       "  'text': 'How does a neural network work? (Sample 845)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0957',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 847)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0961',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 851)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0970',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 860)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0973',\n",
       "  'text': 'How does a neural network work? (Sample 863)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0977',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 867)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0983',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 873)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0984',\n",
       "  'text': 'How does a neural network work? (Sample 874)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0990',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 880)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0992',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 882)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0993',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 883)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0994',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 884)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0996',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 886)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_0999',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 889)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1003',\n",
       "  'text': 'How does a neural network work? (Sample 893)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1006',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 896)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1008',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 898)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1009',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 899)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1029',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 919)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1030',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 920)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1033',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 923)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1035',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 925)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1054',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 944)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1056',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 946)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1057',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 947)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1058',\n",
       "  'text': 'How does a neural network work? (Sample 948)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1059',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 949)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1060',\n",
       "  'text': 'How does a neural network work? (Sample 950)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1065',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 955)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1070',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 960)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1072',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 962)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1077',\n",
       "  'text': 'How does a neural network work? (Sample 967)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1080',\n",
       "  'text': 'Explain cross-validation and its purpose. (Sample 970)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1081',\n",
       "  'text': 'How do you handle missing values in a dataset? (Sample 971)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1085',\n",
       "  'text': 'How does a neural network work? (Sample 975)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1086',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 976)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1087',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 977)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1089',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 979)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1090',\n",
       "  'text': 'How does a neural network work? (Sample 980)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1094',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 984)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1100',\n",
       "  'text': 'What are the advantages of ensemble methods? (Sample 990)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1101',\n",
       "  'text': 'Describe the process of backpropagation. (Sample 991)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1104',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 994)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']},\n",
       " {'id': 'kaggle_ds_interviews_1109',\n",
       "  'text': 'How would you approach data cleaning for a large dataset? (Sample 999)',\n",
       "  'reasons': ['Irrelevant content', 'Duplicate question']}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrelevant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bb50cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_github_kaggle_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d23a67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for que in final_data:\n",
    "    if que[\"id\"] not in ids:\n",
    "        filtered_github_kaggle_data.append(que)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "885950b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_github_kaggle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39cbd2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'What is supervised machine learning?',\n",
       " 'difficulty': 'easy',\n",
       " 'topic': 'supervised_machine\\xa0learning',\n",
       " 'question_type': 'conceptual',\n",
       " 'estimated_time_minutes': 2,\n",
       " 'tags': ['learning',\n",
       "  'supervised_machine\\xa0learning',\n",
       "  'supervised',\n",
       "  'machine'],\n",
       " 'reference_answer': 'Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the\\xa0input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.\\n\\nk-Nearest Neighbors(KNN):Looking at the k closest labeled data points \\n',\n",
       " 'key_concepts': [],\n",
       " 'source': 'github_data_science_interviews',\n",
       " 'id': 'github_ds_interviews_0000'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_github_kaggle_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01384438",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/alokpadhi/ai-interview-system/data/datasets/processed/interview_questions/filtered_github_kaggle_iqs.json\", \"w\") as fp:\n",
    "    json.dump(filtered_github_kaggle_data, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "365f6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/alokpadhi/ai-interview-system/data/validation_recheck_output/quantity/insufficient_categories.json\",  'r') as fp:\n",
    "    topics_to_cover = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fafd45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'category': 'topic',\n",
       "  'name': 'supervised_machine\\xa0learning',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'linear_regression',\n",
       "  'current': 10,\n",
       "  'required': 30,\n",
       "  'deficit': 20},\n",
       " {'category': 'topic',\n",
       "  'name': 'validation',\n",
       "  'current': 4,\n",
       "  'required': 30,\n",
       "  'deficit': 26},\n",
       " {'category': 'topic',\n",
       "  'name': 'classification',\n",
       "  'current': 18,\n",
       "  'required': 30,\n",
       "  'deficit': 12},\n",
       " {'category': 'topic',\n",
       "  'name': 'regularization',\n",
       "  'current': 13,\n",
       "  'required': 30,\n",
       "  'deficit': 17},\n",
       " {'category': 'topic',\n",
       "  'name': 'feature_selection',\n",
       "  'current': 4,\n",
       "  'required': 30,\n",
       "  'deficit': 26},\n",
       " {'category': 'topic',\n",
       "  'name': 'decision_trees',\n",
       "  'current': 6,\n",
       "  'required': 30,\n",
       "  'deficit': 24},\n",
       " {'category': 'topic',\n",
       "  'name': 'random_forest',\n",
       "  'current': 8,\n",
       "  'required': 30,\n",
       "  'deficit': 22},\n",
       " {'category': 'topic',\n",
       "  'name': 'gradient_boosting',\n",
       "  'current': 6,\n",
       "  'required': 30,\n",
       "  'deficit': 24},\n",
       " {'category': 'topic',\n",
       "  'name': 'parameter_tuning',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'neural_networks',\n",
       "  'current': 8,\n",
       "  'required': 30,\n",
       "  'deficit': 22},\n",
       " {'category': 'topic',\n",
       "  'name': 'optimization_in_neural\\xa0networks',\n",
       "  'current': 12,\n",
       "  'required': 30,\n",
       "  'deficit': 18},\n",
       " {'category': 'topic',\n",
       "  'name': 'neural_networks_for_computer\\xa0vision',\n",
       "  'current': 7,\n",
       "  'required': 30,\n",
       "  'deficit': 23},\n",
       " {'category': 'topic',\n",
       "  'name': 'text_classification',\n",
       "  'current': 10,\n",
       "  'required': 30,\n",
       "  'deficit': 20},\n",
       " {'category': 'topic',\n",
       "  'name': 'clustering',\n",
       "  'current': 7,\n",
       "  'required': 30,\n",
       "  'deficit': 23},\n",
       " {'category': 'topic',\n",
       "  'name': 'dimensionality_reduction',\n",
       "  'current': 3,\n",
       "  'required': 30,\n",
       "  'deficit': 27},\n",
       " {'category': 'topic',\n",
       "  'name': 'ranking_and\\xa0search',\n",
       "  'current': 9,\n",
       "  'required': 30,\n",
       "  'deficit': 21},\n",
       " {'category': 'topic',\n",
       "  'name': 'recommender_systems',\n",
       "  'current': 6,\n",
       "  'required': 30,\n",
       "  'deficit': 24},\n",
       " {'category': 'topic',\n",
       "  'name': 'time_series',\n",
       "  'current': 6,\n",
       "  'required': 30,\n",
       "  'deficit': 24},\n",
       " {'category': 'topic',\n",
       "  'name': 'padding',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Activation Functions',\n",
       "  'current': 8,\n",
       "  'required': 30,\n",
       "  'deficit': 22},\n",
       " {'category': 'topic',\n",
       "  'name': 'Natural Language Processing',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'NLP',\n",
       "  'current': 11,\n",
       "  'required': 30,\n",
       "  'deficit': 19},\n",
       " {'category': 'topic',\n",
       "  'name': 'Topic Modeling',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Artificial Intelligence/Neural Networks',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'General',\n",
       "  'current': 4,\n",
       "  'required': 30,\n",
       "  'deficit': 26},\n",
       " {'category': 'topic',\n",
       "  'name': 'Machine Learning',\n",
       "  'current': 25,\n",
       "  'required': 30,\n",
       "  'deficit': 5},\n",
       " {'category': 'topic',\n",
       "  'name': 'Analysis Techniques',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Optimization Techniques',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'Convex Optimization',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Information Retrieval',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Deep Learning Fundamentals',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Optimization Techniques for Online Learning',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Neural Networks',\n",
       "  'current': 3,\n",
       "  'required': 30,\n",
       "  'deficit': 27},\n",
       " {'category': 'topic',\n",
       "  'name': 'Linear Algebra and Matrix Operations',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Word Embeddings',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Batch Size Selection',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Optimization Algorithms',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Deep Learning Optimization',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'ReLU Activation Function and Dying Node Problem',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Deep Learning Frameworks',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Machine Learning vs Deep Learning',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Optimization Techniques in Deep Learning',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Deep Learning Techniques',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'Activation Functions in Neural Networks',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'Natural Language Processing (NLP)',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Text Vectorization',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Machine Learning/Deep Learning',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Generative Models',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Convolutional Neural Networks',\n",
       "  'current': 3,\n",
       "  'required': 30,\n",
       "  'deficit': 27},\n",
       " {'category': 'topic',\n",
       "  'name': 'Convolutional Neural Networks (CNN)',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Batch Normalization',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Machine Learning Algorithm Efficiency',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'Data Science',\n",
       "  'current': 4,\n",
       "  'required': 30,\n",
       "  'deficit': 26},\n",
       " {'category': 'topic',\n",
       "  'name': 'deployment',\n",
       "  'current': 4,\n",
       "  'required': 30,\n",
       "  'deficit': 26},\n",
       " {'category': 'topic',\n",
       "  'name': 'mlops',\n",
       "  'current': 7,\n",
       "  'required': 30,\n",
       "  'deficit': 23},\n",
       " {'category': 'topic',\n",
       "  'name': 'system_design',\n",
       "  'current': 3,\n",
       "  'required': 30,\n",
       "  'deficit': 27},\n",
       " {'category': 'topic',\n",
       "  'name': 'data_engineering',\n",
       "  'current': 6,\n",
       "  'required': 30,\n",
       "  'deficit': 24},\n",
       " {'category': 'topic',\n",
       "  'name': 'evaluation',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'feature_engineering',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'optimization',\n",
       "  'current': 5,\n",
       "  'required': 30,\n",
       "  'deficit': 25},\n",
       " {'category': 'topic',\n",
       "  'name': 'model_development',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'training',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'data_labeling',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'monitoring',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'deep_learning',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'experimentation',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'privacy',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'recsys',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'software_engineering',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'llm_optimization',\n",
       "  'current': 3,\n",
       "  'required': 30,\n",
       "  'deficit': 27},\n",
       " {'category': 'topic',\n",
       "  'name': 'rag',\n",
       "  'current': 4,\n",
       "  'required': 30,\n",
       "  'deficit': 26},\n",
       " {'category': 'topic',\n",
       "  'name': 'model_optimization',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'peft',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'agents',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'math_foundations',\n",
       "  'current': 3,\n",
       "  'required': 30,\n",
       "  'deficit': 27},\n",
       " {'category': 'topic',\n",
       "  'name': 'coding',\n",
       "  'current': 26,\n",
       "  'required': 30,\n",
       "  'deficit': 4},\n",
       " {'category': 'topic',\n",
       "  'name': 'llm_basics',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'deep_learning_training',\n",
       "  'current': 2,\n",
       "  'required': 30,\n",
       "  'deficit': 28},\n",
       " {'category': 'topic',\n",
       "  'name': 'generative_models',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'deep_learning_fundamentals',\n",
       "  'current': 4,\n",
       "  'required': 30,\n",
       "  'deficit': 26},\n",
       " {'category': 'topic',\n",
       "  'name': 'cnn',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'llm_history',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'llm_safety',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'metrics',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'python',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'nlp',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'security',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'pytorch',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'ml_fundamentals',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'llm_training',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'computer_vision',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'topic',\n",
       "  'name': 'training_dynamics',\n",
       "  'current': 1,\n",
       "  'required': 30,\n",
       "  'deficit': 29},\n",
       " {'category': 'difficulty',\n",
       "  'name': 'hard',\n",
       "  'current': 38,\n",
       "  'required': 50,\n",
       "  'deficit': 12},\n",
       " {'category': 'type',\n",
       "  'name': 'math',\n",
       "  'current': 12,\n",
       "  'required': 50,\n",
       "  'deficit': 38},\n",
       " {'category': 'type',\n",
       "  'name': 'scenario',\n",
       "  'current': 27,\n",
       "  'required': 50,\n",
       "  'deficit': 23},\n",
       " {'category': 'type',\n",
       "  'name': 'coding',\n",
       "  'current': 31,\n",
       "  'required': 50,\n",
       "  'deficit': 19},\n",
       " {'category': 'type',\n",
       "  'name': 'system_design',\n",
       "  'current': 3,\n",
       "  'required': 50,\n",
       "  'deficit': 47}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_to_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d025de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [{\"topic\": topic[\"name\"], \"questions_needed\": topic[\"deficit\"]}for topic in topics_to_cover]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ea54e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'topic': 'supervised_machine\\xa0learning', 'questions_needed': 29},\n",
       " {'topic': 'linear_regression', 'questions_needed': 20},\n",
       " {'topic': 'validation', 'questions_needed': 26},\n",
       " {'topic': 'classification', 'questions_needed': 12},\n",
       " {'topic': 'regularization', 'questions_needed': 17},\n",
       " {'topic': 'feature_selection', 'questions_needed': 26},\n",
       " {'topic': 'decision_trees', 'questions_needed': 24},\n",
       " {'topic': 'random_forest', 'questions_needed': 22},\n",
       " {'topic': 'gradient_boosting', 'questions_needed': 24},\n",
       " {'topic': 'parameter_tuning', 'questions_needed': 28},\n",
       " {'topic': 'neural_networks', 'questions_needed': 22},\n",
       " {'topic': 'optimization_in_neural\\xa0networks', 'questions_needed': 18},\n",
       " {'topic': 'neural_networks_for_computer\\xa0vision', 'questions_needed': 23},\n",
       " {'topic': 'text_classification', 'questions_needed': 20},\n",
       " {'topic': 'clustering', 'questions_needed': 23},\n",
       " {'topic': 'dimensionality_reduction', 'questions_needed': 27},\n",
       " {'topic': 'ranking_and\\xa0search', 'questions_needed': 21},\n",
       " {'topic': 'recommender_systems', 'questions_needed': 24},\n",
       " {'topic': 'time_series', 'questions_needed': 24},\n",
       " {'topic': 'padding', 'questions_needed': 29},\n",
       " {'topic': 'Activation Functions', 'questions_needed': 22},\n",
       " {'topic': 'Natural Language Processing', 'questions_needed': 28},\n",
       " {'topic': 'NLP', 'questions_needed': 19},\n",
       " {'topic': 'Topic Modeling', 'questions_needed': 29},\n",
       " {'topic': 'Artificial Intelligence/Neural Networks', 'questions_needed': 29},\n",
       " {'topic': 'General', 'questions_needed': 26},\n",
       " {'topic': 'Machine Learning', 'questions_needed': 5},\n",
       " {'topic': 'Analysis Techniques', 'questions_needed': 29},\n",
       " {'topic': 'Optimization Techniques', 'questions_needed': 28},\n",
       " {'topic': 'Convex Optimization', 'questions_needed': 29},\n",
       " {'topic': 'Information Retrieval', 'questions_needed': 29},\n",
       " {'topic': 'Deep Learning Fundamentals', 'questions_needed': 29},\n",
       " {'topic': 'Optimization Techniques for Online Learning',\n",
       "  'questions_needed': 29},\n",
       " {'topic': 'Neural Networks', 'questions_needed': 27},\n",
       " {'topic': 'Linear Algebra and Matrix Operations', 'questions_needed': 29},\n",
       " {'topic': 'Word Embeddings', 'questions_needed': 29},\n",
       " {'topic': 'Batch Size Selection', 'questions_needed': 29},\n",
       " {'topic': 'Optimization Algorithms', 'questions_needed': 29},\n",
       " {'topic': 'Deep Learning Optimization', 'questions_needed': 28},\n",
       " {'topic': 'ReLU Activation Function and Dying Node Problem',\n",
       "  'questions_needed': 29},\n",
       " {'topic': 'Deep Learning Frameworks', 'questions_needed': 29},\n",
       " {'topic': 'Machine Learning vs Deep Learning', 'questions_needed': 29},\n",
       " {'topic': 'Optimization Techniques in Deep Learning', 'questions_needed': 29},\n",
       " {'topic': 'Deep Learning Techniques', 'questions_needed': 28},\n",
       " {'topic': 'Activation Functions in Neural Networks', 'questions_needed': 28},\n",
       " {'topic': 'Natural Language Processing (NLP)', 'questions_needed': 29},\n",
       " {'topic': 'Text Vectorization', 'questions_needed': 29},\n",
       " {'topic': 'Machine Learning/Deep Learning', 'questions_needed': 29},\n",
       " {'topic': 'Generative Models', 'questions_needed': 29},\n",
       " {'topic': 'Convolutional Neural Networks', 'questions_needed': 27},\n",
       " {'topic': 'Convolutional Neural Networks (CNN)', 'questions_needed': 29},\n",
       " {'topic': 'Batch Normalization', 'questions_needed': 29},\n",
       " {'topic': 'Machine Learning Algorithm Efficiency', 'questions_needed': 29},\n",
       " {'topic': 'Data Science', 'questions_needed': 26},\n",
       " {'topic': 'deployment', 'questions_needed': 26},\n",
       " {'topic': 'mlops', 'questions_needed': 23},\n",
       " {'topic': 'system_design', 'questions_needed': 27},\n",
       " {'topic': 'data_engineering', 'questions_needed': 24},\n",
       " {'topic': 'evaluation', 'questions_needed': 28},\n",
       " {'topic': 'feature_engineering', 'questions_needed': 28},\n",
       " {'topic': 'optimization', 'questions_needed': 25},\n",
       " {'topic': 'model_development', 'questions_needed': 29},\n",
       " {'topic': 'training', 'questions_needed': 29},\n",
       " {'topic': 'data_labeling', 'questions_needed': 29},\n",
       " {'topic': 'monitoring', 'questions_needed': 29},\n",
       " {'topic': 'deep_learning', 'questions_needed': 29},\n",
       " {'topic': 'experimentation', 'questions_needed': 29},\n",
       " {'topic': 'privacy', 'questions_needed': 29},\n",
       " {'topic': 'recsys', 'questions_needed': 29},\n",
       " {'topic': 'software_engineering', 'questions_needed': 29},\n",
       " {'topic': 'llm_optimization', 'questions_needed': 27},\n",
       " {'topic': 'rag', 'questions_needed': 26},\n",
       " {'topic': 'model_optimization', 'questions_needed': 29},\n",
       " {'topic': 'peft', 'questions_needed': 28},\n",
       " {'topic': 'agents', 'questions_needed': 29},\n",
       " {'topic': 'math_foundations', 'questions_needed': 27},\n",
       " {'topic': 'coding', 'questions_needed': 4},\n",
       " {'topic': 'llm_basics', 'questions_needed': 28},\n",
       " {'topic': 'deep_learning_training', 'questions_needed': 28},\n",
       " {'topic': 'generative_models', 'questions_needed': 29},\n",
       " {'topic': 'deep_learning_fundamentals', 'questions_needed': 26},\n",
       " {'topic': 'cnn', 'questions_needed': 29},\n",
       " {'topic': 'llm_history', 'questions_needed': 29},\n",
       " {'topic': 'llm_safety', 'questions_needed': 29},\n",
       " {'topic': 'metrics', 'questions_needed': 29},\n",
       " {'topic': 'python', 'questions_needed': 29},\n",
       " {'topic': 'nlp', 'questions_needed': 29},\n",
       " {'topic': 'security', 'questions_needed': 29},\n",
       " {'topic': 'pytorch', 'questions_needed': 29},\n",
       " {'topic': 'ml_fundamentals', 'questions_needed': 29},\n",
       " {'topic': 'llm_training', 'questions_needed': 29},\n",
       " {'topic': 'computer_vision', 'questions_needed': 29},\n",
       " {'topic': 'training_dynamics', 'questions_needed': 29},\n",
       " {'topic': 'hard', 'questions_needed': 12},\n",
       " {'topic': 'math', 'questions_needed': 38},\n",
       " {'topic': 'scenario', 'questions_needed': 23},\n",
       " {'topic': 'coding', 'questions_needed': 19},\n",
       " {'topic': 'system_design', 'questions_needed': 47}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97595f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average questions needed: 26.520408163265305\n"
     ]
    }
   ],
   "source": [
    "average_needed = sum(i[\"questions_needed\"] for i in topics) / len(topics)\n",
    "print(f\"Average questions needed: {average_needed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d50d6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics = [i[\"topic\"] for i in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa108bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['supervised_machine\\xa0learning',\n",
       " 'linear_regression',\n",
       " 'validation',\n",
       " 'classification',\n",
       " 'regularization',\n",
       " 'feature_selection',\n",
       " 'decision_trees',\n",
       " 'random_forest',\n",
       " 'gradient_boosting',\n",
       " 'parameter_tuning',\n",
       " 'neural_networks',\n",
       " 'optimization_in_neural\\xa0networks',\n",
       " 'neural_networks_for_computer\\xa0vision',\n",
       " 'text_classification',\n",
       " 'clustering',\n",
       " 'dimensionality_reduction',\n",
       " 'ranking_and\\xa0search',\n",
       " 'recommender_systems',\n",
       " 'time_series',\n",
       " 'padding',\n",
       " 'Activation Functions',\n",
       " 'Natural Language Processing',\n",
       " 'NLP',\n",
       " 'Topic Modeling',\n",
       " 'Artificial Intelligence/Neural Networks',\n",
       " 'General',\n",
       " 'Machine Learning',\n",
       " 'Analysis Techniques',\n",
       " 'Optimization Techniques',\n",
       " 'Convex Optimization',\n",
       " 'Information Retrieval',\n",
       " 'Deep Learning Fundamentals',\n",
       " 'Optimization Techniques for Online Learning',\n",
       " 'Neural Networks',\n",
       " 'Linear Algebra and Matrix Operations',\n",
       " 'Word Embeddings',\n",
       " 'Batch Size Selection',\n",
       " 'Optimization Algorithms',\n",
       " 'Deep Learning Optimization',\n",
       " 'ReLU Activation Function and Dying Node Problem',\n",
       " 'Deep Learning Frameworks',\n",
       " 'Machine Learning vs Deep Learning',\n",
       " 'Optimization Techniques in Deep Learning',\n",
       " 'Deep Learning Techniques',\n",
       " 'Activation Functions in Neural Networks',\n",
       " 'Natural Language Processing (NLP)',\n",
       " 'Text Vectorization',\n",
       " 'Machine Learning/Deep Learning',\n",
       " 'Generative Models',\n",
       " 'Convolutional Neural Networks',\n",
       " 'Convolutional Neural Networks (CNN)',\n",
       " 'Batch Normalization',\n",
       " 'Machine Learning Algorithm Efficiency',\n",
       " 'Data Science',\n",
       " 'deployment',\n",
       " 'mlops',\n",
       " 'system_design',\n",
       " 'data_engineering',\n",
       " 'evaluation',\n",
       " 'feature_engineering',\n",
       " 'optimization',\n",
       " 'model_development',\n",
       " 'training',\n",
       " 'data_labeling',\n",
       " 'monitoring',\n",
       " 'deep_learning',\n",
       " 'experimentation',\n",
       " 'privacy',\n",
       " 'recsys',\n",
       " 'software_engineering',\n",
       " 'llm_optimization',\n",
       " 'rag',\n",
       " 'model_optimization',\n",
       " 'peft',\n",
       " 'agents',\n",
       " 'math_foundations',\n",
       " 'coding',\n",
       " 'llm_basics',\n",
       " 'deep_learning_training',\n",
       " 'generative_models',\n",
       " 'deep_learning_fundamentals',\n",
       " 'cnn',\n",
       " 'llm_history',\n",
       " 'llm_safety',\n",
       " 'metrics',\n",
       " 'python',\n",
       " 'nlp',\n",
       " 'security',\n",
       " 'pytorch',\n",
       " 'ml_fundamentals',\n",
       " 'llm_training',\n",
       " 'computer_vision',\n",
       " 'training_dynamics',\n",
       " 'hard',\n",
       " 'math',\n",
       " 'scenario',\n",
       " 'coding',\n",
       " 'system_design']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "96ae3f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2548"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "98*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c86080b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "        \"filtered_github_kaggle_iqs.json\",\n",
    "        \"interview_questions_chip.json\",\n",
    "        \"interview_questions.json\",\n",
    "        \"leetcode_questions.json\",\n",
    "        \"llm_generated_iqs.json\",\n",
    "        \"system_design_iqs.json\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1474bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "661c67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(Path.cwd()).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dae87c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/alokpadhi/ai-interview-system')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4c1a1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = base_dir / \"data/datasets/processed/interview_questions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3517ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/alokpadhi/ai-interview-system/data/datasets/processed/interview_questions')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2730594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ddc44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open(file, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6a77e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Loading filtered_github_kaggle_iqs.json...\n",
      "  - Loading interview_questions_chip.json...\n",
      "  - Loading interview_questions.json...\n",
      "  - Loading leetcode_questions.json...\n",
      "  - Loading llm_generated_iqs.json...\n",
      "  - Loading system_design_iqs.json...\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print(f\"  - Loading {file}...\")\n",
    "    data = load_json(datasets_path / file)\n",
    "    final_data.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8fcad406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "717"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "293c9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [data[\"id\"] for data in final_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "502af9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids) == len(set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5a3bc493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "717"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9b045384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([data[\"mistakes\"] for data in final_data if \"mistakes\" in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "00a55625",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/alokpadhi/ai-interview-system/data/rubrics/all_rubrics.json\", \"r\") as f:\n",
    "    rubrics = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e4a9a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(rubrics.keys()) == ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "48a2d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/alokpadhi/ai-interview-system/data/rubrics/manual_review_needed.json\", \"r\") as fp:\n",
    "    reviews = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "26c84305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hard question - needs detailed rubric',\n",
       " 'Math question - verify key steps',\n",
       " 'No reference answer',\n",
       " 'System design - needs custom criteria'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(review['reason'] for review in reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b41306",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"ml_concepts.json\", \"wikipedia_ml_concepts_part2.json\", \"wikipedia_ml_concepts.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22e3bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81ff007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a2e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(\"/home/alokpadhi/ai-interview-system/data/datasets/processed/concepts/\"+file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        concepts.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2235cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7155387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-interview-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
